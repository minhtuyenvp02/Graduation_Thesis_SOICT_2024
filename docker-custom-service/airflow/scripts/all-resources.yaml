apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-07-01T07:28:53Z"
    generateName: airflow-postgresql-
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: airflow
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: airflow-postgresql-59f5bcc96d
      helm.sh/chart: postgresql-13.2.24
      statefulset.kubernetes.io/pod-name: airflow-postgresql-0
    name: airflow-postgresql-0
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: airflow-postgresql
      uid: 2194de4e-9c3d-4766-91eb-a2f74afe12cf
    resourceVersion: "7818591"
    uid: 056365cb-9a10-45e5-8bb3-2acd7febf55b
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: primary
                app.kubernetes.io/instance: airflow
                app.kubernetes.io/name: postgresql
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: postgres-password
            name: airflow-postgresql
      - name: POSTGRESQL_ENABLE_LDAP
        value: "no"
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "false"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit
      image: docker.io/bitnami/postgresql:16.1.0-debian-11-r15
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: tcp-postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - -e
          - |
            exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
            [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dev/shm
        name: dshm
      - mountPath: /bitnami/postgresql
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ngwbk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: airflow-postgresql-0
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
    serviceAccount: default
    serviceAccountName: default
    subdomain: airflow-postgresql-hl
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-airflow-postgresql-0
    - emptyDir:
        medium: Memory
      name: dshm
    - name: kube-api-access-ngwbk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:53Z"
      message: '0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes
        are available: 2 No preemption victims found for incoming pod.'
      reason: Unschedulable
      status: "False"
      type: PodScheduled
    phase: Pending
    qosClass: Burstable
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/airflow-config: cdff98e8c3f0bebc19523bf34fd5a6eb9ca62c9c556e71201f3b5386fd6127b2
      checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8
      checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827
      checksum/metadata-secret: 1527346545415bf13f4c9ad69470086eb90d854f2b83594d78ec1badb5e13eb0
      checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688
      checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2024-07-01T07:28:22Z"
    generateName: airflow-scheduler-64b495fdcd-
    labels:
      component: scheduler
      pod-template-hash: 64b495fdcd
      release: airflow
      tier: airflow
    name: airflow-scheduler-64b495fdcd-569xt
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: airflow-scheduler-64b495fdcd
      uid: cfd15427-3847-4af4-b649-a6698237bcd0
    resourceVersion: "7816667"
    uid: 26e5525f-cd4e-48f7-840c-efad4a1e73af
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                component: scheduler
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - bash
      - -c
      - exec airflow scheduler
      env:
      - name: AIRFLOW_VAR_S3_ACCESS_KEY
        value: admin
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
        value: admin
      - name: AIRFLOW_VAR_S3_SECRET_KEY
        value: admin123
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
        value: admin123
      - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
        value: minhtuyenvp02/trip-generator
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
        value: minhtuyenvp02/trip-generator
      - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
        value: kafka-controller-headless.kafka.svc.cluster.local:9092
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
        value: kafka-controller-headless.kafka.svc.cluster.local:9092
      - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
        value: kafka.kafka.svc.cluster.local:9092
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
        value: kafka.kafka.svc.cluster.local:9092
      - name: AIRFLOW_VAR_TOPIC
        value: yellow_tripdata,fhvhv_tripdata
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
        value: yellow_tripdata,fhvhv_tripdata
      - name: AIRFLOW_VAR_S3_ENDPOINT
        value: http://minio.minio.svc.cluster.local:9000
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
        value: http://minio.minio.svc.cluster.local:9000
      - name: AIRFLOW_VAR_PATH_LOCATION_CSV
        value: s3a://nyc-trip-bucket/nyc-data/location.csv
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
        value: s3a://nyc-trip-bucket/nyc-data/location.csv
      - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
        value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
        value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
      - name: AIRFLOW_VAR_SPARK_CLUSTER
        value: spark://spark-master-svc.spark.svc.cluster.local:7077
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
        value: spark://spark-master-svc.spark.svc.cluster.local:7077
      - name: AIRFLOW_VAR_DATA_DIR
        value: nyc-trip-bucket/nyc-data/2023
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
        value: nyc-trip-bucket/nyc-data/2023
      - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
        value: "200"
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
        value: "200"
      - name: AIRFLOW_VAR_S3_BUCKET_NAME
        value: nyc-trip-bucket
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
        value: nyc-trip-bucket
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /opt/airflow/secrets/key.json
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
        value: /opt/airflow/secrets/key.json
      - name: AIRFLOW_VAR_SLACK_WEB_HOOK
        valueFrom:
          secretKeyRef:
            key: SLACK_WEB_HOOK
            name: webhook-secret
      - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
        value: webhook-secret=SLACK_WEB_HOOK
      - name: AIRFLOW__CORE__FERNET_KEY
        valueFrom:
          secretKeyRef:
            key: fernet-key
            name: airflow-fernet-key
      - name: AIRFLOW_HOME
        value: /opt/airflow
      - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW_CONN_AIRFLOW_DB
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW__WEBSERVER__SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: webserver-secret-key
            name: airflow-webserver-secret-key
      image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imagePullPolicy: Always
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - |
            CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
            airflow jobs check --job-type SchedulerJob --local
        failureThreshold: 30
        initialDelaySeconds: 120
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: scheduler
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      startupProbe:
        exec:
          command:
          - sh
          - -c
          - |
            CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
            airflow jobs check --job-type SchedulerJob --local
        failureThreshold: 6
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 20
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/pod_templates/pod_template_file.yaml
        name: config
        readOnly: true
        subPath: pod_template_file.yaml
      - mountPath: /opt/airflow/logs
        name: logs
      - mountPath: /opt/airflow/airflow.cfg
        name: config
        readOnly: true
        subPath: airflow.cfg
      - mountPath: /opt/airflow/config/airflow_local_settings.py
        name: config
        readOnly: true
        subPath: airflow_local_settings.py
      - mountPath: /opt/airflow/dags
        name: dags
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x9fc9
        readOnly: true
    - env:
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/ssh
      - name: GITSYNC_SSH_KEY_FILE
        value: /etc/git-secret/ssh
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GITSYNC_SSH
        value: "true"
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: GITSYNC_SSH_KNOWN_HOSTS
        value: "false"
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GITSYNC_REF
        value: v2-2-stable
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REPO
        value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
      - name: GITSYNC_REPO
        value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GITSYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_ROOT
        value: /git
      - name: GITSYNC_ROOT
        value: /git
      - name: GIT_SYNC_DEST
        value: repo
      - name: GITSYNC_LINK
        value: repo
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GITSYNC_ADD_USER
        value: "true"
      - name: GITSYNC_PERIOD
        value: 5s
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GITSYNC_MAX_FAILURES
        value: "0"
      image: registry.k8s.io/git-sync/git-sync:v4.1.0
      imagePullPolicy: IfNotPresent
      name: git-sync
      resources: {}
      securityContext:
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /git
        name: dags
      - mountPath: /etc/git-secret/ssh
        name: git-sync-ssh-key
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x9fc9
        readOnly: true
    - args:
      - bash
      - /clean-logs
      env:
      - name: AIRFLOW__LOG_RETENTION_DAYS
        value: "15"
      - name: AIRFLOW_HOME
        value: /opt/airflow
      image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imagePullPolicy: Always
      name: scheduler-log-groomer
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/logs
        name: logs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x9fc9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - airflow
      - db
      - check-migrations
      - --migration-wait-timeout=60
      env:
      - name: AIRFLOW_VAR_S3_ACCESS_KEY
        value: admin
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
        value: admin
      - name: AIRFLOW_VAR_S3_SECRET_KEY
        value: admin123
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
        value: admin123
      - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
        value: minhtuyenvp02/trip-generator
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
        value: minhtuyenvp02/trip-generator
      - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
        value: kafka-controller-headless.kafka.svc.cluster.local:9092
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
        value: kafka-controller-headless.kafka.svc.cluster.local:9092
      - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
        value: kafka.kafka.svc.cluster.local:9092
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
        value: kafka.kafka.svc.cluster.local:9092
      - name: AIRFLOW_VAR_TOPIC
        value: yellow_tripdata,fhvhv_tripdata
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
        value: yellow_tripdata,fhvhv_tripdata
      - name: AIRFLOW_VAR_S3_ENDPOINT
        value: http://minio.minio.svc.cluster.local:9000
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
        value: http://minio.minio.svc.cluster.local:9000
      - name: AIRFLOW_VAR_PATH_LOCATION_CSV
        value: s3a://nyc-trip-bucket/nyc-data/location.csv
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
        value: s3a://nyc-trip-bucket/nyc-data/location.csv
      - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
        value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
        value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
      - name: AIRFLOW_VAR_SPARK_CLUSTER
        value: spark://spark-master-svc.spark.svc.cluster.local:7077
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
        value: spark://spark-master-svc.spark.svc.cluster.local:7077
      - name: AIRFLOW_VAR_DATA_DIR
        value: nyc-trip-bucket/nyc-data/2023
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
        value: nyc-trip-bucket/nyc-data/2023
      - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
        value: "200"
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
        value: "200"
      - name: AIRFLOW_VAR_S3_BUCKET_NAME
        value: nyc-trip-bucket
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
        value: nyc-trip-bucket
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /opt/airflow/secrets/key.json
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
        value: /opt/airflow/secrets/key.json
      - name: AIRFLOW_VAR_SLACK_WEB_HOOK
        valueFrom:
          secretKeyRef:
            key: SLACK_WEB_HOOK
            name: webhook-secret
      - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
        value: webhook-secret=SLACK_WEB_HOOK
      - name: AIRFLOW__CORE__FERNET_KEY
        valueFrom:
          secretKeyRef:
            key: fernet-key
            name: airflow-fernet-key
      - name: AIRFLOW_HOME
        value: /opt/airflow
      - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW_CONN_AIRFLOW_DB
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW__WEBSERVER__SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: webserver-secret-key
            name: airflow-webserver-secret-key
      image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imagePullPolicy: Always
      name: wait-for-airflow-migrations
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/airflow.cfg
        name: config
        readOnly: true
        subPath: airflow.cfg
      - mountPath: /opt/airflow/config/airflow_local_settings.py
        name: config
        readOnly: true
        subPath: airflow_local_settings.py
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x9fc9
        readOnly: true
    - env:
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/ssh
      - name: GITSYNC_SSH_KEY_FILE
        value: /etc/git-secret/ssh
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GITSYNC_SSH
        value: "true"
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: GITSYNC_SSH_KNOWN_HOSTS
        value: "false"
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GITSYNC_REF
        value: v2-2-stable
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REPO
        value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
      - name: GITSYNC_REPO
        value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GITSYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_ROOT
        value: /git
      - name: GITSYNC_ROOT
        value: /git
      - name: GIT_SYNC_DEST
        value: repo
      - name: GITSYNC_LINK
        value: repo
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GITSYNC_ADD_USER
        value: "true"
      - name: GITSYNC_PERIOD
        value: 5s
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GITSYNC_MAX_FAILURES
        value: "0"
      - name: GIT_SYNC_ONE_TIME
        value: "true"
      - name: GITSYNC_ONE_TIME
        value: "true"
      image: registry.k8s.io/git-sync/git-sync:v4.1.0
      imagePullPolicy: IfNotPresent
      name: git-sync-init
      resources: {}
      securityContext:
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /git
        name: dags
      - mountPath: /etc/git-secret/ssh
        name: git-sync-ssh-key
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x9fc9
        readOnly: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 0
      runAsUser: 50000
    serviceAccount: airflow-scheduler
    serviceAccountName: airflow-scheduler
    terminationGracePeriodSeconds: 10
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: airflow-config
      name: config
    - emptyDir: {}
      name: dags
    - name: git-sync-ssh-key
      secret:
        defaultMode: 288
        secretName: airflow-ssh-secret
    - emptyDir: {}
      name: logs
    - name: kube-api-access-x9fc9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:25Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:22Z"
      message: 'containers with incomplete status: [wait-for-airflow-migrations git-sync-init]'
      reason: ContainersNotInitialized
      status: "False"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:22Z"
      message: 'containers with unready status: [scheduler git-sync scheduler-log-groomer]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:22Z"
      message: 'containers with unready status: [scheduler git-sync scheduler-log-groomer]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: registry.k8s.io/git-sync/git-sync:v4.1.0
      imageID: ""
      lastState: {}
      name: git-sync
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
    - image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imageID: ""
      lastState: {}
      name: scheduler
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
    - image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imageID: ""
      lastState: {}
      name: scheduler-log-groomer
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    initContainerStatuses:
    - containerID: containerd://47686c6cdf02531a4978f554518baf2b35c4cfa1257ab81ac11084aadcaad862
      image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imageID: docker.io/minhtuyenvp02/airflow-spark@sha256:ca7cd2a1efdc4fd9c1ed3569c152759957dd406908ce7007c98390b5fa44f958
      lastState: {}
      name: wait-for-airflow-migrations
      ready: false
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-07-01T07:28:25Z"
    - image: registry.k8s.io/git-sync/git-sync:v4.1.0
      imageID: ""
      lastState: {}
      name: git-sync-init
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
    phase: Pending
    podIP: 10.244.0.105
    podIPs:
    - ip: 10.244.0.105
    qosClass: BestEffort
    startTime: "2024-07-01T07:28:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-07-01T07:28:22Z"
    generateName: airflow-statsd-5667dd85ff-
    labels:
      component: statsd
      pod-template-hash: 5667dd85ff
      release: airflow
      tier: airflow
    name: airflow-statsd-5667dd85ff-kdwxt
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: airflow-statsd-5667dd85ff
      uid: df8b04b0-a5c7-4980-b95d-894b81655e69
    resourceVersion: "7816866"
    uid: a7993a91-deaf-45f0-b338-d4d251faf210
  spec:
    affinity: {}
    containers:
    - args:
      - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml
      image: quay.io/prometheus/statsd-exporter:v0.26.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: 9102
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: statsd
      ports:
      - containerPort: 9125
        name: statsd-ingest
        protocol: UDP
      - containerPort: 9102
        name: statsd-scrape
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: 9102
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/statsd-exporter/mappings.yml
        name: config
        subPath: mappings.yml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5sw79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pool-1em8noa5l-rwr1p
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsUser: 65534
    serviceAccount: airflow-statsd
    serviceAccountName: airflow-statsd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: airflow-statsd
      name: config
    - name: kube-api-access-5sw79
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:24Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:33Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:33Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b9e14e3d1c217863c335aea4e35122df47ea9339bc9204aab7fe6bc9b470110d
      image: quay.io/prometheus/statsd-exporter:v0.26.0
      imageID: quay.io/prometheus/statsd-exporter@sha256:a3924f9429c8237293336ff40c5a246238ff9f64aaf712521b2d29f45d6214d5
      lastState: {}
      name: statsd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-07-01T07:28:23Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    phase: Running
    podIP: 10.244.26.105
    podIPs:
    - ip: 10.244.26.105
    qosClass: BestEffort
    startTime: "2024-07-01T07:28:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/airflow-config: cdff98e8c3f0bebc19523bf34fd5a6eb9ca62c9c556e71201f3b5386fd6127b2
      checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8
      checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827
      checksum/metadata-secret: 1527346545415bf13f4c9ad69470086eb90d854f2b83594d78ec1badb5e13eb0
      checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2024-06-18T17:29:17Z"
    generateName: airflow-triggerer-
    labels:
      apps.kubernetes.io/pod-index: "0"
      component: triggerer
      controller-revision-hash: airflow-triggerer-659668cfc8
      release: airflow
      statefulset.kubernetes.io/pod-name: airflow-triggerer-0
      tier: airflow
    name: airflow-triggerer-0
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: airflow-triggerer
      uid: cd7a8e94-71a4-457d-934a-e103df36d833
    resourceVersion: "7820158"
    uid: ea38441f-8bd7-42f5-987b-950a525c0136
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                component: triggerer
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - bash
      - -c
      - exec airflow triggerer
      env:
      - name: AIRFLOW_VAR_S3_ACCESS_KEY
        value: admin
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
        value: admin
      - name: AIRFLOW_VAR_S3_SECRET_KEY
        value: admin123
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
        value: admin123
      - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
        value: minhtuyenvp02/trip-generator
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
        value: minhtuyenvp02/trip-generator
      - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
        value: kafka-controller-headless.kafka.svc.cluster.local:9092
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
        value: kafka-controller-headless.kafka.svc.cluster.local:9092
      - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
        value: kafka.kafka.svc.cluster.local:9092
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
        value: kafka.kafka.svc.cluster.local:9092
      - name: AIRFLOW_VAR_TOPIC
        value: yellow_tripdata,fhvhv_tripdata
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
        value: yellow_tripdata,fhvhv_tripdata
      - name: AIRFLOW_VAR_S3_ENDPOINT
        value: http://minio.minio.svc.cluster.local:9000
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
        value: http://minio.minio.svc.cluster.local:9000
      - name: AIRFLOW_VAR_PATH_LOCATION_CSV
        value: s3a://nyc-trip-bucket/nyc-data/location.csv
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
        value: s3a://nyc-trip-bucket/nyc-data/location.csv
      - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
        value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
        value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
      - name: AIRFLOW_VAR_SPARK_CLUSTER
        value: spark://spark-master-svc.spark.svc.cluster.local:7077
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
        value: spark://spark-master-svc.spark.svc.cluster.local:7077
      - name: AIRFLOW_VAR_DATA_DIR
        value: nyc-trip-bucket/nyc-data/2023
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
        value: nyc-trip-bucket/nyc-data/2023
      - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
        value: "200"
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
        value: "200"
      - name: AIRFLOW_VAR_S3_BUCKET_NAME
        value: nyc-trip-bucket
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
        value: nyc-trip-bucket
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /opt/airflow/secrets/key.json
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
        value: /opt/airflow/secrets/key.json
      - name: AIRFLOW_VAR_SLACK_WEB_HOOK
        valueFrom:
          secretKeyRef:
            key: SLACK_WEB_HOOK
            name: webhook-secret
      - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
        value: webhook-secret=SLACK_WEB_HOOK
      - name: AIRFLOW__CORE__FERNET_KEY
        valueFrom:
          secretKeyRef:
            key: fernet-key
            name: airflow-fernet-key
      - name: AIRFLOW_HOME
        value: /opt/airflow
      - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW_CONN_AIRFLOW_DB
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW__WEBSERVER__SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: webserver-secret-key
            name: airflow-webserver-secret-key
      image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imagePullPolicy: Always
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - |
            CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
            airflow jobs check --job-type TriggererJob --local
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: triggerer
      ports:
      - containerPort: 8794
        name: triggerer-logs
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/logs
        name: logs
      - mountPath: /opt/airflow/airflow.cfg
        name: config
        readOnly: true
        subPath: airflow.cfg
      - mountPath: /opt/airflow/config/airflow_local_settings.py
        name: config
        readOnly: true
        subPath: airflow_local_settings.py
      - mountPath: /opt/airflow/dags
        name: dags
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p4t46
        readOnly: true
    - env:
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/ssh
      - name: GITSYNC_SSH_KEY_FILE
        value: /etc/git-secret/ssh
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GITSYNC_SSH
        value: "true"
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: GITSYNC_SSH_KNOWN_HOSTS
        value: "false"
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GITSYNC_REF
        value: v2-2-stable
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REPO
        value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
      - name: GITSYNC_REPO
        value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GITSYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_ROOT
        value: /git
      - name: GITSYNC_ROOT
        value: /git
      - name: GIT_SYNC_DEST
        value: repo
      - name: GITSYNC_LINK
        value: repo
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GITSYNC_ADD_USER
        value: "true"
      - name: GITSYNC_PERIOD
        value: 5s
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GITSYNC_MAX_FAILURES
        value: "0"
      image: registry.k8s.io/git-sync/git-sync:v4.1.0
      imagePullPolicy: IfNotPresent
      name: git-sync
      resources: {}
      securityContext:
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /git
        name: dags
      - mountPath: /etc/git-secret/ssh
        name: git-sync-ssh-key
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p4t46
        readOnly: true
    - args:
      - bash
      - /clean-logs
      env:
      - name: AIRFLOW__LOG_RETENTION_DAYS
        value: "15"
      - name: AIRFLOW_HOME
        value: /opt/airflow
      image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imagePullPolicy: Always
      name: triggerer-log-groomer
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/logs
        name: logs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p4t46
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: airflow-triggerer-0
    initContainers:
    - args:
      - airflow
      - db
      - check-migrations
      - --migration-wait-timeout=60
      env:
      - name: AIRFLOW_VAR_S3_ACCESS_KEY
        value: admin
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
        value: admin
      - name: AIRFLOW_VAR_S3_SECRET_KEY
        value: admin123
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
        value: admin123
      - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
        value: minhtuyenvp02/trip-generator
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
        value: minhtuyenvp02/trip-generator
      - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
        value: kafka-controller-headless.kafka.svc.cluster.local:9092
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
        value: kafka-controller-headless.kafka.svc.cluster.local:9092
      - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
        value: kafka.kafka.svc.cluster.local:9092
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
        value: kafka.kafka.svc.cluster.local:9092
      - name: AIRFLOW_VAR_TOPIC
        value: yellow_tripdata,fhvhv_tripdata
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
        value: yellow_tripdata,fhvhv_tripdata
      - name: AIRFLOW_VAR_S3_ENDPOINT
        value: http://minio.minio.svc.cluster.local:9000
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
        value: http://minio.minio.svc.cluster.local:9000
      - name: AIRFLOW_VAR_PATH_LOCATION_CSV
        value: s3a://nyc-trip-bucket/nyc-data/location.csv
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
        value: s3a://nyc-trip-bucket/nyc-data/location.csv
      - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
        value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
        value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
      - name: AIRFLOW_VAR_SPARK_CLUSTER
        value: spark://spark-master-svc.spark.svc.cluster.local:7077
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
        value: spark://spark-master-svc.spark.svc.cluster.local:7077
      - name: AIRFLOW_VAR_DATA_DIR
        value: nyc-trip-bucket/nyc-data/2023
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
        value: nyc-trip-bucket/nyc-data/2023
      - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
        value: "200"
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
        value: "200"
      - name: AIRFLOW_VAR_S3_BUCKET_NAME
        value: nyc-trip-bucket
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
        value: nyc-trip-bucket
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /opt/airflow/secrets/key.json
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
        value: /opt/airflow/secrets/key.json
      - name: AIRFLOW_VAR_SLACK_WEB_HOOK
        valueFrom:
          secretKeyRef:
            key: SLACK_WEB_HOOK
            name: webhook-secret
      - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
        value: webhook-secret=SLACK_WEB_HOOK
      - name: AIRFLOW__CORE__FERNET_KEY
        valueFrom:
          secretKeyRef:
            key: fernet-key
            name: airflow-fernet-key
      - name: AIRFLOW_HOME
        value: /opt/airflow
      - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW_CONN_AIRFLOW_DB
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW__WEBSERVER__SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: webserver-secret-key
            name: airflow-webserver-secret-key
      image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imagePullPolicy: Always
      name: wait-for-airflow-migrations
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/airflow.cfg
        name: config
        readOnly: true
        subPath: airflow.cfg
      - mountPath: /opt/airflow/config/airflow_local_settings.py
        name: config
        readOnly: true
        subPath: airflow_local_settings.py
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p4t46
        readOnly: true
    - env:
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/ssh
      - name: GITSYNC_SSH_KEY_FILE
        value: /etc/git-secret/ssh
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GITSYNC_SSH
        value: "true"
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: GITSYNC_SSH_KNOWN_HOSTS
        value: "false"
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GITSYNC_REF
        value: v2-2-stable
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REPO
        value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
      - name: GITSYNC_REPO
        value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GITSYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_ROOT
        value: /git
      - name: GITSYNC_ROOT
        value: /git
      - name: GIT_SYNC_DEST
        value: repo
      - name: GITSYNC_LINK
        value: repo
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GITSYNC_ADD_USER
        value: "true"
      - name: GITSYNC_PERIOD
        value: 5s
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GITSYNC_MAX_FAILURES
        value: "0"
      - name: GIT_SYNC_ONE_TIME
        value: "true"
      - name: GITSYNC_ONE_TIME
        value: "true"
      image: registry.k8s.io/git-sync/git-sync:v4.1.0
      imagePullPolicy: IfNotPresent
      name: git-sync-init
      resources: {}
      securityContext:
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /git
        name: dags
      - mountPath: /etc/git-secret/ssh
        name: git-sync-ssh-key
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p4t46
        readOnly: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 0
      runAsUser: 50000
    serviceAccount: airflow-triggerer
    serviceAccountName: airflow-triggerer
    subdomain: airflow-triggerer
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: logs
      persistentVolumeClaim:
        claimName: logs-airflow-triggerer-0
    - configMap:
        defaultMode: 420
        name: airflow-config
      name: config
    - emptyDir: {}
      name: dags
    - name: git-sync-ssh-key
      secret:
        defaultMode: 288
        secretName: airflow-ssh-secret
    - name: kube-api-access-p4t46
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-18T17:29:33Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-18T17:30:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-18T17:30:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-18T17:30:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-18T17:29:17Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d31522941d249250b7c6a843060741a04a200a04a2752afc7dbe78fa062a3b42
      image: registry.k8s.io/git-sync/git-sync:v4.1.0
      imageID: registry.k8s.io/git-sync/git-sync@sha256:fd9722fd02e3a559fd6bb4427417c53892068f588fc8372aa553fbf2f05f9902
      lastState: {}
      name: git-sync
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-18T17:30:38Z"
    - containerID: containerd://ab4cf71a249ccc4106c16554a26dbb790112d00a33fd62a56c7ef40e42bb348e
      image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imageID: docker.io/minhtuyenvp02/airflow-spark@sha256:ca7cd2a1efdc4fd9c1ed3569c152759957dd406908ce7007c98390b5fa44f958
      lastState:
        terminated:
          containerID: containerd://15caa02e6a2675cfad76ddde2c42c915b36b23c8d2ffb4e1af55d8ecba2c6e3f
          exitCode: 143
          finishedAt: "2024-07-01T07:38:59Z"
          reason: Error
          startedAt: "2024-07-01T07:34:03Z"
      name: triggerer
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2024-07-01T07:39:01Z"
    - containerID: containerd://cbfdfe55138124f05b25313d582074ef2eacf57861cdde07ef15db60662f479a
      image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imageID: docker.io/minhtuyenvp02/airflow-spark@sha256:ca7cd2a1efdc4fd9c1ed3569c152759957dd406908ce7007c98390b5fa44f958
      lastState: {}
      name: triggerer-log-groomer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-18T17:30:44Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    initContainerStatuses:
    - containerID: containerd://8ea0b0d42d065b1e4e94a83b499aee99f35dc42000b5c26c99bf07c4bea9d747
      image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imageID: docker.io/minhtuyenvp02/airflow-spark@sha256:ca7cd2a1efdc4fd9c1ed3569c152759957dd406908ce7007c98390b5fa44f958
      lastState: {}
      name: wait-for-airflow-migrations
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://8ea0b0d42d065b1e4e94a83b499aee99f35dc42000b5c26c99bf07c4bea9d747
          exitCode: 0
          finishedAt: "2024-06-18T17:30:32Z"
          reason: Completed
          startedAt: "2024-06-18T17:29:33Z"
    - containerID: containerd://26be2f058ed1619c05dd8efddea16db2c58ecab4cbd59fdf88941b0f837feeb1
      image: registry.k8s.io/git-sync/git-sync:v4.1.0
      imageID: registry.k8s.io/git-sync/git-sync@sha256:fd9722fd02e3a559fd6bb4427417c53892068f588fc8372aa553fbf2f05f9902
      lastState: {}
      name: git-sync-init
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://26be2f058ed1619c05dd8efddea16db2c58ecab4cbd59fdf88941b0f837feeb1
          exitCode: 0
          finishedAt: "2024-06-18T17:30:35Z"
          reason: Completed
          startedAt: "2024-06-18T17:30:33Z"
    phase: Running
    podIP: 10.244.0.5
    podIPs:
    - ip: 10.244.0.5
    qosClass: BestEffort
    startTime: "2024-06-18T17:29:17Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/airflow-config: cdff98e8c3f0bebc19523bf34fd5a6eb9ca62c9c556e71201f3b5386fd6127b2
      checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8
      checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827
      checksum/metadata-secret: 1527346545415bf13f4c9ad69470086eb90d854f2b83594d78ec1badb5e13eb0
      checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688
      checksum/webserver-config: 2f3fdfd294a37094d2abee43b2b09888a5c195ee03414996bf99a4681658af94
      checksum/webserver-secret-key: c2e7d741976bcca47534b96b419058c6ee4a482ca2ee5adad27507770148319b
    creationTimestamp: "2024-07-01T07:28:22Z"
    generateName: airflow-webserver-676ff8f7b9-
    labels:
      component: webserver
      pod-template-hash: 676ff8f7b9
      release: airflow
      tier: airflow
    name: airflow-webserver-676ff8f7b9-wgglk
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: airflow-webserver-676ff8f7b9
      uid: a067f510-1d45-47e4-a3ae-1f30b5361634
    resourceVersion: "7818603"
    uid: 86c69a60-3bb1-48b1-a248-7a79f19aeabc
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                component: webserver
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - bash
      - -c
      - exec airflow webserver
      env:
      - name: AIRFLOW_VAR_S3_ACCESS_KEY
        value: admin
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
        value: admin
      - name: AIRFLOW_VAR_S3_SECRET_KEY
        value: admin123
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
        value: admin123
      - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
        value: minhtuyenvp02/trip-generator
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
        value: minhtuyenvp02/trip-generator
      - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
        value: kafka-controller-headless.kafka.svc.cluster.local:9092
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
        value: kafka-controller-headless.kafka.svc.cluster.local:9092
      - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
        value: kafka.kafka.svc.cluster.local:9092
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
        value: kafka.kafka.svc.cluster.local:9092
      - name: AIRFLOW_VAR_TOPIC
        value: yellow_tripdata,fhvhv_tripdata
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
        value: yellow_tripdata,fhvhv_tripdata
      - name: AIRFLOW_VAR_S3_ENDPOINT
        value: http://minio.minio.svc.cluster.local:9000
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
        value: http://minio.minio.svc.cluster.local:9000
      - name: AIRFLOW_VAR_PATH_LOCATION_CSV
        value: s3a://nyc-trip-bucket/nyc-data/location.csv
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
        value: s3a://nyc-trip-bucket/nyc-data/location.csv
      - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
        value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
        value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
      - name: AIRFLOW_VAR_SPARK_CLUSTER
        value: spark://spark-master-svc.spark.svc.cluster.local:7077
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
        value: spark://spark-master-svc.spark.svc.cluster.local:7077
      - name: AIRFLOW_VAR_DATA_DIR
        value: nyc-trip-bucket/nyc-data/2023
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
        value: nyc-trip-bucket/nyc-data/2023
      - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
        value: "200"
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
        value: "200"
      - name: AIRFLOW_VAR_S3_BUCKET_NAME
        value: nyc-trip-bucket
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
        value: nyc-trip-bucket
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /opt/airflow/secrets/key.json
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
        value: /opt/airflow/secrets/key.json
      - name: AIRFLOW_VAR_SLACK_WEB_HOOK
        valueFrom:
          secretKeyRef:
            key: SLACK_WEB_HOOK
            name: webhook-secret
      - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
        value: webhook-secret=SLACK_WEB_HOOK
      - name: AIRFLOW__CORE__FERNET_KEY
        valueFrom:
          secretKeyRef:
            key: fernet-key
            name: airflow-fernet-key
      - name: AIRFLOW_HOME
        value: /opt/airflow
      - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW_CONN_AIRFLOW_DB
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW__WEBSERVER__SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: webserver-secret-key
            name: airflow-webserver-secret-key
      image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: webserver
      ports:
      - containerPort: 8080
        name: airflow-ui
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "3"
          memory: 5Gi
        requests:
          cpu: "2"
          memory: 2Gi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      startupProbe:
        failureThreshold: 6
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/pod_templates/pod_template_file.yaml
        name: config
        readOnly: true
        subPath: pod_template_file.yaml
      - mountPath: /opt/airflow/airflow.cfg
        name: config
        readOnly: true
        subPath: airflow.cfg
      - mountPath: /opt/airflow/config/airflow_local_settings.py
        name: config
        readOnly: true
        subPath: airflow_local_settings.py
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gm9g8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - airflow
      - db
      - check-migrations
      - --migration-wait-timeout=60
      env:
      - name: AIRFLOW_VAR_S3_ACCESS_KEY
        value: admin
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
        value: admin
      - name: AIRFLOW_VAR_S3_SECRET_KEY
        value: admin123
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
        value: admin123
      - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
        value: minhtuyenvp02/trip-generator
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
        value: minhtuyenvp02/trip-generator
      - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
        value: kafka-controller-headless.kafka.svc.cluster.local:9092
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
        value: kafka-controller-headless.kafka.svc.cluster.local:9092
      - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
        value: kafka.kafka.svc.cluster.local:9092
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
        value: kafka.kafka.svc.cluster.local:9092
      - name: AIRFLOW_VAR_TOPIC
        value: yellow_tripdata,fhvhv_tripdata
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
        value: yellow_tripdata,fhvhv_tripdata
      - name: AIRFLOW_VAR_S3_ENDPOINT
        value: http://minio.minio.svc.cluster.local:9000
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
        value: http://minio.minio.svc.cluster.local:9000
      - name: AIRFLOW_VAR_PATH_LOCATION_CSV
        value: s3a://nyc-trip-bucket/nyc-data/location.csv
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
        value: s3a://nyc-trip-bucket/nyc-data/location.csv
      - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
        value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
        value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
      - name: AIRFLOW_VAR_SPARK_CLUSTER
        value: spark://spark-master-svc.spark.svc.cluster.local:7077
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
        value: spark://spark-master-svc.spark.svc.cluster.local:7077
      - name: AIRFLOW_VAR_DATA_DIR
        value: nyc-trip-bucket/nyc-data/2023
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
        value: nyc-trip-bucket/nyc-data/2023
      - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
        value: "200"
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
        value: "200"
      - name: AIRFLOW_VAR_S3_BUCKET_NAME
        value: nyc-trip-bucket
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
        value: nyc-trip-bucket
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /opt/airflow/secrets/key.json
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
        value: /opt/airflow/secrets/key.json
      - name: AIRFLOW_VAR_SLACK_WEB_HOOK
        valueFrom:
          secretKeyRef:
            key: SLACK_WEB_HOOK
            name: webhook-secret
      - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
        value: webhook-secret=SLACK_WEB_HOOK
      - name: AIRFLOW__CORE__FERNET_KEY
        valueFrom:
          secretKeyRef:
            key: fernet-key
            name: airflow-fernet-key
      - name: AIRFLOW_HOME
        value: /opt/airflow
      - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW_CONN_AIRFLOW_DB
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW__WEBSERVER__SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: webserver-secret-key
            name: airflow-webserver-secret-key
      image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imagePullPolicy: Always
      name: wait-for-airflow-migrations
      resources:
        limits:
          cpu: "3"
          memory: 5Gi
        requests:
          cpu: "2"
          memory: 2Gi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/airflow.cfg
        name: config
        readOnly: true
        subPath: airflow.cfg
      - mountPath: /opt/airflow/config/airflow_local_settings.py
        name: config
        readOnly: true
        subPath: airflow_local_settings.py
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gm9g8
        readOnly: true
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 0
      runAsUser: 50000
    serviceAccount: airflow-webserver
    serviceAccountName: airflow-webserver
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: airflow-config
      name: config
    - name: kube-api-access-gm9g8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:22Z"
      message: '0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes
        are available: 2 No preemption victims found for incoming pod.'
      reason: Unschedulable
      status: "False"
      type: PodScheduled
    phase: Pending
    qosClass: Burstable
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      dag_id: trip_streaming_kafka
      run_id: scheduled__2024-06-26T00:00:00+00:00
      task_id: kafka_streaming.kafka_fhvhv_trip_producer
      try_number: "1"
    creationTimestamp: "2024-06-28T05:34:39Z"
    labels:
      airflow-worker: "333"
      airflow_version: 2.9.1
      component: worker
      dag_id: trip_streaming_kafka
      kubernetes_executor: "True"
      release: airflow
      run_id: scheduled__2024-06-26T0000000000-4dd9190cf
      task_id: kafka_streaming.kafka_fhvhv_trip_producer
      tier: airflow
      try_number: "1"
    name: trip-streaming-kafka-kafka-streaming-kafka-fhvhv-trip-mlckf3zt
    namespace: airflow
    resourceVersion: "6928774"
    uid: 63e6cf60-d967-495c-b42e-8d58ec901153
  spec:
    affinity: {}
    containers:
    - args:
      - airflow
      - tasks
      - run
      - trip_streaming_kafka
      - kafka_streaming.kafka_fhvhv_trip_producer
      - scheduled__2024-06-26T00:00:00+00:00
      - --local
      - --subdir
      - DAGS_FOLDER/trip_generation_dag.py
      env:
      - name: AIRFLOW__CORE__EXECUTOR
        value: LocalExecutor
      - name: AIRFLOW__CORE__FERNET_KEY
        valueFrom:
          secretKeyRef:
            key: fernet-key
            name: airflow-fernet-key
      - name: AIRFLOW_HOME
        value: /opt/airflow
      - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW_CONN_AIRFLOW_DB
        valueFrom:
          secretKeyRef:
            key: connection
            name: airflow-metadata
      - name: AIRFLOW__WEBSERVER__SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: webserver-secret-key
            name: airflow-webserver-secret-key
      - name: AIRFLOW_VAR_S3_ACCESS_KEY
        value: admin
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
        value: admin
      - name: AIRFLOW_VAR_S3_SECRET_KEY
        value: admin123
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
        value: admin123
      - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
        value: minhtuyenvp02/trip-generator
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
        value: minhtuyenvp02/trip-generator
      - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
        value: kafka-controller-headless.kafka.svc.cluster.local:9092
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
        value: kafka-controller-headless.kafka.svc.cluster.local:9092
      - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
        value: kafka.kafka.svc.cluster.local:9092
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
        value: kafka.kafka.svc.cluster.local:9092
      - name: AIRFLOW_VAR_TOPIC
        value: yellow_tripdata,fhvhv_tripdata
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
        value: yellow_tripdata,fhvhv_tripdata
      - name: AIRFLOW_VAR_S3_ENDPOINT
        value: http://minio.minio.svc.cluster.local:9000
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
        value: http://minio.minio.svc.cluster.local:9000
      - name: AIRFLOW_VAR_PATH_LOCATION_CSV
        value: s3a://nyc-trip-bucket/nyc-data/location.csv
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
        value: s3a://nyc-trip-bucket/nyc-data/location.csv
      - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
        value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
        value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
      - name: AIRFLOW_VAR_SPARK_CLUSTER
        value: spark://spark-master-svc.spark.svc.cluster.local:7077
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
        value: spark://spark-master-svc.spark.svc.cluster.local:7077
      - name: AIRFLOW_VAR_DATA_DIR
        value: nyc-trip-bucket/nyc-data/2023
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
        value: nyc-trip-bucket/nyc-data/2023
      - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
        value: "200"
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
        value: "200"
      - name: AIRFLOW_VAR_S3_BUCKET_NAME
        value: nyc-trip-bucket
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
        value: nyc-trip-bucket
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /opt/airflow/secrets/key.json
      - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
        value: /opt/airflow/secrets/key.json
      - name: AIRFLOW_VAR_SLACK_WEB_HOOK
        valueFrom:
          secretKeyRef:
            key: SLACK_WEB_HOOK
            name: webhook-secret
      - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
        value: webhook-secret=SLACK_WEB_HOOK
      - name: AIRFLOW_IS_K8S_EXECUTOR_POD
        value: "True"
      image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imagePullPolicy: IfNotPresent
      name: base
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/airflow/logs
        name: logs
      - mountPath: /opt/airflow/airflow.cfg
        name: config
        readOnly: true
        subPath: airflow.cfg
      - mountPath: /opt/airflow/config/airflow_local_settings.py
        name: config
        readOnly: true
        subPath: airflow_local_settings.py
      - mountPath: /opt/airflow/dags
        name: dags
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9v6dj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - env:
      - name: GIT_SSH_KEY_FILE
        value: /etc/git-secret/ssh
      - name: GITSYNC_SSH_KEY_FILE
        value: /etc/git-secret/ssh
      - name: GIT_SYNC_SSH
        value: "true"
      - name: GITSYNC_SSH
        value: "true"
      - name: GIT_KNOWN_HOSTS
        value: "false"
      - name: GITSYNC_SSH_KNOWN_HOSTS
        value: "false"
      - name: GIT_SYNC_REV
        value: HEAD
      - name: GITSYNC_REF
        value: v2-2-stable
      - name: GIT_SYNC_BRANCH
        value: main
      - name: GIT_SYNC_REPO
        value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
      - name: GITSYNC_REPO
        value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
      - name: GIT_SYNC_DEPTH
        value: "1"
      - name: GITSYNC_DEPTH
        value: "1"
      - name: GIT_SYNC_ROOT
        value: /git
      - name: GITSYNC_ROOT
        value: /git
      - name: GIT_SYNC_DEST
        value: repo
      - name: GITSYNC_LINK
        value: repo
      - name: GIT_SYNC_ADD_USER
        value: "true"
      - name: GITSYNC_ADD_USER
        value: "true"
      - name: GITSYNC_PERIOD
        value: 5s
      - name: GIT_SYNC_MAX_SYNC_FAILURES
        value: "0"
      - name: GITSYNC_MAX_FAILURES
        value: "0"
      - name: GIT_SYNC_ONE_TIME
        value: "true"
      - name: GITSYNC_ONE_TIME
        value: "true"
      image: registry.k8s.io/git-sync/git-sync:v4.1.0
      imagePullPolicy: IfNotPresent
      name: git-sync-init
      resources: {}
      securityContext:
        runAsUser: 65533
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /git
        name: dags
      - mountPath: /etc/git-secret/ssh
        name: git-sync-ssh-key
        readOnly: true
        subPath: gitSshKey
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9v6dj
        readOnly: true
    nodeName: pool-1em8noa5l-rwr1p
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 0
      runAsUser: 50000
    serviceAccount: airflow-worker
    serviceAccountName: airflow-worker
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: dags
    - emptyDir: {}
      name: logs
    - name: git-sync-ssh-key
      secret:
        defaultMode: 288
        secretName: airflow-ssh-secret
    - configMap:
        defaultMode: 420
        name: airflow-config
      name: config
    - name: kube-api-access-9v6dj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T23:41:50Z"
      message: 'The node was low on resource: memory. Threshold quantity: 100Mi, available:
        3732Ki. Container base was using 382476Ki, request is 0, has larger consumption
        of memory. '
      reason: TerminationByKubelet
      status: "True"
      type: DisruptionTarget
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T23:41:50Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T05:34:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T23:41:50Z"
      reason: PodFailed
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T23:41:50Z"
      reason: PodFailed
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T05:34:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: docker.io/minhtuyenvp02/airflow-spark:pr-49
      imageID: ""
      lastState:
        terminated:
          exitCode: 137
          finishedAt: null
          message: The container could not be located when the pod was deleted.  The
            container used to be Running
          reason: ContainerStatusUnknown
          startedAt: null
      name: base
      ready: false
      restartCount: 1
      started: false
      state:
        terminated:
          exitCode: 137
          finishedAt: null
          message: The container could not be located when the pod was terminated
          reason: ContainerStatusUnknown
          startedAt: null
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    initContainerStatuses:
    - containerID: containerd://3685d43f7d91aa271119cdd4ba1fe3d96058230791883cf54d0fc9a86482d7ec
      image: registry.k8s.io/git-sync/git-sync:v4.1.0
      imageID: registry.k8s.io/git-sync/git-sync@sha256:fd9722fd02e3a559fd6bb4427417c53892068f588fc8372aa553fbf2f05f9902
      lastState: {}
      name: git-sync-init
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://3685d43f7d91aa271119cdd4ba1fe3d96058230791883cf54d0fc9a86482d7ec
          exitCode: 0
          finishedAt: "2024-06-28T05:34:41Z"
          reason: Completed
          startedAt: "2024-06-28T05:34:40Z"
    message: 'The node was low on resource: memory. Threshold quantity: 100Mi, available:
      3732Ki. Container base was using 382476Ki, request is 0, has larger consumption
      of memory. '
    phase: Failed
    podIP: 10.244.26.4
    podIPs:
    - ip: 10.244.26.4
    qosClass: BestEffort
    reason: Evicted
    startTime: "2024-06-28T05:34:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-06-14T20:21:28Z"
    generateName: ingress-ingress-nginx-controller-6b4df77c7b-
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.10.1
      helm.sh/chart: ingress-nginx-4.10.1
      pod-template-hash: 6b4df77c7b
    name: ingress-ingress-nginx-controller-6b4df77c7b-b5rcx
    namespace: ingress
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: ingress-ingress-nginx-controller-6b4df77c7b
      uid: 534442d0-7fe1-48d6-8e76-7d2c69cb23ec
    resourceVersion: "5341029"
    uid: c55162fa-fa14-4282-92e2-85973ac1dd95
  spec:
    containers:
    - args:
      - /nginx-ingress-controller
      - --publish-service=$(POD_NAMESPACE)/ingress-ingress-nginx-controller
      - --election-id=ingress-ingress-nginx-leader
      - --controller-class=k8s.io/ingress-nginx
      - --ingress-class=nginx
      - --configmap=$(POD_NAMESPACE)/ingress-ingress-nginx-controller
      - --validating-webhook=:8443
      - --validating-webhook-certificate=/usr/local/certificates/cert
      - --validating-webhook-key=/usr/local/certificates/key
      - --enable-metrics=false
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_PRELOAD
        value: /usr/local/lib/libmimalloc.so
      image: registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /wait-shutdown
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: controller
      ports:
      - containerPort: 80
        hostPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        hostPort: 443
        name: https
        protocol: TCP
      - containerPort: 8443
        hostPort: 8443
        name: webhook
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 90Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 101
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/local/certificates/
        name: webhook-cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f2jnj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: pool-1em8noa5l-rwr1p
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ingress-ingress-nginx
    serviceAccountName: ingress-ingress-nginx
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: webhook-cert
      secret:
        defaultMode: 420
        secretName: ingress-ingress-nginx-admission
    - name: kube-api-access-f2jnj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T20:21:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T20:21:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T11:37:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T11:37:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T20:21:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0fed12e278bb0522b314f194fff9762472433247202b0e16c840ffbf7dd31358
      image: sha256:ee54966f3891d75b255d160236368a4f9d3b588d32fb44bd04aea5101143e829
      imageID: registry.k8s.io/ingress-nginx/controller@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e
      lastState: {}
      name: controller
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-14T20:21:29Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    phase: Running
    podIP: 10.104.0.3
    podIPs:
    - ip: 10.104.0.3
    qosClass: Burstable
    startTime: "2024-06-14T20:21:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configuration: bc0132febe701d72fa3f24a7edd2bdcb339930dab72a2c941e8523a833f64cc0
      checksum/jmx-configuration: 7d1557a23f87db78f1d531f35aaaff742125fbf3dd218346346b5c707cd8d6e1
    creationTimestamp: "2024-07-01T07:28:29Z"
    generateName: kafka-controller-
    labels:
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
      app.kubernetes.io/version: 3.7.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: kafka-controller-7c45646788
      helm.sh/chart: kafka-29.3.2
      statefulset.kubernetes.io/pod-name: kafka-controller-0
    name: kafka-controller-0
    namespace: kafka
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: kafka-controller
      uid: a173ae5e-dc2b-4807-80c6-27fec8d2b4cf
    resourceVersion: "7818607"
    uid: c27e8a39-5f75-4157-8c37-d9ccf2c1477f
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: controller-eligible
                app.kubernetes.io/instance: kafka
                app.kubernetes.io/name: kafka
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: true
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: KAFKA_HEAP_OPTS
        value: -Xmx1024m -Xms1024m
      - name: KAFKA_KRAFT_CLUSTER_ID
        valueFrom:
          secretKeyRef:
            key: kraft-cluster-id
            name: kafka-kraft-cluster-id
      - name: JMX_PORT
        value: "5555"
      image: docker.io/bitnami/kafka:3.6.1-debian-11-r6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - pgrep
          - -f
          - kafka
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kafka
      ports:
      - containerPort: 9093
        name: controller
        protocol: TCP
      - containerPort: 9092
        name: client
        protocol: TCP
      - containerPort: 9094
        name: interbroker
        protocol: TCP
      readinessProbe:
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: controller
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "2"
          memory: 1500Mi
        requests:
          cpu: "1"
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/kafka
        name: data
      - mountPath: /opt/bitnami/kafka/logs
        name: logs
      - mountPath: /opt/bitnami/kafka/config/server.properties
        name: kafka-config
        subPath: server.properties
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-965vr
        readOnly: true
    - args:
      - -XX:MaxRAMPercentage=100
      - -XshowSettings:vm
      - -jar
      - jmx_prometheus_httpserver.jar
      - "5556"
      - /etc/jmx-kafka/jmx-kafka-prometheus.yml
      command:
      - java
      image: docker.io/bitnami/jmx-exporter:0.20.0-debian-11-r6
      imagePullPolicy: IfNotPresent
      name: jmx-exporter
      ports:
      - containerPort: 5556
        name: metrics
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/jmx-kafka
        name: jmx-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-965vr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: kafka-controller-0
    initContainers:
    - args:
      - -ec
      - |
        mkdir -p "/bitnami/kafka" "/opt/bitnami/kafka/logs"
        chown -R 1001:1001 "/bitnami/kafka" "/opt/bitnami/kafka/logs"
        find "/bitnami/kafka" -mindepth 1 -maxdepth 1 -not -name ".snapshot" -not -name "lost+found" | xargs -r chown -R 1001:1001
        find "/opt/bitnami/kafka/logs" -mindepth 1 -maxdepth 1 -not -name ".snapshot" -not -name "lost+found" | xargs -r chown -R 1001:1001
      command:
      - /bin/bash
      image: docker.io/bitnami/os-shell:11-debian-11-r96
      imagePullPolicy: IfNotPresent
      name: volume-permissions
      resources: {}
      securityContext:
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/kafka
        name: data
      - mountPath: /opt/bitnami/kafka/logs
        name: logs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-965vr
        readOnly: true
    - args:
      - -ec
      - |
        /scripts/kafka-init.sh
      command:
      - /bin/bash
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: KAFKA_VOLUME_DIR
        value: /bitnami/kafka
      - name: KAFKA_MIN_ID
        value: "0"
      image: docker.io/bitnami/kafka:3.6.1-debian-11-r6
      imagePullPolicy: IfNotPresent
      name: kafka-init
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/kafka
        name: data
      - mountPath: /config
        name: kafka-config
      - mountPath: /configmaps
        name: kafka-configmaps
      - mountPath: /secret-config
        name: kafka-secret-config
      - mountPath: /scripts
        name: scripts
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-965vr
        readOnly: true
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kafka
    serviceAccountName: kafka
    subdomain: kafka-controller-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-kafka-controller-0
    - configMap:
        defaultMode: 420
        name: kafka-controller-configuration
      name: kafka-configmaps
    - emptyDir: {}
      name: kafka-secret-config
    - emptyDir: {}
      name: kafka-config
    - emptyDir: {}
      name: tmp
    - configMap:
        defaultMode: 493
        name: kafka-scripts
      name: scripts
    - configMap:
        defaultMode: 420
        name: kafka-jmx-configuration
      name: jmx-config
    - emptyDir: {}
      name: logs
    - name: kube-api-access-965vr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:29Z"
      message: '0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes
        are available: 2 No preemption victims found for incoming pod.'
      reason: Unschedulable
      status: "False"
      type: PodScheduled
    phase: Pending
    qosClass: Burstable
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configuration: bc0132febe701d72fa3f24a7edd2bdcb339930dab72a2c941e8523a833f64cc0
      checksum/jmx-configuration: 7d1557a23f87db78f1d531f35aaaff742125fbf3dd218346346b5c707cd8d6e1
    creationTimestamp: "2024-06-28T12:26:43Z"
    generateName: kafka-controller-
    labels:
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
      app.kubernetes.io/version: 3.7.0
      apps.kubernetes.io/pod-index: "1"
      controller-revision-hash: kafka-controller-7c45646788
      helm.sh/chart: kafka-29.3.2
      statefulset.kubernetes.io/pod-name: kafka-controller-1
    name: kafka-controller-1
    namespace: kafka
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: kafka-controller
      uid: a173ae5e-dc2b-4807-80c6-27fec8d2b4cf
    resourceVersion: "6754981"
    uid: 2fc481b9-0980-4c1e-96ee-bdc77e9782ed
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: controller-eligible
                app.kubernetes.io/instance: kafka
                app.kubernetes.io/name: kafka
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: true
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: KAFKA_HEAP_OPTS
        value: -Xmx1024m -Xms1024m
      - name: KAFKA_KRAFT_CLUSTER_ID
        valueFrom:
          secretKeyRef:
            key: kraft-cluster-id
            name: kafka-kraft-cluster-id
      - name: JMX_PORT
        value: "5555"
      image: docker.io/bitnami/kafka:3.6.1-debian-11-r6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - pgrep
          - -f
          - kafka
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kafka
      ports:
      - containerPort: 9093
        name: controller
        protocol: TCP
      - containerPort: 9092
        name: client
        protocol: TCP
      - containerPort: 9094
        name: interbroker
        protocol: TCP
      readinessProbe:
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: controller
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "2"
          memory: 1500Mi
        requests:
          cpu: "1"
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/kafka
        name: data
      - mountPath: /opt/bitnami/kafka/logs
        name: logs
      - mountPath: /opt/bitnami/kafka/config/server.properties
        name: kafka-config
        subPath: server.properties
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5z558
        readOnly: true
    - args:
      - -XX:MaxRAMPercentage=100
      - -XshowSettings:vm
      - -jar
      - jmx_prometheus_httpserver.jar
      - "5556"
      - /etc/jmx-kafka/jmx-kafka-prometheus.yml
      command:
      - java
      image: docker.io/bitnami/jmx-exporter:0.20.0-debian-11-r6
      imagePullPolicy: IfNotPresent
      name: jmx-exporter
      ports:
      - containerPort: 5556
        name: metrics
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/jmx-kafka
        name: jmx-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5z558
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: kafka-controller-1
    initContainers:
    - args:
      - -ec
      - |
        mkdir -p "/bitnami/kafka" "/opt/bitnami/kafka/logs"
        chown -R 1001:1001 "/bitnami/kafka" "/opt/bitnami/kafka/logs"
        find "/bitnami/kafka" -mindepth 1 -maxdepth 1 -not -name ".snapshot" -not -name "lost+found" | xargs -r chown -R 1001:1001
        find "/opt/bitnami/kafka/logs" -mindepth 1 -maxdepth 1 -not -name ".snapshot" -not -name "lost+found" | xargs -r chown -R 1001:1001
      command:
      - /bin/bash
      image: docker.io/bitnami/os-shell:11-debian-11-r96
      imagePullPolicy: IfNotPresent
      name: volume-permissions
      resources: {}
      securityContext:
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/kafka
        name: data
      - mountPath: /opt/bitnami/kafka/logs
        name: logs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5z558
        readOnly: true
    - args:
      - -ec
      - |
        /scripts/kafka-init.sh
      command:
      - /bin/bash
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: KAFKA_VOLUME_DIR
        value: /bitnami/kafka
      - name: KAFKA_MIN_ID
        value: "0"
      image: docker.io/bitnami/kafka:3.6.1-debian-11-r6
      imagePullPolicy: IfNotPresent
      name: kafka-init
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/kafka
        name: data
      - mountPath: /config
        name: kafka-config
      - mountPath: /configmaps
        name: kafka-configmaps
      - mountPath: /secret-config
        name: kafka-secret-config
      - mountPath: /scripts
        name: scripts
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5z558
        readOnly: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kafka
    serviceAccountName: kafka
    subdomain: kafka-controller-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-kafka-controller-1
    - configMap:
        defaultMode: 420
        name: kafka-controller-configuration
      name: kafka-configmaps
    - emptyDir: {}
      name: kafka-secret-config
    - emptyDir: {}
      name: kafka-config
    - emptyDir: {}
      name: tmp
    - configMap:
        defaultMode: 493
        name: kafka-scripts
      name: scripts
    - configMap:
        defaultMode: 420
        name: kafka-jmx-configuration
      name: jmx-config
    - emptyDir: {}
      name: logs
    - name: kube-api-access-5z558
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T12:26:45Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T12:26:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T12:27:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T12:27:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T12:26:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c10543f093b034f1f5b297e8db1f9afb5c332d466e613c46486bb338c21f991d
      image: docker.io/bitnami/jmx-exporter:0.20.0-debian-11-r6
      imageID: docker.io/bitnami/jmx-exporter@sha256:03ca35b20abc7775517103301a244cbab1eb9057044bfbc9f1f5f51d891e9265
      lastState: {}
      name: jmx-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-28T12:26:47Z"
    - containerID: containerd://e954889b20692beaba70de7a5a771b08ddfecfa8351ab4a58e16fc1ea2c79bdb
      image: docker.io/bitnami/kafka:3.6.1-debian-11-r6
      imageID: docker.io/bitnami/kafka@sha256:c0c40b6c652759b7fdfeef85387a122e2d0b6e8a8a92efcb9d945f4218852ffc
      lastState: {}
      name: kafka
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-28T12:26:46Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    initContainerStatuses:
    - containerID: containerd://5b332feecfc2282024c39416570278855a2e9868224e45de7b44074b5f7c8d89
      image: docker.io/bitnami/os-shell:11-debian-11-r96
      imageID: docker.io/bitnami/os-shell@sha256:8643af4facffb20e14d135b9e5bdae7f4e604db1e755896f60ea55d05ca09287
      lastState: {}
      name: volume-permissions
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://5b332feecfc2282024c39416570278855a2e9868224e45de7b44074b5f7c8d89
          exitCode: 0
          finishedAt: "2024-06-28T12:26:44Z"
          reason: Completed
          startedAt: "2024-06-28T12:26:44Z"
    - containerID: containerd://8998426dfaa0315bd226def2dda427072088f80d46fd94f5801ba8e030444b97
      image: docker.io/bitnami/kafka:3.6.1-debian-11-r6
      imageID: docker.io/bitnami/kafka@sha256:c0c40b6c652759b7fdfeef85387a122e2d0b6e8a8a92efcb9d945f4218852ffc
      lastState: {}
      name: kafka-init
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://8998426dfaa0315bd226def2dda427072088f80d46fd94f5801ba8e030444b97
          exitCode: 0
          finishedAt: "2024-06-28T12:26:45Z"
          reason: Completed
          startedAt: "2024-06-28T12:26:45Z"
    phase: Running
    podIP: 10.244.0.42
    podIPs:
    - ip: 10.244.0.42
    qosClass: Burstable
    startTime: "2024-06-28T12:26:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configuration: bc0132febe701d72fa3f24a7edd2bdcb339930dab72a2c941e8523a833f64cc0
      checksum/jmx-configuration: 7d1557a23f87db78f1d531f35aaaff742125fbf3dd218346346b5c707cd8d6e1
    creationTimestamp: "2024-06-14T21:18:48Z"
    generateName: kafka-controller-
    labels:
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
      app.kubernetes.io/version: 3.7.0
      apps.kubernetes.io/pod-index: "2"
      controller-revision-hash: kafka-controller-7c45646788
      helm.sh/chart: kafka-29.3.2
      statefulset.kubernetes.io/pod-name: kafka-controller-2
    name: kafka-controller-2
    namespace: kafka
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: kafka-controller
      uid: a173ae5e-dc2b-4807-80c6-27fec8d2b4cf
    resourceVersion: "1864312"
    uid: d5dfc684-7bfd-4305-bee9-1e629453af8c
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: controller-eligible
                app.kubernetes.io/instance: kafka
                app.kubernetes.io/name: kafka
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: true
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: KAFKA_HEAP_OPTS
        value: -Xmx1024m -Xms1024m
      - name: KAFKA_KRAFT_CLUSTER_ID
        valueFrom:
          secretKeyRef:
            key: kraft-cluster-id
            name: kafka-kraft-cluster-id
      - name: JMX_PORT
        value: "5555"
      image: docker.io/bitnami/kafka:3.6.1-debian-11-r6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - pgrep
          - -f
          - kafka
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kafka
      ports:
      - containerPort: 9093
        name: controller
        protocol: TCP
      - containerPort: 9092
        name: client
        protocol: TCP
      - containerPort: 9094
        name: interbroker
        protocol: TCP
      readinessProbe:
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: controller
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "2"
          memory: 1500Mi
        requests:
          cpu: "1"
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/kafka
        name: data
      - mountPath: /opt/bitnami/kafka/logs
        name: logs
      - mountPath: /opt/bitnami/kafka/config/server.properties
        name: kafka-config
        subPath: server.properties
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-57x57
        readOnly: true
    - args:
      - -XX:MaxRAMPercentage=100
      - -XshowSettings:vm
      - -jar
      - jmx_prometheus_httpserver.jar
      - "5556"
      - /etc/jmx-kafka/jmx-kafka-prometheus.yml
      command:
      - java
      image: docker.io/bitnami/jmx-exporter:0.20.0-debian-11-r6
      imagePullPolicy: IfNotPresent
      name: jmx-exporter
      ports:
      - containerPort: 5556
        name: metrics
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/jmx-kafka
        name: jmx-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-57x57
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: kafka-controller-2
    initContainers:
    - args:
      - -ec
      - |
        mkdir -p "/bitnami/kafka" "/opt/bitnami/kafka/logs"
        chown -R 1001:1001 "/bitnami/kafka" "/opt/bitnami/kafka/logs"
        find "/bitnami/kafka" -mindepth 1 -maxdepth 1 -not -name ".snapshot" -not -name "lost+found" | xargs -r chown -R 1001:1001
        find "/opt/bitnami/kafka/logs" -mindepth 1 -maxdepth 1 -not -name ".snapshot" -not -name "lost+found" | xargs -r chown -R 1001:1001
      command:
      - /bin/bash
      image: docker.io/bitnami/os-shell:11-debian-11-r96
      imagePullPolicy: IfNotPresent
      name: volume-permissions
      resources: {}
      securityContext:
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/kafka
        name: data
      - mountPath: /opt/bitnami/kafka/logs
        name: logs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-57x57
        readOnly: true
    - args:
      - -ec
      - |
        /scripts/kafka-init.sh
      command:
      - /bin/bash
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: KAFKA_VOLUME_DIR
        value: /bitnami/kafka
      - name: KAFKA_MIN_ID
        value: "0"
      image: docker.io/bitnami/kafka:3.6.1-debian-11-r6
      imagePullPolicy: IfNotPresent
      name: kafka-init
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/kafka
        name: data
      - mountPath: /config
        name: kafka-config
      - mountPath: /configmaps
        name: kafka-configmaps
      - mountPath: /secret-config
        name: kafka-secret-config
      - mountPath: /scripts
        name: scripts
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-57x57
        readOnly: true
    nodeName: pool-1em8noa5l-rwr1p
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kafka
    serviceAccountName: kafka
    subdomain: kafka-controller-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-kafka-controller-2
    - configMap:
        defaultMode: 420
        name: kafka-controller-configuration
      name: kafka-configmaps
    - emptyDir: {}
      name: kafka-secret-config
    - emptyDir: {}
      name: kafka-config
    - emptyDir: {}
      name: tmp
    - configMap:
        defaultMode: 493
        name: kafka-scripts
      name: scripts
    - configMap:
        defaultMode: 420
        name: kafka-jmx-configuration
      name: jmx-config
    - emptyDir: {}
      name: logs
    - name: kube-api-access-57x57
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T21:18:52Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T21:18:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T21:19:11Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T21:19:11Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T21:18:48Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://fb16930d7e7313e2af7742711649ee758ff56cfcb9a6d32195d1188c16bfbf3c
      image: docker.io/bitnami/jmx-exporter:0.20.0-debian-11-r6
      imageID: docker.io/bitnami/jmx-exporter@sha256:03ca35b20abc7775517103301a244cbab1eb9057044bfbc9f1f5f51d891e9265
      lastState: {}
      name: jmx-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-14T21:18:54Z"
    - containerID: containerd://82af5998ad3b8cd4641f452b05964a7d5102a2fefbb7083aec8733e40fd6585c
      image: docker.io/bitnami/kafka:3.6.1-debian-11-r6
      imageID: docker.io/bitnami/kafka@sha256:c0c40b6c652759b7fdfeef85387a122e2d0b6e8a8a92efcb9d945f4218852ffc
      lastState: {}
      name: kafka
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-14T21:18:53Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    initContainerStatuses:
    - containerID: containerd://ebe6eeee368c4116805994090f5c4d9a0d122ef824d242564770002c32645e33
      image: docker.io/bitnami/os-shell:11-debian-11-r96
      imageID: docker.io/bitnami/os-shell@sha256:8643af4facffb20e14d135b9e5bdae7f4e604db1e755896f60ea55d05ca09287
      lastState: {}
      name: volume-permissions
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://ebe6eeee368c4116805994090f5c4d9a0d122ef824d242564770002c32645e33
          exitCode: 0
          finishedAt: "2024-06-14T21:18:52Z"
          reason: Completed
          startedAt: "2024-06-14T21:18:52Z"
    - containerID: containerd://54fbf3cdefc61df598bc4b13f4c98088152506580c471a29119440c611d6caf8
      image: docker.io/bitnami/kafka:3.6.1-debian-11-r6
      imageID: docker.io/bitnami/kafka@sha256:c0c40b6c652759b7fdfeef85387a122e2d0b6e8a8a92efcb9d945f4218852ffc
      lastState: {}
      name: kafka-init
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://54fbf3cdefc61df598bc4b13f4c98088152506580c471a29119440c611d6caf8
          exitCode: 0
          finishedAt: "2024-06-14T21:18:52Z"
          reason: Completed
          startedAt: "2024-06-14T21:18:52Z"
    phase: Running
    podIP: 10.244.26.62
    podIPs:
    - ip: 10.244.26.62
    qosClass: Burstable
    startTime: "2024-06-14T21:18:48Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-06-16T09:10:46Z"
    generateName: kafka-metrics-prometheus-kafka-exporter-86b48d7b76-
    labels:
      app: prometheus-kafka-exporter
      pod-template-hash: 86b48d7b76
      release: kafka-metrics
    name: kafka-metrics-prometheus-kafka-exporter-86b48d7b76-7cn8g
    namespace: kafka
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kafka-metrics-prometheus-kafka-exporter-86b48d7b76
      uid: f8fc8e47-6bfa-4a37-b041-623666a497e4
    resourceVersion: "2398533"
    uid: abc94df8-d209-44c8-b382-5c8090c3ee0f
  spec:
    containers:
    - args:
      - --verbosity=0
      - --kafka.server=kafka-controller-headless.kafka.svc.cluster.local:9092
      - --kafka.version=3.6.1
      image: danielqsj/kafka-exporter:v1.7.0
      imagePullPolicy: IfNotPresent
      name: prometheus-kafka-exporter
      ports:
      - containerPort: 9308
        name: exporter-port
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wn6qq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pool-1em8noa5l-rwr1p
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kafka-metrics-prometheus-kafka-exporter
    serviceAccountName: kafka-metrics-prometheus-kafka-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-wn6qq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T09:10:47Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T09:10:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T09:10:47Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T09:10:47Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T09:10:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://494602b0547053072d2e2b0898d67a3b21134b44097b6247f5ed238eb0dd45da
      image: docker.io/danielqsj/kafka-exporter:v1.7.0
      imageID: docker.io/danielqsj/kafka-exporter@sha256:e90b7ba06d97717224d67774fc7f208c26d1c4dafd4c8011185b0d45b5b38ace
      lastState: {}
      name: prometheus-kafka-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-16T09:10:47Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    phase: Running
    podIP: 10.244.26.15
    podIPs:
    - ip: 10.244.26.15
    qosClass: BestEffort
    startTime: "2024-06-16T09:10:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
      container.apparmor.security.beta.kubernetes.io/apply-sysctl-overwrites: unconfined
      container.apparmor.security.beta.kubernetes.io/cilium-agent: unconfined
      container.apparmor.security.beta.kubernetes.io/clean-cilium-state: unconfined
      container.apparmor.security.beta.kubernetes.io/mount-cgroup: unconfined
      kubectl.kubernetes.io/default-container: cilium-agent
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-06-15T00:44:39Z"
    generateName: cilium-
    labels:
      app.kubernetes.io/name: cilium-agent
      app.kubernetes.io/part-of: cilium
      controller-revision-hash: f76cc49b
      doks.digitalocean.com/managed: "true"
      k8s-app: cilium
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "1"
    name: cilium-kqn4j
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cilium
      uid: 338cdf62-638a-4469-8517-3c4fe503a2e8
    resourceVersion: "1921223"
    uid: 79fc2db1-0318-4d64-8fe1-5ac6b17183c3
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pool-1em8noa5l-rw5at
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              k8s-app: cilium
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: true
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      - --k8s-api-server=https://f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
      - --ipv4-native-routing-cidr=10.244.0.0/16
      command:
      - cilium-agent
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_CLUSTERMESH_CONFIG
        value: /var/lib/cilium/clustermesh/
      - name: KUBERNETES_SERVICE_HOST
        value: f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            command:
            - bash
            - -c
            - |
              set -o errexit
              set -o pipefail
              set -o nounset

              # When running in AWS ENI mode, it's likely that 'aws-node' has
              # had a chance to install SNAT iptables rules. These can result
              # in dropped traffic, so we should attempt to remove them.
              # We do it using a 'postStart' hook since this may need to run
              # for nodes which might have already been init'ed but may still
              # have dangling rules. This is safe because there are no
              # dependencies on anything that is part of the startup script
              # itself, and can be safely run multiple times per node (e.g. in
              # case of a restart).
              if [[ "$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')" != "0" ]];
              then
                  echo 'Deleting iptables rules created by the AWS CNI VPC plugin'
                  iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore
              fi
              echo 'Done!'
        preStop:
          exec:
            command:
            - /cni-uninstall.sh
      livenessProbe:
        failureThreshold: 10
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        initialDelaySeconds: 120
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: cilium-agent
      ports:
      - containerPort: 4244
        hostPort: 4244
        name: peer-service
        protocol: TCP
      - containerPort: 9090
        hostPort: 9090
        name: prometheus
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 300m
          memory: 300Mi
      securityContext:
        capabilities:
          add:
          - CHOWN
          - KILL
          - NET_ADMIN
          - NET_RAW
          - IPC_LOCK
          - SYS_MODULE
          - SYS_ADMIN
          - SYS_RESOURCE
          - DAC_OVERRIDE
          - FOWNER
          - SETGID
          - SETUID
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      startupProbe:
        failureThreshold: 105
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /host/proc/sys/net
        name: host-proc-sys-net
      - mountPath: /host/proc/sys/kernel
        name: host-proc-sys-kernel
      - mountPath: /sys/fs/bpf
        mountPropagation: HostToContainer
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/lib/cilium/tls/hubble
        name: hubble-tls
        readOnly: true
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6qjlk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - bash
      - -e
      - -c
      - |
        # This will get the node object for the local node and search through
        # the assigned addresses in the object in order to check whether CCM
        # already set the internal AND external IP since cilium needs both
        # for a clean startup.
        # The grep matches regardless of the order of IPs.
        until /host/usr/bin/kubectl get node ${HOSTNAME} -o jsonpath="{.status.addresses[*].type}" | grep -E "InternalIP.*ExternalIP|ExternalIP.*InternalIP"; do echo "waiting for CCM to store internal and external IP addresses in node object: ${HOSTNAME}" && sleep 3; done;
      env:
      - name: KUBERNETES_SERVICE_HOST
        value: f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      name: delay-cilium-for-ccm
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/usr/bin/kubectl
        name: host-kubectl
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6qjlk
        readOnly: true
    - command:
      - cilium
      - build-config
      - --source=config-map:cilium-config
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: KUBERNETES_SERVICE_HOST
        value: f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      name: config
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6qjlk
        readOnly: true
    - command:
      - sh
      - -ec
      - |
        cp /usr/bin/cilium-mount /hostbin/cilium-mount;
        nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
        rm /hostbin/cilium-mount
      env:
      - name: CGROUP_ROOT
        value: /run/cilium/cgroupv2
      - name: BIN_PATH
        value: /opt/cni/bin
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      name: mount-cgroup
      resources: {}
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
          - SYS_CHROOT
          - SYS_PTRACE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6qjlk
        readOnly: true
    - command:
      - sh
      - -ec
      - |
        cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;
        nsenter --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-sysctlfix";
        rm /hostbin/cilium-sysctlfix
      env:
      - name: BIN_PATH
        value: /opt/cni/bin
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      name: apply-sysctl-overwrites
      resources: {}
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
          - SYS_CHROOT
          - SYS_PTRACE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6qjlk
        readOnly: true
    - args:
      - mount | grep "/sys/fs/bpf type bpf" || mount -t bpf bpf /sys/fs/bpf
      command:
      - /bin/bash
      - -c
      - --
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      name: mount-bpf-fs
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /sys/fs/bpf
        mountPropagation: Bidirectional
        name: bpf-maps
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6qjlk
        readOnly: true
    - command:
      - /init-container.sh
      env:
      - name: CILIUM_ALL_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-state
            name: cilium-config
            optional: true
      - name: CILIUM_BPF_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-bpf-state
            name: cilium-config
            optional: true
      - name: KUBERNETES_SERVICE_HOST
        value: f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      name: clean-cilium-state
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - SYS_MODULE
          - SYS_ADMIN
          - SYS_RESOURCE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /run/cilium/cgroupv2
        mountPropagation: HostToContainer
        name: cilium-cgroup
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6qjlk
        readOnly: true
    - command:
      - /install-plugin.sh
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      name: install-cni-binaries
      resources:
        requests:
          cpu: 100m
          memory: 10Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6qjlk
        readOnly: true
    nodeName: pool-1em8noa5l-rw5at
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium
    serviceAccountName: cilium
    terminationGracePeriodSeconds: 1
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/bin/kubectl
        type: File
      name: host-kubectl
    - emptyDir: {}
      name: tmp
    - hostPath:
        path: /var/run/cilium
        type: DirectoryOrCreate
      name: cilium-run
    - hostPath:
        path: /sys/fs/bpf
        type: DirectoryOrCreate
      name: bpf-maps
    - hostPath:
        path: /proc
        type: Directory
      name: hostproc
    - hostPath:
        path: /run/cilium/cgroupv2
        type: DirectoryOrCreate
      name: cilium-cgroup
    - hostPath:
        path: /opt/cni/bin
        type: DirectoryOrCreate
      name: cni-path
    - hostPath:
        path: /etc/cni/net.d
        type: DirectoryOrCreate
      name: etc-cni-netd
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: clustermesh-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            name: cilium-clustermesh
            optional: true
        - secret:
            items:
            - key: tls.key
              path: common-etcd-client.key
            - key: tls.crt
              path: common-etcd-client.crt
            - key: ca.crt
              path: common-etcd-client-ca.crt
            name: clustermesh-apiserver-remote-cert
            optional: true
    - hostPath:
        path: /proc/sys/net
        type: Directory
      name: host-proc-sys-net
    - hostPath:
        path: /proc/sys/kernel
        type: Directory
      name: host-proc-sys-kernel
    - name: hubble-tls
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: tls.crt
              path: server.crt
            - key: tls.key
              path: server.key
            - key: ca.crt
              path: client-ca.crt
            name: hubble-server-certs
            optional: true
    - name: kube-api-access-6qjlk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:45:04Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:45:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:45:14Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:45:14Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://429b44e5aa2fabee459b62bdf862e8bc32c12b20fca77f4ac89449119106239c
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: cilium-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-15T00:45:10Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    initContainerStatuses:
    - containerID: containerd://838ca4d7b9a3de002881f89a213661de9d93939e5b7b360184c8e9372c3d59f3
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: delay-cilium-for-ccm
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://838ca4d7b9a3de002881f89a213661de9d93939e5b7b360184c8e9372c3d59f3
          exitCode: 0
          finishedAt: "2024-06-15T00:45:03Z"
          reason: Completed
          startedAt: "2024-06-15T00:45:03Z"
    - containerID: containerd://e0e8d96add86072c13c2327a8376a4344ae451067ddb5421e340592125a82c86
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: config
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://e0e8d96add86072c13c2327a8376a4344ae451067ddb5421e340592125a82c86
          exitCode: 0
          finishedAt: "2024-06-15T00:45:04Z"
          reason: Completed
          startedAt: "2024-06-15T00:45:04Z"
    - containerID: containerd://2473afa8764dd68fb6d67b54f52a9977a88301d21ceb22867c83e21abe7a2614
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: mount-cgroup
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://2473afa8764dd68fb6d67b54f52a9977a88301d21ceb22867c83e21abe7a2614
          exitCode: 0
          finishedAt: "2024-06-15T00:45:05Z"
          reason: Completed
          startedAt: "2024-06-15T00:45:05Z"
    - containerID: containerd://dd331db1df5e7b6b0e5934b4f1005e743aa5f5f06294a23381b2c5b65ecba70e
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: apply-sysctl-overwrites
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://dd331db1df5e7b6b0e5934b4f1005e743aa5f5f06294a23381b2c5b65ecba70e
          exitCode: 0
          finishedAt: "2024-06-15T00:45:06Z"
          reason: Completed
          startedAt: "2024-06-15T00:45:06Z"
    - containerID: containerd://0a7cf007e169910389638ee0ae0b678583cd3029801c9afa445379fd174e156b
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: mount-bpf-fs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://0a7cf007e169910389638ee0ae0b678583cd3029801c9afa445379fd174e156b
          exitCode: 0
          finishedAt: "2024-06-15T00:45:07Z"
          reason: Completed
          startedAt: "2024-06-15T00:45:07Z"
    - containerID: containerd://4fc22e64ca707b23294d3f57e37670b1ae70bb158ca5d5ad1192a0b32f99ac87
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: clean-cilium-state
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://4fc22e64ca707b23294d3f57e37670b1ae70bb158ca5d5ad1192a0b32f99ac87
          exitCode: 0
          finishedAt: "2024-06-15T00:45:08Z"
          reason: Completed
          startedAt: "2024-06-15T00:45:08Z"
    - containerID: containerd://e7480f2c6f00f4e387060e4742885478ea9bd7a6b891aadbe1bdd09cfa5b5b88
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: install-cni-binaries
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://e7480f2c6f00f4e387060e4742885478ea9bd7a6b891aadbe1bdd09cfa5b5b88
          exitCode: 0
          finishedAt: "2024-06-15T00:45:09Z"
          reason: Completed
          startedAt: "2024-06-15T00:45:09Z"
    phase: Running
    podIP: 10.104.0.7
    podIPs:
    - ip: 10.104.0.7
    qosClass: Burstable
    startTime: "2024-06-15T00:44:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
      container.apparmor.security.beta.kubernetes.io/apply-sysctl-overwrites: unconfined
      container.apparmor.security.beta.kubernetes.io/cilium-agent: unconfined
      container.apparmor.security.beta.kubernetes.io/clean-cilium-state: unconfined
      container.apparmor.security.beta.kubernetes.io/mount-cgroup: unconfined
      kubectl.kubernetes.io/default-container: cilium-agent
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-06-14T13:33:54Z"
    generateName: cilium-
    labels:
      app.kubernetes.io/name: cilium-agent
      app.kubernetes.io/part-of: cilium
      controller-revision-hash: f76cc49b
      doks.digitalocean.com/managed: "true"
      k8s-app: cilium
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "1"
    name: cilium-z28c7
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cilium
      uid: 338cdf62-638a-4469-8517-3c4fe503a2e8
    resourceVersion: "1740884"
    uid: 6a480d5a-f82a-4ddc-95a0-c13eb215ac79
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pool-1em8noa5l-rwr1p
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              k8s-app: cilium
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: true
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      - --k8s-api-server=https://f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
      - --ipv4-native-routing-cidr=10.244.0.0/16
      command:
      - cilium-agent
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_CLUSTERMESH_CONFIG
        value: /var/lib/cilium/clustermesh/
      - name: KUBERNETES_SERVICE_HOST
        value: f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            command:
            - bash
            - -c
            - |
              set -o errexit
              set -o pipefail
              set -o nounset

              # When running in AWS ENI mode, it's likely that 'aws-node' has
              # had a chance to install SNAT iptables rules. These can result
              # in dropped traffic, so we should attempt to remove them.
              # We do it using a 'postStart' hook since this may need to run
              # for nodes which might have already been init'ed but may still
              # have dangling rules. This is safe because there are no
              # dependencies on anything that is part of the startup script
              # itself, and can be safely run multiple times per node (e.g. in
              # case of a restart).
              if [[ "$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')" != "0" ]];
              then
                  echo 'Deleting iptables rules created by the AWS CNI VPC plugin'
                  iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore
              fi
              echo 'Done!'
        preStop:
          exec:
            command:
            - /cni-uninstall.sh
      livenessProbe:
        failureThreshold: 10
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        initialDelaySeconds: 120
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: cilium-agent
      ports:
      - containerPort: 4244
        hostPort: 4244
        name: peer-service
        protocol: TCP
      - containerPort: 9090
        hostPort: 9090
        name: prometheus
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 300m
          memory: 300Mi
      securityContext:
        capabilities:
          add:
          - CHOWN
          - KILL
          - NET_ADMIN
          - NET_RAW
          - IPC_LOCK
          - SYS_MODULE
          - SYS_ADMIN
          - SYS_RESOURCE
          - DAC_OVERRIDE
          - FOWNER
          - SETGID
          - SETUID
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      startupProbe:
        failureThreshold: 105
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /host/proc/sys/net
        name: host-proc-sys-net
      - mountPath: /host/proc/sys/kernel
        name: host-proc-sys-kernel
      - mountPath: /sys/fs/bpf
        mountPropagation: HostToContainer
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/lib/cilium/tls/hubble
        name: hubble-tls
        readOnly: true
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w9xxx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - bash
      - -e
      - -c
      - |
        # This will get the node object for the local node and search through
        # the assigned addresses in the object in order to check whether CCM
        # already set the internal AND external IP since cilium needs both
        # for a clean startup.
        # The grep matches regardless of the order of IPs.
        until /host/usr/bin/kubectl get node ${HOSTNAME} -o jsonpath="{.status.addresses[*].type}" | grep -E "InternalIP.*ExternalIP|ExternalIP.*InternalIP"; do echo "waiting for CCM to store internal and external IP addresses in node object: ${HOSTNAME}" && sleep 3; done;
      env:
      - name: KUBERNETES_SERVICE_HOST
        value: f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      name: delay-cilium-for-ccm
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/usr/bin/kubectl
        name: host-kubectl
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w9xxx
        readOnly: true
    - command:
      - cilium
      - build-config
      - --source=config-map:cilium-config
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: KUBERNETES_SERVICE_HOST
        value: f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      name: config
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w9xxx
        readOnly: true
    - command:
      - sh
      - -ec
      - |
        cp /usr/bin/cilium-mount /hostbin/cilium-mount;
        nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
        rm /hostbin/cilium-mount
      env:
      - name: CGROUP_ROOT
        value: /run/cilium/cgroupv2
      - name: BIN_PATH
        value: /opt/cni/bin
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      name: mount-cgroup
      resources: {}
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
          - SYS_CHROOT
          - SYS_PTRACE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w9xxx
        readOnly: true
    - command:
      - sh
      - -ec
      - |
        cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;
        nsenter --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-sysctlfix";
        rm /hostbin/cilium-sysctlfix
      env:
      - name: BIN_PATH
        value: /opt/cni/bin
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      name: apply-sysctl-overwrites
      resources: {}
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
          - SYS_CHROOT
          - SYS_PTRACE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w9xxx
        readOnly: true
    - args:
      - mount | grep "/sys/fs/bpf type bpf" || mount -t bpf bpf /sys/fs/bpf
      command:
      - /bin/bash
      - -c
      - --
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      name: mount-bpf-fs
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /sys/fs/bpf
        mountPropagation: Bidirectional
        name: bpf-maps
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w9xxx
        readOnly: true
    - command:
      - /init-container.sh
      env:
      - name: CILIUM_ALL_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-state
            name: cilium-config
            optional: true
      - name: CILIUM_BPF_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-bpf-state
            name: cilium-config
            optional: true
      - name: KUBERNETES_SERVICE_HOST
        value: f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      name: clean-cilium-state
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - SYS_MODULE
          - SYS_ADMIN
          - SYS_RESOURCE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /run/cilium/cgroupv2
        mountPropagation: HostToContainer
        name: cilium-cgroup
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w9xxx
        readOnly: true
    - command:
      - /install-plugin.sh
      image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      imagePullPolicy: IfNotPresent
      name: install-cni-binaries
      resources:
        requests:
          cpu: 100m
          memory: 10Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w9xxx
        readOnly: true
    nodeName: pool-1em8noa5l-rwr1p
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium
    serviceAccountName: cilium
    terminationGracePeriodSeconds: 1
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/bin/kubectl
        type: File
      name: host-kubectl
    - emptyDir: {}
      name: tmp
    - hostPath:
        path: /var/run/cilium
        type: DirectoryOrCreate
      name: cilium-run
    - hostPath:
        path: /sys/fs/bpf
        type: DirectoryOrCreate
      name: bpf-maps
    - hostPath:
        path: /proc
        type: Directory
      name: hostproc
    - hostPath:
        path: /run/cilium/cgroupv2
        type: DirectoryOrCreate
      name: cilium-cgroup
    - hostPath:
        path: /opt/cni/bin
        type: DirectoryOrCreate
      name: cni-path
    - hostPath:
        path: /etc/cni/net.d
        type: DirectoryOrCreate
      name: etc-cni-netd
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: clustermesh-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            name: cilium-clustermesh
            optional: true
        - secret:
            items:
            - key: tls.key
              path: common-etcd-client.key
            - key: tls.crt
              path: common-etcd-client.crt
            - key: ca.crt
              path: common-etcd-client-ca.crt
            name: clustermesh-apiserver-remote-cert
            optional: true
    - hostPath:
        path: /proc/sys/net
        type: Directory
      name: host-proc-sys-net
    - hostPath:
        path: /proc/sys/kernel
        type: Directory
      name: host-proc-sys-kernel
    - name: hubble-tls
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: tls.crt
              path: server.crt
            - key: tls.key
              path: server.key
            - key: ca.crt
              path: client-ca.crt
            name: hubble-server-certs
            optional: true
    - name: kube-api-access-w9xxx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:34:16Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:34:24Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:34:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:34:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:33:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d3feb1cfac03bf7b0fdcbbbeccfc74066d8adee482ec48a8c529674ff73fc013
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: cilium-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-14T13:34:24Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    initContainerStatuses:
    - containerID: containerd://e04d78268b0cd38634de72164a0187863654f625abfc6f3d46558b10b4bce092
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: delay-cilium-for-ccm
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://e04d78268b0cd38634de72164a0187863654f625abfc6f3d46558b10b4bce092
          exitCode: 0
          finishedAt: "2024-06-14T13:34:16Z"
          reason: Completed
          startedAt: "2024-06-14T13:34:16Z"
    - containerID: containerd://ac733d6334e877860678ef8ca099bc912595e85601e11dd16a04b90b7a7f1e0c
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: config
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://ac733d6334e877860678ef8ca099bc912595e85601e11dd16a04b90b7a7f1e0c
          exitCode: 0
          finishedAt: "2024-06-14T13:34:19Z"
          reason: Completed
          startedAt: "2024-06-14T13:34:18Z"
    - containerID: containerd://8c7096f903bfb8fb609b9e87afe483230288889380560e6550952e8536b427a1
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: mount-cgroup
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://8c7096f903bfb8fb609b9e87afe483230288889380560e6550952e8536b427a1
          exitCode: 0
          finishedAt: "2024-06-14T13:34:19Z"
          reason: Completed
          startedAt: "2024-06-14T13:34:19Z"
    - containerID: containerd://7e98651cf951fb1c1a86e2c7dcbfd05f9f7387518f3cf910bcc784b15cf98a0e
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: apply-sysctl-overwrites
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://7e98651cf951fb1c1a86e2c7dcbfd05f9f7387518f3cf910bcc784b15cf98a0e
          exitCode: 0
          finishedAt: "2024-06-14T13:34:20Z"
          reason: Completed
          startedAt: "2024-06-14T13:34:20Z"
    - containerID: containerd://d4858ac890f0e9e4b1a8d7b98041d3354de5bad40a2f24b4a6495ce2a693d313
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: mount-bpf-fs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://d4858ac890f0e9e4b1a8d7b98041d3354de5bad40a2f24b4a6495ce2a693d313
          exitCode: 0
          finishedAt: "2024-06-14T13:34:21Z"
          reason: Completed
          startedAt: "2024-06-14T13:34:21Z"
    - containerID: containerd://928e709d234cf610beffd3f9aed6bf3685cab264dd7dc837e87ba166d22a71e2
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: clean-cilium-state
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://928e709d234cf610beffd3f9aed6bf3685cab264dd7dc837e87ba166d22a71e2
          exitCode: 0
          finishedAt: "2024-06-14T13:34:22Z"
          reason: Completed
          startedAt: "2024-06-14T13:34:22Z"
    - containerID: containerd://d7aefbab90d3bed929e3980a74ac281ca6bdfba62922c9ee4cf5f8e97c2d13c6
      image: sha256:91f137339f40c37b2fd2e0d7f470bbb05acfab2084e096a1160ba082c86cfac7
      imageID: docker.io/cilium/cilium@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
      lastState: {}
      name: install-cni-binaries
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://d7aefbab90d3bed929e3980a74ac281ca6bdfba62922c9ee4cf5f8e97c2d13c6
          exitCode: 0
          finishedAt: "2024-06-14T13:34:23Z"
          reason: Completed
          startedAt: "2024-06-14T13:34:23Z"
    phase: Running
    podIP: 10.104.0.3
    podIPs:
    - ip: 10.104.0.3
    qosClass: Burstable
    startTime: "2024-06-14T13:33:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-06-16T18:31:04Z"
    generateName: coredns-6857f5494d-
    labels:
      doks.digitalocean.com/managed: "true"
      k8s-app: kube-dns
      pod-template-hash: 6857f5494d
    name: coredns-6857f5494d-7x648
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-6857f5494d
      uid: 82f373ac-9d1b-4c43-a223-554312c1a6ec
    resourceVersion: "2548464"
    uid: ba3a0190-66cd-401c-af92-5c147a8084dc
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: docker.io/coredns/coredns:1.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 150M
        requests:
          cpu: 100m
          memory: 150M
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2sjqs
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: pool-1em8noa5l-rw5at
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - configMap:
        defaultMode: 420
        name: coredns-custom
        optional: true
      name: custom-config-volume
    - name: kube-api-access-2sjqs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T18:31:13Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T18:31:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T18:31:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T18:31:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T18:31:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c52c1b1b7fdd9b93f9d590c49228a68750ae6939f2e582bc05ba1dd72cb2690f
      image: docker.io/coredns/coredns:1.11.1
      imageID: docker.io/coredns/coredns@sha256:1eeb4c7316bacb1d4c8ead65571cd92dd21e27359f0d4917f1a5822a73b75db1
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-16T18:31:12Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.244.0.29
    podIPs:
    - ip: 10.244.0.29
    qosClass: Burstable
    startTime: "2024-06-16T18:31:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-07-01T07:28:23Z"
    generateName: coredns-6857f5494d-
    labels:
      doks.digitalocean.com/managed: "true"
      k8s-app: kube-dns
      pod-template-hash: 6857f5494d
    name: coredns-6857f5494d-ckbh6
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-6857f5494d
      uid: 82f373ac-9d1b-4c43-a223-554312c1a6ec
    resourceVersion: "7816830"
    uid: af2bb299-91a9-48e1-9aa9-ca18558e0ac2
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: docker.io/coredns/coredns:1.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 150M
        requests:
          cpu: 100m
          memory: 150M
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cgqbb
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: pool-1em8noa5l-rwr1p
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - configMap:
        defaultMode: 420
        name: coredns-custom
        optional: true
      name: custom-config-volume
    - name: kube-api-access-cgqbb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3a58962a99d7d631da973c1ca778505fade0ead7cd60fd2e0f9d75a81c72a11d
      image: docker.io/coredns/coredns:1.11.1
      imageID: docker.io/coredns/coredns@sha256:1eeb4c7316bacb1d4c8ead65571cd92dd21e27359f0d4917f1a5822a73b75db1
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-07-01T07:28:30Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    phase: Running
    podIP: 10.244.26.122
    podIPs:
    - ip: 10.244.26.122
    qosClass: Burstable
    startTime: "2024-07-01T07:28:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: resource-requirements
    creationTimestamp: "2024-06-14T13:33:54Z"
    generateName: cpc-bridge-proxy-
    labels:
      app: cpc-bridge-proxy
      controller-revision-hash: 6c4d9598d9
      doks.digitalocean.com/managed: "true"
      pod-template-generation: "1"
    name: cpc-bridge-proxy-mcjqz
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cpc-bridge-proxy
      uid: ed5c6c96-c2b0-4db2-81e2-228ec8de77e2
    resourceVersion: "1751894"
    uid: afea4ccc-cb14-4e7c-8705-fb00d2162e8c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pool-1em8noa5l-rwr1p
    automountServiceAccountToken: false
    containers:
    - image: digitalocean/cpbridge:1.25.3
      imagePullPolicy: IfNotPresent
      name: cpc-bridge-proxy
      resources:
        requests:
          cpu: 100m
          memory: 75Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/nginx
        name: cpc-bridge-proxy-config
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - |
        set -o errexit
        set -o pipefail
        set -o nounset
        ipt_nat="iptables-legacy -t nat"
        # Avoid racing with kube-proxy on the initial iptables rules population which makes the rule order indeterministic.
        until ${ipt_nat} --list KUBE-SERVICES > /dev/null; do echo "waiting for kube-proxy to populate iptables rules"; sleep 3; done
        ipt_output_args="OUTPUT -p tcp -d 10.245.0.1/32 --dport 443 -j DNAT --to-destination 100.65.52.51:16443"
        ipt_prerouting_args="PREROUTING -p tcp -d 100.65.52.51 --dport 443 -j DNAT --to-destination 100.65.52.51:16443"
        ${ipt_nat} --check ${ipt_output_args} || ${ipt_nat} --insert ${ipt_output_args}
        ${ipt_nat} --check ${ipt_prerouting_args} || ${ipt_nat} --insert ${ipt_prerouting_args}
      image: digitalocean/cpbridge:1.25.3
      imagePullPolicy: IfNotPresent
      name: init-iptables
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    nodeName: pool-1em8noa5l-rwr1p
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: cpc-bridge-proxy-config
      name: cpc-bridge-proxy-config
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:34:14Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T14:08:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T14:08:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T14:08:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:33:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ad9d0076fa9bf639bc7e6a4c3b879260f65eb1a27c6e01c5fd8c6a648b06a7ce
      image: docker.io/digitalocean/cpbridge:1.25.3
      imageID: docker.io/digitalocean/cpbridge@sha256:26214c7eff1fcf06ffbe6e8de9e1df5ec3fc5ad5bb4a1e4b37ba2bb2bcbcbc48
      lastState: {}
      name: cpc-bridge-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-14T14:08:55Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    initContainerStatuses:
    - containerID: containerd://7db72f6b4c9f7bf8532c2f5ca42a74d38360f15ddb957cb3289c2cd287d3e9a6
      image: docker.io/digitalocean/cpbridge:1.25.3
      imageID: docker.io/digitalocean/cpbridge@sha256:26214c7eff1fcf06ffbe6e8de9e1df5ec3fc5ad5bb4a1e4b37ba2bb2bcbcbc48
      lastState: {}
      name: init-iptables
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://7db72f6b4c9f7bf8532c2f5ca42a74d38360f15ddb957cb3289c2cd287d3e9a6
          exitCode: 0
          finishedAt: "2024-06-14T14:08:55Z"
          reason: Completed
          startedAt: "2024-06-14T13:34:14Z"
    phase: Running
    podIP: 10.104.0.3
    podIPs:
    - ip: 10.104.0.3
    qosClass: Burstable
    startTime: "2024-06-14T13:33:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: resource-requirements
    creationTimestamp: "2024-06-15T00:44:39Z"
    generateName: cpc-bridge-proxy-
    labels:
      app: cpc-bridge-proxy
      controller-revision-hash: 6c4d9598d9
      doks.digitalocean.com/managed: "true"
      pod-template-generation: "1"
    name: cpc-bridge-proxy-mj7vk
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cpc-bridge-proxy
      uid: ed5c6c96-c2b0-4db2-81e2-228ec8de77e2
    resourceVersion: "1921128"
    uid: 729c2a69-bf66-41e4-9fec-c7852fd89cc3
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pool-1em8noa5l-rw5at
    automountServiceAccountToken: false
    containers:
    - image: digitalocean/cpbridge:1.25.3
      imagePullPolicy: IfNotPresent
      name: cpc-bridge-proxy
      resources:
        requests:
          cpu: 100m
          memory: 75Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/nginx
        name: cpc-bridge-proxy-config
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - |
        set -o errexit
        set -o pipefail
        set -o nounset
        ipt_nat="iptables-legacy -t nat"
        # Avoid racing with kube-proxy on the initial iptables rules population which makes the rule order indeterministic.
        until ${ipt_nat} --list KUBE-SERVICES > /dev/null; do echo "waiting for kube-proxy to populate iptables rules"; sleep 3; done
        ipt_output_args="OUTPUT -p tcp -d 10.245.0.1/32 --dport 443 -j DNAT --to-destination 100.65.52.51:16443"
        ipt_prerouting_args="PREROUTING -p tcp -d 100.65.52.51 --dport 443 -j DNAT --to-destination 100.65.52.51:16443"
        ${ipt_nat} --check ${ipt_output_args} || ${ipt_nat} --insert ${ipt_output_args}
        ${ipt_nat} --check ${ipt_prerouting_args} || ${ipt_nat} --insert ${ipt_prerouting_args}
      image: digitalocean/cpbridge:1.25.3
      imagePullPolicy: IfNotPresent
      name: init-iptables
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: cpc-bridge-proxy-config
      name: cpc-bridge-proxy-config
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:57Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:45:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:45:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:45:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://12735ecc5e997b33c849532422ab4a1608efe2ea584e35ff14c79d687373f4db
      image: docker.io/digitalocean/cpbridge:1.25.3
      imageID: docker.io/digitalocean/cpbridge@sha256:26214c7eff1fcf06ffbe6e8de9e1df5ec3fc5ad5bb4a1e4b37ba2bb2bcbcbc48
      lastState: {}
      name: cpc-bridge-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-15T00:45:02Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    initContainerStatuses:
    - containerID: containerd://ea1672a87079c79c06221c02fe9cce4a323e3dd81024a020bb90a60d27494bd4
      image: docker.io/digitalocean/cpbridge:1.25.3
      imageID: docker.io/digitalocean/cpbridge@sha256:26214c7eff1fcf06ffbe6e8de9e1df5ec3fc5ad5bb4a1e4b37ba2bb2bcbcbc48
      lastState: {}
      name: init-iptables
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://ea1672a87079c79c06221c02fe9cce4a323e3dd81024a020bb90a60d27494bd4
          exitCode: 0
          finishedAt: "2024-06-15T00:45:00Z"
          reason: Completed
          startedAt: "2024-06-15T00:44:57Z"
    phase: Running
    podIP: 10.104.0.7
    podIPs:
    - ip: 10.104.0.7
    qosClass: Burstable
    startTime: "2024-06-15T00:44:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
      kubectl.kubernetes.io/default-container: csi-do-plugin
    creationTimestamp: "2024-06-24T08:02:02Z"
    generateName: csi-do-node-
    labels:
      app: csi-do-node
      controller-revision-hash: 5cfbd4cc7
      doks.digitalocean.com/managed: "true"
      pod-template-generation: "2"
      role: csi-do
    name: csi-do-node-9ghht
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-do-node
      uid: b7a14168-e8bb-439d-aff4-78856303073b
    resourceVersion: "5288172"
    uid: bcdfddf8-1cfa-4865-af23-f5af53a2a845
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pool-1em8noa5l-rwr1p
    containers:
    - args:
      - --v=5
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: DRIVER_REG_SOCK_PATH
        value: /var/lib/kubelet/plugins/dobs.csi.digitalocean.com/csi.sock
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.3
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -rf /registration/dobs.csi.digitalocean.com /registration/dobs.csi.digitalocean.com-reg.sock
      name: csi-node-driver-registrar
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: plugin-dir
      - mountPath: /registration/
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zmsr5
        readOnly: true
    - args:
      - --endpoint=$(CSI_ENDPOINT)
      - --validate-attachment=true
      - --url=https://api.digitalocean.com
      - --driver-name=dobs.csi.digitalocean.com
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: docker.io/digitalocean/do-csi-plugin:v4.10.0
      imagePullPolicy: Always
      name: csi-do-plugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /dev
        name: device-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zmsr5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: pool-1em8noa5l-rwr1p
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: csi-do-node-sa
    serviceAccountName: csi-do-node-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: DirectoryOrCreate
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/dobs.csi.digitalocean.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: device-dir
    - name: kube-api-access-zmsr5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T08:02:10Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T08:02:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T08:02:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T08:02:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T08:02:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cee68ea644a918b1c7250e911920426398ecf2788358a6e2c2dfa6356aecd636
      image: docker.io/digitalocean/do-csi-plugin:v4.10.0
      imageID: docker.io/digitalocean/do-csi-plugin@sha256:75b423365ae954757a350d0adc54da97dd9e66afecf1e6ebf2ccc63aecb9c883
      lastState: {}
      name: csi-do-plugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-24T08:02:09Z"
    - containerID: containerd://5fc83c2aeaa2ffdcf370bc3975df4de7c2be4cd67deff9b40ef06e6e91a3bbae
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.3
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:0f64602ea791246712b51df334bbd701a0f31df9950a4cb9c28c059f367baa9e
      lastState: {}
      name: csi-node-driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-24T08:02:02Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    phase: Running
    podIP: 10.104.0.3
    podIPs:
    - ip: 10.104.0.3
    qosClass: BestEffort
    startTime: "2024-06-24T08:02:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
      kubectl.kubernetes.io/default-container: csi-do-plugin
    creationTimestamp: "2024-06-24T08:02:34Z"
    generateName: csi-do-node-
    labels:
      app: csi-do-node
      controller-revision-hash: 5cfbd4cc7
      doks.digitalocean.com/managed: "true"
      pod-template-generation: "2"
      role: csi-do
    name: csi-do-node-t45w5
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-do-node
      uid: b7a14168-e8bb-439d-aff4-78856303073b
    resourceVersion: "5288434"
    uid: 11bf015a-0a6a-4d33-a09a-2ca979e07a86
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pool-1em8noa5l-rw5at
    containers:
    - args:
      - --v=5
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: DRIVER_REG_SOCK_PATH
        value: /var/lib/kubelet/plugins/dobs.csi.digitalocean.com/csi.sock
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.3
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -rf /registration/dobs.csi.digitalocean.com /registration/dobs.csi.digitalocean.com-reg.sock
      name: csi-node-driver-registrar
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: plugin-dir
      - mountPath: /registration/
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wl44c
        readOnly: true
    - args:
      - --endpoint=$(CSI_ENDPOINT)
      - --validate-attachment=true
      - --url=https://api.digitalocean.com
      - --driver-name=dobs.csi.digitalocean.com
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: docker.io/digitalocean/do-csi-plugin:v4.10.0
      imagePullPolicy: Always
      name: csi-do-plugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /dev
        name: device-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wl44c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: csi-do-node-sa
    serviceAccountName: csi-do-node-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: DirectoryOrCreate
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/dobs.csi.digitalocean.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: device-dir
    - name: kube-api-access-wl44c
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T08:02:42Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T08:02:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T08:02:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T08:02:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T08:02:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://50f8cab0b0ab7deeb5bd9552cb3cf069dc2938406bc360a87b1517614c400b0c
      image: docker.io/digitalocean/do-csi-plugin:v4.10.0
      imageID: docker.io/digitalocean/do-csi-plugin@sha256:75b423365ae954757a350d0adc54da97dd9e66afecf1e6ebf2ccc63aecb9c883
      lastState: {}
      name: csi-do-plugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-24T08:02:42Z"
    - containerID: containerd://d0bef6e19165edab958e386637139c5d28de3cdf47f43edcd8a6d337bb04f003
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.3
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:0f64602ea791246712b51df334bbd701a0f31df9950a4cb9c28c059f367baa9e
      lastState: {}
      name: csi-node-driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-24T08:02:35Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.104.0.7
    podIPs:
    - ip: 10.104.0.7
    qosClass: BestEffort
    startTime: "2024-06-24T08:02:34Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: hostpath-volume
    creationTimestamp: "2024-06-15T00:44:39Z"
    generateName: do-node-agent-
    labels:
      app: do-node-agent
      controller-revision-hash: 7fc84d85bf
      doks.digitalocean.com/managed: "true"
      pod-template-generation: "1"
    name: do-node-agent-bv2ls
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: do-node-agent
      uid: 1a45765d-3c7e-442b-8f76-11d21f5c353c
    resourceVersion: "1921048"
    uid: bed03f4e-60af-469d-9689-684414ff6d31
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pool-1em8noa5l-rw5at
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --k8s-metrics-path=http://kube-state-metrics.kube-system.svc.cluster.local:8080/metrics
      - --additional-label=kubernetes_cluster_uuid:f9962841-03c0-44b1-bde7-37fc077b7777
      command:
      - /bin/do-agent
      image: docker.io/digitalocean/do-agent:3.16.7
      imagePullPolicy: IfNotPresent
      name: do-node-agent
      resources:
        limits:
          memory: 300Mi
        requests:
          cpu: 102m
          memory: 80Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9f7zs
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: pool-1em8noa5l-rw5at
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: do-agent
    serviceAccountName: do-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
    - name: kube-api-access-9f7zs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:54Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2bc6a83037be0ef8b41959c8b94942391e2803ba944cb3095503e786ad633dbf
      image: docker.io/digitalocean/do-agent:3.16.7
      imageID: docker.io/digitalocean/do-agent@sha256:3e7fd2623ffce0bce5224d7c220564234a77bf9b9f56cc5d5321203191a8a98e
      lastState: {}
      name: do-node-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-15T00:44:53Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.104.0.7
    podIPs:
    - ip: 10.104.0.7
    qosClass: Burstable
    startTime: "2024-06-15T00:44:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: hostpath-volume
    creationTimestamp: "2024-06-14T13:33:54Z"
    generateName: do-node-agent-
    labels:
      app: do-node-agent
      controller-revision-hash: 7fc84d85bf
      doks.digitalocean.com/managed: "true"
      pod-template-generation: "1"
    name: do-node-agent-pd7m4
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: do-node-agent
      uid: 1a45765d-3c7e-442b-8f76-11d21f5c353c
    resourceVersion: "1740760"
    uid: d8492fff-ee04-4afc-9cc4-c56dcc9cfdb2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pool-1em8noa5l-rwr1p
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --k8s-metrics-path=http://kube-state-metrics.kube-system.svc.cluster.local:8080/metrics
      - --additional-label=kubernetes_cluster_uuid:f9962841-03c0-44b1-bde7-37fc077b7777
      command:
      - /bin/do-agent
      image: docker.io/digitalocean/do-agent:3.16.7
      imagePullPolicy: IfNotPresent
      name: do-node-agent
      resources:
        limits:
          memory: 300Mi
        requests:
          cpu: 102m
          memory: 80Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pmbww
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: pool-1em8noa5l-rwr1p
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: do-agent
    serviceAccountName: do-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
    - name: kube-api-access-pmbww
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:34:12Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:33:56Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:34:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:34:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:33:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://09ab6f8fbf24f483ceb92d759d6c55a9b844c5cf075b763c59c9a8a1e3cf61a7
      image: docker.io/digitalocean/do-agent:3.16.7
      imageID: docker.io/digitalocean/do-agent@sha256:3e7fd2623ffce0bce5224d7c220564234a77bf9b9f56cc5d5321203191a8a98e
      lastState: {}
      name: do-node-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-14T13:34:12Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    phase: Running
    podIP: 10.104.0.3
    podIPs:
    - ip: 10.104.0.3
    qosClass: Burstable
    startTime: "2024-06-14T13:33:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2024-07-01T07:28:24Z"
    generateName: hubble-relay-6b67bbc75f-
    labels:
      app.kubernetes.io/name: hubble-relay
      app.kubernetes.io/part-of: cilium
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-relay
      pod-template-hash: 6b67bbc75f
    name: hubble-relay-6b67bbc75f-679tg
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hubble-relay-6b67bbc75f
      uid: d180e92f-dc60-4a5a-8a58-733c44ff6b60
    resourceVersion: "7816851"
    uid: b098e30c-dbec-4787-9ff4-cfb74e3860cd
  spec:
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              k8s-app: cilium
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: false
    containers:
    - args:
      - serve
      command:
      - hubble-relay
      image: quay.io/cilium/hubble-relay:v1.14.10@sha256:c156c4fc2da520d2876142ea17490440b95431a1be755d2050e72115a495cfd0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: grpc
        timeoutSeconds: 1
      name: hubble-relay
      ports:
      - containerPort: 4245
        name: grpc
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: grpc
        timeoutSeconds: 1
      resources: {}
      securityContext:
        capabilities:
          drop:
          - ALL
        runAsGroup: 65532
        runAsNonRoot: true
        runAsUser: 65532
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hubble-relay
        name: config
        readOnly: true
      - mountPath: /var/lib/hubble-relay/tls
        name: tls
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pool-1em8noa5l-rw5at
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65532
    serviceAccount: hubble-relay
    serviceAccountName: hubble-relay
    terminationGracePeriodSeconds: 1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: config.yaml
          path: config.yaml
        name: hubble-relay-config
      name: config
    - name: tls
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: tls.crt
              path: client.crt
            - key: tls.key
              path: client.key
            - key: ca.crt
              path: hubble-server-ca.crt
            name: hubble-relay-client-certs
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:24Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://82a632c9a3c2638e3e825650e46495a8af9673e3325a0072915147d1bdf84763
      image: sha256:c1c3c23052b596a196bafdde792de08748eb9d634c3a9fd75f3d8264d8b7ced9
      imageID: quay.io/cilium/hubble-relay@sha256:c156c4fc2da520d2876142ea17490440b95431a1be755d2050e72115a495cfd0
      lastState: {}
      name: hubble-relay
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-07-01T07:28:31Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.244.0.11
    podIPs:
    - ip: 10.244.0.11
    qosClass: BestEffort
    startTime: "2024-07-01T07:28:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2024-07-01T07:28:24Z"
    generateName: hubble-ui-776986f894-
    labels:
      app.kubernetes.io/name: hubble-ui
      app.kubernetes.io/part-of: cilium
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-ui
      pod-template-hash: 776986f894
    name: hubble-ui-776986f894-zk8r2
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hubble-ui-776986f894
      uid: dabbab36-afec-4941-ad57-94b7d2411c5c
    resourceVersion: "7816970"
    uid: 90d35a82-1a04-4862-b851-56bd4c0603d2
  spec:
    automountServiceAccountToken: true
    containers:
    - image: quay.io/cilium/hubble-ui:v0.13.0@sha256:7d663dc16538dd6e29061abd1047013a645e6e69c115e008bee9ea9fef9a6666
      imagePullPolicy: IfNotPresent
      name: frontend
      ports:
      - containerPort: 8081
        name: http
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/nginx/conf.d/default.conf
        name: hubble-ui-nginx-conf
        subPath: nginx.conf
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ld7dx
        readOnly: true
    - env:
      - name: EVENTS_SERVER_PORT
        value: "8090"
      - name: FLOWS_API_ADDR
        value: hubble-relay:80
      image: quay.io/cilium/hubble-ui-backend:v0.13.0@sha256:1e7657d997c5a48253bb8dc91ecee75b63018d16ff5e5797e5af367336bc8803
      imagePullPolicy: IfNotPresent
      name: backend
      ports:
      - containerPort: 8090
        name: grpc
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ld7dx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pool-1em8noa5l-rw5at
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: hubble-ui
    serviceAccountName: hubble-ui
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: hubble-ui-nginx
      name: hubble-ui-nginx-conf
    - emptyDir: {}
      name: tmp-dir
    - name: kube-api-access-ld7dx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:41Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:24Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:41Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:41Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://78d40b92f7a81a4ba4bf21c72467717f8e9d5b3e74470844012113dae8f08bff
      image: sha256:1ea21544e094404b724cc15293af153f44149b5b081f32d4d43967b88bc98b3f
      imageID: quay.io/cilium/hubble-ui-backend@sha256:1e7657d997c5a48253bb8dc91ecee75b63018d16ff5e5797e5af367336bc8803
      lastState: {}
      name: backend
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-07-01T07:28:40Z"
    - containerID: containerd://7149c2730a110012b34ab178d6d93053431d27412f81ac365b63eb3d1009bb90
      image: sha256:78000f56b0cffca17d887126b1f52527f6c5e8104e3dd5ac8344a29c4ae38713
      imageID: quay.io/cilium/hubble-ui@sha256:7d663dc16538dd6e29061abd1047013a645e6e69c115e008bee9ea9fef9a6666
      lastState: {}
      name: frontend
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-07-01T07:28:32Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.244.0.33
    podIPs:
    - ip: 10.244.0.33
    qosClass: BestEffort
    startTime: "2024-07-01T07:28:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-06-15T00:44:39Z"
    generateName: konnectivity-agent-
    labels:
      controller-revision-hash: 85456c957d
      doks.digitalocean.com/managed: "true"
      k8s-app: konnectivity-agent
      pod-template-generation: "1"
    name: konnectivity-agent-j952f
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: konnectivity-agent
      uid: 4283b747-52bc-4c3d-9853-6c9ce8e24d7e
    resourceVersion: "1921301"
    uid: 30b6864f-2c58-4eff-9ddb-0aa96a8bb29e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pool-1em8noa5l-rw5at
    containers:
    - args:
      - --logtostderr=true
      - --ca-cert=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      - --proxy-server-port=8132
      - --admin-server-port=8133
      - --health-server-port=8134
      - --keepalive-time=5m
      - --service-account-token-path=/var/run/secrets/tokens/konnectivity-agent-token
      - --proxy-server-host=f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
      command:
      - /proxy-agent
      image: registry.k8s.io/kas-network-proxy/proxy-agent:v0.29.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8134
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: konnectivity-agent
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/tokens
        name: konnectivity-agent-token
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lfkhv
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: konnectivity-agent
    serviceAccountName: konnectivity-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: konnectivity-agent-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: system:konnectivity-server
            expirationSeconds: 3600
            path: konnectivity-agent-token
    - name: kube-api-access-lfkhv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:45:25Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:45:25Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:45:25Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://66d1c0d9b9a91dd209a1bd90f258ac850a94ac52f3673a4da327cd2d4e248cc3
      image: registry.k8s.io/kas-network-proxy/proxy-agent:v0.29.0
      imageID: registry.k8s.io/kas-network-proxy/proxy-agent@sha256:0349e45ae85f9aec3fafc289498b0058b2c0c4725fe4e7f69fef361182976833
      lastState: {}
      name: konnectivity-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-15T00:45:25Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.244.0.10
    podIPs:
    - ip: 10.244.0.10
    qosClass: BestEffort
    startTime: "2024-06-15T00:44:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-06-14T13:33:54Z"
    generateName: konnectivity-agent-
    labels:
      controller-revision-hash: 85456c957d
      doks.digitalocean.com/managed: "true"
      k8s-app: konnectivity-agent
      pod-template-generation: "1"
    name: konnectivity-agent-pm5db
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: konnectivity-agent
      uid: 4283b747-52bc-4c3d-9853-6c9ce8e24d7e
    resourceVersion: "1741864"
    uid: 7281631a-30b1-44c5-bafb-479153f41588
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pool-1em8noa5l-rwr1p
    containers:
    - args:
      - --logtostderr=true
      - --ca-cert=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      - --proxy-server-port=8132
      - --admin-server-port=8133
      - --health-server-port=8134
      - --keepalive-time=5m
      - --service-account-token-path=/var/run/secrets/tokens/konnectivity-agent-token
      - --proxy-server-host=f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
      command:
      - /proxy-agent
      image: registry.k8s.io/kas-network-proxy/proxy-agent:v0.29.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8134
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: konnectivity-agent
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/tokens
        name: konnectivity-agent-token
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ncxjz
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: pool-1em8noa5l-rwr1p
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: konnectivity-agent
    serviceAccountName: konnectivity-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: konnectivity-agent-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: system:konnectivity-server
            expirationSeconds: 3600
            path: konnectivity-agent-token
    - name: kube-api-access-ncxjz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:37:23Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:33:56Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:37:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:37:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:33:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://590c0f45c844c200f897b171589403d9db262169b8543092c81f236db13eb457
      image: registry.k8s.io/kas-network-proxy/proxy-agent:v0.29.0
      imageID: registry.k8s.io/kas-network-proxy/proxy-agent@sha256:0349e45ae85f9aec3fafc289498b0058b2c0c4725fe4e7f69fef361182976833
      lastState: {}
      name: konnectivity-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-14T13:37:22Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    phase: Running
    podIP: 10.244.26.51
    podIPs:
    - ip: 10.244.26.51
    qosClass: BestEffort
    startTime: "2024-06-14T13:33:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
    creationTimestamp: "2024-06-15T00:44:39Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 6fdcc7c877
      doks.digitalocean.com/managed: "true"
      k8s-app: kube-proxy
      pod-template-generation: "1"
      tier: node
    name: kube-proxy-kwvjw
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: af07a04a-b595-4e65-a81d-fdf7fbb57537
    resourceVersion: "1921091"
    uid: 30eccb65-f32c-4430-897a-9fdc5447923d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pool-1em8noa5l-rw5at
    containers:
    - command:
      - kube-proxy
      - --config=/etc/kubernetes/config/kube-proxy-config.yaml
      image: registry.k8s.io/kube-proxy:v1.29.5
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources:
        requests:
          memory: 125Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes
        name: kube-proxy-kubeconfig
        readOnly: true
      - mountPath: /etc/kubernetes/config
        name: kube-proxy-config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mf6lr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: kube-proxy-kubeconfig
      secret:
        defaultMode: 420
        secretName: kube-proxy
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy-config
    - name: kube-api-access-mf6lr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:59Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:59Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:59Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-15T00:44:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f3bbb3af968ca55a852849bb0c9ad0b030ad9b083964954855a8e60f0f5ed668
      image: registry.k8s.io/kube-proxy:v1.29.5
      imageID: registry.k8s.io/kube-proxy@sha256:4c9681a68b0f068f66e6c4120be71a4416621cad1427802deaaa79d01fdffb85
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-15T00:44:59Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.104.0.7
    podIPs:
    - ip: 10.104.0.7
    qosClass: Burstable
    startTime: "2024-06-15T00:44:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
    creationTimestamp: "2024-06-14T13:33:54Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 6fdcc7c877
      doks.digitalocean.com/managed: "true"
      k8s-app: kube-proxy
      pod-template-generation: "1"
      tier: node
    name: kube-proxy-tjzkl
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: af07a04a-b595-4e65-a81d-fdf7fbb57537
    resourceVersion: "1751881"
    uid: 8cc6560f-f025-477e-9323-c283d8a14c8f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pool-1em8noa5l-rwr1p
    containers:
    - command:
      - kube-proxy
      - --config=/etc/kubernetes/config/kube-proxy-config.yaml
      image: registry.k8s.io/kube-proxy:v1.29.5
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources:
        requests:
          memory: 125Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes
        name: kube-proxy-kubeconfig
        readOnly: true
      - mountPath: /etc/kubernetes/config
        name: kube-proxy-config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cx7dc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: pool-1em8noa5l-rwr1p
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: kube-proxy-kubeconfig
      secret:
        defaultMode: 420
        secretName: kube-proxy
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy-config
    - name: kube-api-access-cx7dc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:48:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:33:56Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T14:08:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T14:08:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-14T13:33:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9e263854314e6da291976c9e8d51cf20ce164a90423ccb1d18b564e73cb23cdf
      image: registry.k8s.io/kube-proxy:v1.29.5
      imageID: registry.k8s.io/kube-proxy@sha256:4c9681a68b0f068f66e6c4120be71a4416621cad1427802deaaa79d01fdffb85
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-14T14:08:55Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    phase: Running
    podIP: 10.104.0.3
    podIPs:
    - ip: 10.104.0.3
    qosClass: Burstable
    startTime: "2024-06-14T13:33:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/credentials-secret: f83fbdfae78c625e0f0d9e490afbd2044d84c4ececfeed24133f02d487e506b9
    creationTimestamp: "2024-06-19T22:09:38Z"
    generateName: minio-
    labels:
      app.kubernetes.io/instance: minio
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: minio
      app.kubernetes.io/version: 2024.6.13
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: minio-56bfc7b957
      helm.sh/chart: minio-14.6.11
      statefulset.kubernetes.io/pod-name: minio-0
    name: minio-0
    namespace: minio
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: minio
      uid: a6e6347b-3fbf-49bc-927d-e79ce0e17bd6
    resourceVersion: "7814161"
    uid: f8c79a34-85d5-4d8c-b8ed-e4b4d1f9bd80
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/instance: minio
                app.kubernetes.io/name: minio
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: MINIO_DISTRIBUTED_MODE_ENABLED
        value: "yes"
      - name: MINIO_DISTRIBUTED_NODES
        value: minio-{0...2}.minio-headless.minio.svc.cluster.local:9000/bitnami/minio/data-{0...1}
      - name: MINIO_SCHEME
        value: http
      - name: MINIO_FORCE_NEW_KEYS
        value: "no"
      - name: MINIO_ROOT_USER
        valueFrom:
          secretKeyRef:
            key: root-user
            name: minio
      - name: MINIO_ROOT_PASSWORD
        valueFrom:
          secretKeyRef:
            key: root-password
            name: minio
      - name: MINIO_SKIP_CLIENT
        value: "yes"
      - name: MINIO_BROWSER
        value: "on"
      - name: MINIO_PROMETHEUS_AUTH_TYPE
        value: public
      - name: MINIO_DATA_DIR
        value: /bitnami/minio/data-0
      image: docker.io/bitnami/minio:2024.4.18-debian-12-r0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /minio/health/live
          port: minio-api
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      name: minio
      ports:
      - containerPort: 9000
        name: minio-api
        protocol: TCP
      - containerPort: 9001
        name: minio-console
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        tcpSocket:
          port: minio-api
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "2"
          memory: 800Mi
        requests:
          cpu: "1"
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
      - mountPath: /opt/bitnami/minio/tmp
        name: empty-dir
        subPath: app-tmp-dir
      - mountPath: /.mc
        name: empty-dir
        subPath: app-mc-dir
      - mountPath: /bitnami/minio/data-0
        name: data-0
      - mountPath: /bitnami/minio/data-1
        name: data-1
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: minio-0
    initContainers:
    - command:
      - /bin/bash
      - -ec
      - "chown -R 1001:1001 /bitnami/minio/data-0 /bitnami/minio/data-1 \n"
      image: docker.io/bitnami/os-shell:12-debian-12-r18
      imagePullPolicy: IfNotPresent
      name: volume-permissions
      resources:
        limits:
          cpu: 150m
          ephemeral-storage: 1Gi
          memory: 192Mi
        requests:
          cpu: 100m
          ephemeral-storage: 50Mi
          memory: 128Mi
      securityContext:
        runAsUser: 0
        seLinuxOptions: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
      - mountPath: /bitnami/minio/data-0
        name: data-0
      - mountPath: /bitnami/minio/data-1
        name: data-1
    nodeName: pool-1em8noa5l-rwr1p
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: OnRootMismatch
    serviceAccount: minio
    serviceAccountName: minio
    subdomain: minio-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data-1
      persistentVolumeClaim:
        claimName: data-1-minio-0
    - name: data-0
      persistentVolumeClaim:
        claimName: data-0-minio-0
    - emptyDir: {}
      name: empty-dir
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-19T22:09:57Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-19T22:10:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:21:11Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:21:11Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-19T22:09:38Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d72bb52308d32236df1e918511dbc6db54719a9974990ee0c6711ed219059ec7
      image: docker.io/bitnami/minio:2024.4.18-debian-12-r0
      imageID: docker.io/bitnami/minio@sha256:126dde449621531356426d59f9bac6aa493d91b3424e9d1f63b28b509521e8ee
      lastState:
        terminated:
          containerID: containerd://fb3d536db25527eda5594e7789af8f6e1718ca11c0a91bb7c1525f3c221c8e66
          exitCode: 137
          finishedAt: "2024-07-01T07:20:49Z"
          reason: OOMKilled
          startedAt: "2024-07-01T07:13:25Z"
      name: minio
      ready: true
      restartCount: 140
      started: true
      state:
        running:
          startedAt: "2024-07-01T07:21:05Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    initContainerStatuses:
    - containerID: containerd://21591841ffaa82520fd42b88fdd31565742ed10483be0cd4612d4e73814765f8
      image: docker.io/bitnami/os-shell:12-debian-12-r18
      imageID: docker.io/bitnami/os-shell@sha256:308932799b147e42e0fb10ce673a31caf22fa28b902473e7a032cf5a270ebc36
      lastState: {}
      name: volume-permissions
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://21591841ffaa82520fd42b88fdd31565742ed10483be0cd4612d4e73814765f8
          exitCode: 0
          finishedAt: "2024-06-19T22:10:21Z"
          reason: Completed
          startedAt: "2024-06-19T22:09:56Z"
    phase: Running
    podIP: 10.244.26.81
    podIPs:
    - ip: 10.244.26.81
    qosClass: Burstable
    startTime: "2024-06-19T22:09:38Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/credentials-secret: f83fbdfae78c625e0f0d9e490afbd2044d84c4ececfeed24133f02d487e506b9
    creationTimestamp: "2024-07-01T07:28:25Z"
    generateName: minio-
    labels:
      app.kubernetes.io/instance: minio
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: minio
      app.kubernetes.io/version: 2024.6.13
      apps.kubernetes.io/pod-index: "1"
      controller-revision-hash: minio-56bfc7b957
      helm.sh/chart: minio-14.6.11
      statefulset.kubernetes.io/pod-name: minio-1
    name: minio-1
    namespace: minio
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: minio
      uid: a6e6347b-3fbf-49bc-927d-e79ce0e17bd6
    resourceVersion: "7818609"
    uid: a861f442-efdf-4585-9c7a-5fe81ff9c898
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/instance: minio
                app.kubernetes.io/name: minio
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: MINIO_DISTRIBUTED_MODE_ENABLED
        value: "yes"
      - name: MINIO_DISTRIBUTED_NODES
        value: minio-{0...2}.minio-headless.minio.svc.cluster.local:9000/bitnami/minio/data-{0...1}
      - name: MINIO_SCHEME
        value: http
      - name: MINIO_FORCE_NEW_KEYS
        value: "no"
      - name: MINIO_ROOT_USER
        valueFrom:
          secretKeyRef:
            key: root-user
            name: minio
      - name: MINIO_ROOT_PASSWORD
        valueFrom:
          secretKeyRef:
            key: root-password
            name: minio
      - name: MINIO_SKIP_CLIENT
        value: "yes"
      - name: MINIO_BROWSER
        value: "on"
      - name: MINIO_PROMETHEUS_AUTH_TYPE
        value: public
      - name: MINIO_DATA_DIR
        value: /bitnami/minio/data-0
      image: docker.io/bitnami/minio:2024.4.18-debian-12-r0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /minio/health/live
          port: minio-api
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      name: minio
      ports:
      - containerPort: 9000
        name: minio-api
        protocol: TCP
      - containerPort: 9001
        name: minio-console
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        tcpSocket:
          port: minio-api
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "2"
          memory: 800Mi
        requests:
          cpu: "1"
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
      - mountPath: /opt/bitnami/minio/tmp
        name: empty-dir
        subPath: app-tmp-dir
      - mountPath: /.mc
        name: empty-dir
        subPath: app-mc-dir
      - mountPath: /bitnami/minio/data-0
        name: data-0
      - mountPath: /bitnami/minio/data-1
        name: data-1
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: minio-1
    initContainers:
    - command:
      - /bin/bash
      - -ec
      - "chown -R 1001:1001 /bitnami/minio/data-0 /bitnami/minio/data-1 \n"
      image: docker.io/bitnami/os-shell:12-debian-12-r18
      imagePullPolicy: IfNotPresent
      name: volume-permissions
      resources:
        limits:
          cpu: 150m
          ephemeral-storage: 1Gi
          memory: 192Mi
        requests:
          cpu: 100m
          ephemeral-storage: 50Mi
          memory: 128Mi
      securityContext:
        runAsUser: 0
        seLinuxOptions: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
      - mountPath: /bitnami/minio/data-0
        name: data-0
      - mountPath: /bitnami/minio/data-1
        name: data-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: OnRootMismatch
    serviceAccount: minio
    serviceAccountName: minio
    subdomain: minio-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data-0
      persistentVolumeClaim:
        claimName: data-0-minio-1
    - name: data-1
      persistentVolumeClaim:
        claimName: data-1-minio-1
    - emptyDir: {}
      name: empty-dir
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:25Z"
      message: '0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes
        are available: 2 No preemption victims found for incoming pod.'
      reason: Unschedulable
      status: "False"
      type: PodScheduled
    phase: Pending
    qosClass: Burstable
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/credentials-secret: f83fbdfae78c625e0f0d9e490afbd2044d84c4ececfeed24133f02d487e506b9
    creationTimestamp: "2024-06-19T22:20:59Z"
    generateName: minio-
    labels:
      app.kubernetes.io/instance: minio
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: minio
      app.kubernetes.io/version: 2024.6.13
      apps.kubernetes.io/pod-index: "2"
      controller-revision-hash: minio-56bfc7b957
      helm.sh/chart: minio-14.6.11
      statefulset.kubernetes.io/pod-name: minio-2
    name: minio-2
    namespace: minio
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: minio
      uid: a6e6347b-3fbf-49bc-927d-e79ce0e17bd6
    resourceVersion: "7815718"
    uid: 331c1ac5-8a65-46e6-96dd-01c9c8b68a04
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/instance: minio
                app.kubernetes.io/name: minio
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: MINIO_DISTRIBUTED_MODE_ENABLED
        value: "yes"
      - name: MINIO_DISTRIBUTED_NODES
        value: minio-{0...2}.minio-headless.minio.svc.cluster.local:9000/bitnami/minio/data-{0...1}
      - name: MINIO_SCHEME
        value: http
      - name: MINIO_FORCE_NEW_KEYS
        value: "no"
      - name: MINIO_ROOT_USER
        valueFrom:
          secretKeyRef:
            key: root-user
            name: minio
      - name: MINIO_ROOT_PASSWORD
        valueFrom:
          secretKeyRef:
            key: root-password
            name: minio
      - name: MINIO_SKIP_CLIENT
        value: "yes"
      - name: MINIO_BROWSER
        value: "on"
      - name: MINIO_PROMETHEUS_AUTH_TYPE
        value: public
      - name: MINIO_DATA_DIR
        value: /bitnami/minio/data-0
      image: docker.io/bitnami/minio:2024.4.18-debian-12-r0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /minio/health/live
          port: minio-api
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      name: minio
      ports:
      - containerPort: 9000
        name: minio-api
        protocol: TCP
      - containerPort: 9001
        name: minio-console
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        tcpSocket:
          port: minio-api
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "2"
          memory: 800Mi
        requests:
          cpu: "1"
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
      - mountPath: /opt/bitnami/minio/tmp
        name: empty-dir
        subPath: app-tmp-dir
      - mountPath: /.mc
        name: empty-dir
        subPath: app-mc-dir
      - mountPath: /bitnami/minio/data-0
        name: data-0
      - mountPath: /bitnami/minio/data-1
        name: data-1
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: minio-2
    initContainers:
    - command:
      - /bin/bash
      - -ec
      - "chown -R 1001:1001 /bitnami/minio/data-0 /bitnami/minio/data-1 \n"
      image: docker.io/bitnami/os-shell:12-debian-12-r18
      imagePullPolicy: IfNotPresent
      name: volume-permissions
      resources:
        limits:
          cpu: 150m
          ephemeral-storage: 1Gi
          memory: 192Mi
        requests:
          cpu: 100m
          ephemeral-storage: 50Mi
          memory: 128Mi
      securityContext:
        runAsUser: 0
        seLinuxOptions: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
      - mountPath: /bitnami/minio/data-0
        name: data-0
      - mountPath: /bitnami/minio/data-1
        name: data-1
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: OnRootMismatch
    serviceAccount: minio
    serviceAccountName: minio
    subdomain: minio-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data-0
      persistentVolumeClaim:
        claimName: data-0-minio-2
    - name: data-1
      persistentVolumeClaim:
        claimName: data-1-minio-2
    - emptyDir: {}
      name: empty-dir
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-19T22:21:23Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-19T22:21:51Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:26:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:26:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-19T22:20:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f29ec27978903a70d126dd9aa3c1e00b85d9549e09f7e716929fab072d880aa4
      image: docker.io/bitnami/minio:2024.4.18-debian-12-r0
      imageID: docker.io/bitnami/minio@sha256:126dde449621531356426d59f9bac6aa493d91b3424e9d1f63b28b509521e8ee
      lastState:
        terminated:
          containerID: containerd://2d640c515932fc50fb9980b26bdc4d70a31f76fe2f15be56b816dc613bcba7cb
          exitCode: 137
          finishedAt: "2024-07-01T07:26:13Z"
          reason: OOMKilled
          startedAt: "2024-07-01T07:07:12Z"
      name: minio
      ready: true
      restartCount: 122
      started: true
      state:
        running:
          startedAt: "2024-07-01T07:26:14Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    initContainerStatuses:
    - containerID: containerd://e6f78e1e25936d25ea6b38f3193041a7af51c15cbca47c67cae3a90503c19f46
      image: docker.io/bitnami/os-shell:12-debian-12-r18
      imageID: docker.io/bitnami/os-shell@sha256:308932799b147e42e0fb10ce673a31caf22fa28b902473e7a032cf5a270ebc36
      lastState: {}
      name: volume-permissions
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://e6f78e1e25936d25ea6b38f3193041a7af51c15cbca47c67cae3a90503c19f46
          exitCode: 0
          finishedAt: "2024-06-19T22:21:51Z"
          reason: Completed
          startedAt: "2024-06-19T22:21:22Z"
    phase: Running
    podIP: 10.244.0.111
    podIPs:
    - ip: 10.244.0.111
    qosClass: Burstable
    startTime: "2024-06-19T22:20:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: alertmanager
    creationTimestamp: "2024-06-16T08:51:51Z"
    generateName: alertmanager-prometheus-kube-prometheus-alertmanager-
    labels:
      alertmanager: prometheus-kube-prometheus-alertmanager
      app.kubernetes.io/instance: prometheus-kube-prometheus-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.27.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: alertmanager-prometheus-kube-prometheus-alertmanager-5dc495957f
      statefulset.kubernetes.io/pod-name: alertmanager-prometheus-kube-prometheus-alertmanager-0
    name: alertmanager-prometheus-kube-prometheus-alertmanager-0
    namespace: prometheus
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-prometheus-kube-prometheus-alertmanager
      uid: 64cc3ccf-2a14-4efc-8dfd-e3bfedd9a409
    resourceVersion: "2393692"
    uid: b9afeda6-a49b-4c4d-97a9-58c9237a4660
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --storage.path=/alertmanager
      - --data.retention=120h
      - --cluster.listen-address=
      - --web.listen-address=:9093
      - --web.external-url=http://prometheus-kube-prometheus-alertmanager.prometheus:9093
      - --web.route-prefix=/
      - --cluster.label=prometheus/prometheus-kube-prometheus-alertmanager
      - --cluster.peer=alertmanager-prometheus-kube-prometheus-alertmanager-0.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.27.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http-web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-prometheus-kube-prometheus-alertmanager-db
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nv4w6
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9093/-/reload
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nv4w6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-prometheus-kube-prometheus-alertmanager-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8080
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nv4w6
        readOnly: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-kube-prometheus-alertmanager
    serviceAccountName: prometheus-kube-prometheus-alertmanager
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-prometheus-kube-prometheus-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-prometheus-kube-prometheus-alertmanager-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - name: web-config
      secret:
        defaultMode: 420
        secretName: alertmanager-prometheus-kube-prometheus-alertmanager-web-config
    - emptyDir: {}
      name: alertmanager-prometheus-kube-prometheus-alertmanager-db
    - name: kube-api-access-nv4w6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:53Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:54Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:52:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:52:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5d2e26ca556e0a7ba4e59df699ddd62ecdd8d2c4764bbf26114d06d3f98b7e3d
      image: quay.io/prometheus/alertmanager:v0.27.0
      imageID: quay.io/prometheus/alertmanager@sha256:e13b6ed5cb929eeaee733479dce55e10eb3bc2e9c4586c705a4e8da41e5eacf5
      lastState: {}
      name: alertmanager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-16T08:52:01Z"
    - containerID: containerd://441f22fabf32c04928c6069c356e474df8cdc6cb7326dac6522220cfc0831e3a
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:d55631c7a740d355eabf3bdc9e48c1e91d0f08d10d861c8928b4df4b3b96b4c9
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-16T08:52:01Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    initContainerStatuses:
    - containerID: containerd://1f0b897fb7161b6c111542bb0a7a17f9d2c787b6478bbb65534b5eb27fc5e9ad
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:d55631c7a740d355eabf3bdc9e48c1e91d0f08d10d861c8928b4df4b3b96b4c9
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://1f0b897fb7161b6c111542bb0a7a17f9d2c787b6478bbb65534b5eb27fc5e9ad
          exitCode: 0
          finishedAt: "2024-06-16T08:51:54Z"
          reason: Completed
          startedAt: "2024-06-16T08:51:53Z"
    phase: Running
    podIP: 10.244.0.6
    podIPs:
    - ip: 10.244.0.6
    qosClass: Burstable
    startTime: "2024-06-16T08:51:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: d8d9157e07caef7c3a90b40c8373d900f3c676a242c290ba75f122344fe1a0cd
      checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
      checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
      kubectl.kubernetes.io/default-container: grafana
    creationTimestamp: "2024-06-16T09:32:00Z"
    generateName: prometheus-grafana-7c4bd5956d-
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: grafana
      pod-template-hash: 7c4bd5956d
    name: prometheus-grafana-7c4bd5956d-8wmvh
    namespace: prometheus
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-grafana-7c4bd5956d
      uid: 3fe9dd25-4cba-4390-b9de-1a5584cec52b
    resourceVersion: "3095578"
    uid: 9387cb87-6b93-4ca1-98e2-43ce79b6481b
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_dashboard
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /tmp/dashboards
      - name: RESOURCE
        value: both
      - name: NAMESPACE
        value: ALL
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: prometheus-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: prometheus-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/dashboards/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.26.1
      imagePullPolicy: IfNotPresent
      name: grafana-sc-dashboard
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7jhpq
        readOnly: true
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_datasource
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /etc/grafana/provisioning/datasources
      - name: RESOURCE
        value: both
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: prometheus-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: prometheus-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/datasources/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.26.1
      imagePullPolicy: IfNotPresent
      name: grafana-sc-datasources
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7jhpq
        readOnly: true
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: GF_SECURITY_ADMIN_USER
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: prometheus-grafana
      - name: GF_SECURITY_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: prometheus-grafana
      - name: GF_PATHS_DATA
        value: /var/lib/grafana/
      - name: GF_PATHS_LOGS
        value: /var/log/grafana
      - name: GF_PATHS_PLUGINS
        value: /var/lib/grafana/plugins
      - name: GF_PATHS_PROVISIONING
        value: /etc/grafana/provisioning
      image: docker.io/grafana/grafana:11.0.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: grafana
      ports:
      - containerPort: 3000
        name: grafana
        protocol: TCP
      - containerPort: 9094
        name: gossip-tcp
        protocol: TCP
      - containerPort: 9094
        name: gossip-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
        subPath: grafana.ini
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
        name: sc-dashboard-provider
        subPath: provider.yaml
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7jhpq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 472
      runAsGroup: 472
      runAsNonRoot: true
      runAsUser: 472
    serviceAccount: prometheus-grafana
    serviceAccountName: prometheus-grafana
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: prometheus-grafana
      name: config
    - emptyDir: {}
      name: storage
    - emptyDir: {}
      name: sc-dashboard-volume
    - configMap:
        defaultMode: 420
        name: prometheus-grafana-config-dashboards
      name: sc-dashboard-provider
    - emptyDir: {}
      name: sc-datasources-volume
    - name: kube-api-access-7jhpq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T09:32:02Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T09:32:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-18T05:11:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-18T05:11:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T09:32:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f1d69c06162cc5c562df5663c579c271ec5b4b9b92b37df96457c19d39e8be9c
      image: docker.io/grafana/grafana:11.0.0
      imageID: docker.io/grafana/grafana@sha256:0dc5a246ab16bb2c38a349fb588174e832b4c6c2db0981d0c3e6cd774ba66a54
      lastState: {}
      name: grafana
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-16T09:32:02Z"
    - containerID: containerd://ed84ab85906764c4e408f11694f44e727706add3e96ac5756865e084e5577f69
      image: quay.io/kiwigrid/k8s-sidecar:1.26.1
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:b8d5067137fec093cf48670dc3a1dbb38f9e734f3a6683015c2e89a45db5fd16
      lastState: {}
      name: grafana-sc-dashboard
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-16T09:32:01Z"
    - containerID: containerd://985a18fc78802f45ec8368b8c5975fbece039f1a01a7ea3bcd262b2edd6c7a78
      image: quay.io/kiwigrid/k8s-sidecar:1.26.1
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:b8d5067137fec093cf48670dc3a1dbb38f9e734f3a6683015c2e89a45db5fd16
      lastState: {}
      name: grafana-sc-datasources
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-16T09:32:01Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.244.0.28
    podIPs:
    - ip: 10.244.0.28
    qosClass: BestEffort
    startTime: "2024-06-16T09:32:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-06-16T08:51:44Z"
    generateName: prometheus-kube-prometheus-operator-64776975f5-
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 60.1.0
      chart: kube-prometheus-stack-60.1.0
      heritage: Helm
      pod-template-hash: 64776975f5
      release: prometheus
    name: prometheus-kube-prometheus-operator-64776975f5-wh4rz
    namespace: prometheus
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-kube-prometheus-operator-64776975f5
      uid: 8cc744fd-aab7-49b0-b142-ad9fb47f2349
    resourceVersion: "2393483"
    uid: ae9746b7-d500-455f-a05b-8d1a144c8abb
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --kubelet-service=kube-system/prometheus-kube-prometheus-kubelet
      - --localhost=127.0.0.1
      - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
      - --config-reloader-cpu-request=0
      - --config-reloader-cpu-limit=0
      - --config-reloader-memory-request=0
      - --config-reloader-memory-limit=0
      - --thanos-default-base-image=quay.io/thanos/thanos:v0.35.1
      - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
      - --web.enable-tls=true
      - --web.cert-file=/cert/cert
      - --web.key-file=/cert/key
      - --web.listen-address=:10250
      - --web.tls-min-version=VersionTLS13
      env:
      - name: GOGC
        value: "30"
      image: quay.io/prometheus-operator/prometheus-operator:v0.74.0
      imagePullPolicy: IfNotPresent
      name: kube-prometheus-stack
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /cert
        name: tls-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kmqzn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-kube-prometheus-operator
    serviceAccountName: prometheus-kube-prometheus-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: tls-secret
      secret:
        defaultMode: 420
        secretName: prometheus-kube-prometheus-admission
    - name: kube-api-access-kmqzn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:51Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:51Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:51Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4b0280f71c1bd88534f31d471dcd6d303dce007dac95968cb2fae3b96d5cc297
      image: quay.io/prometheus-operator/prometheus-operator:v0.74.0
      imageID: quay.io/prometheus-operator/prometheus-operator@sha256:6b3f6d8b4c0a6c77dfd447ee5870f6abd939151915934f1c61e379a039798030
      lastState: {}
      name: kube-prometheus-stack
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-16T08:51:51Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.244.0.40
    podIPs:
    - ip: 10.244.0.40
    qosClass: BestEffort
    startTime: "2024-06-16T08:51:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-06-16T08:51:44Z"
    generateName: prometheus-kube-state-metrics-6c97d9f546-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.12.0
      helm.sh/chart: kube-state-metrics-5.20.0
      pod-template-hash: 6c97d9f546
      release: prometheus
    name: prometheus-kube-state-metrics-6c97d9f546-lwcrh
    namespace: prometheus
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-kube-state-metrics-6c97d9f546
      uid: ef778e80-5f95-4745-b6ad-d0308204ef68
    resourceVersion: "3095604"
    uid: 0f079ca1-c8a6-4d21-bfe8-724ec313c63c
  spec:
    containers:
    - args:
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4q8kg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-kube-state-metrics
    serviceAccountName: prometheus-kube-state-metrics
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-4q8kg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:52:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-18T05:11:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-18T05:11:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://578e9b0f3bcd94a8e1f56e41f3f4763c563e64fe4ee3cfe1e5f953e243a7c16b
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0
      imageID: registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:b401fae262a5decf83c4311083f8efb4d6ca7b6a733e57b95344cb8dccd14e11
      lastState:
        terminated:
          containerID: containerd://469ab146bbcdcce9cdc1fbad9f48947b4aacd0e991c6fbbe93b97bbc5aa97047
          exitCode: 2
          finishedAt: "2024-06-18T05:11:21Z"
          reason: Error
          startedAt: "2024-06-16T08:52:05Z"
      name: kube-state-metrics
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-06-18T05:11:28Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.244.0.19
    podIPs:
    - ip: 10.244.0.19
    qosClass: BestEffort
    startTime: "2024-06-16T08:51:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: prometheus
    creationTimestamp: "2024-06-16T09:32:04Z"
    generateName: prometheus-prometheus-kube-prometheus-prometheus-
    labels:
      app.kubernetes.io/instance: prometheus-kube-prometheus-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 2.52.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: prometheus-prometheus-kube-prometheus-prometheus-87649c98c
      operator.prometheus.io/name: prometheus-kube-prometheus-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: prometheus-kube-prometheus-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-prometheus-kube-prometheus-prometheus-0
    name: prometheus-prometheus-kube-prometheus-prometheus-0
    namespace: prometheus
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-prometheus-kube-prometheus-prometheus
      uid: 1a165e0a-2ba5-4669-bac9-c110eded300b
    resourceVersion: "5341162"
    uid: 2ed79e63-4751-4c08-9328-57005705ad26
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --web.enable-lifecycle
      - --web.external-url=http://prometheus.minhtuyenvp02.id.vn/
      - --web.route-prefix=/
      - --storage.tsdb.retention.time=10d
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.wal-compression
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      image: quay.io/prometheus/prometheus:v2.52.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      name: prometheus
      ports:
      - containerPort: 9090
        name: http-web
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 3
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
      - mountPath: /prometheus
        name: prometheus-prometheus-kube-prometheus-prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
        name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cz7t4
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9090/-/reload
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
        name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cz7t4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-prometheus-kube-prometheus-prometheus-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8080
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
        name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cz7t4
        readOnly: true
    nodeName: pool-1em8noa5l-rwr1p
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-kube-prometheus-prometheus
    serviceAccountName: prometheus-kube-prometheus-prometheus
    shareProcessNamespace: false
    subdomain: prometheus-operated
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: prometheus-prometheus-kube-prometheus-prometheus
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: prometheus-prometheus-kube-prometheus-prometheus-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - configMap:
        defaultMode: 420
        name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
      name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
    - name: web-config
      secret:
        defaultMode: 420
        secretName: prometheus-prometheus-kube-prometheus-prometheus-web-config
    - emptyDir: {}
      name: prometheus-prometheus-kube-prometheus-prometheus-db
    - name: kube-api-access-cz7t4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T09:32:10Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T09:32:11Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T11:38:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T11:38:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T09:32:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://785c28cd7d3ed3b3d2fe6a82d924ce6e60747c33f8fecb2d39bbb67630e46e06
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:d55631c7a740d355eabf3bdc9e48c1e91d0f08d10d861c8928b4df4b3b96b4c9
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-16T09:32:19Z"
    - containerID: containerd://5f02c392be24b97836e65a20a5840321ff7dc3ba6da6733f25ac133e2a2a2317
      image: quay.io/prometheus/prometheus:v2.52.0
      imageID: quay.io/prometheus/prometheus@sha256:5c435642ca4d8427ca26f4901c11114023004709037880cd7860d5b7176aa731
      lastState:
        terminated:
          containerID: containerd://151081daa49ba4dca4d5eb97241a16b579518af85be3fc2efacec75fc6a9074b
          exitCode: 0
          finishedAt: "2024-06-24T11:37:50Z"
          reason: Completed
          startedAt: "2024-06-19T18:00:27Z"
      name: prometheus
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2024-06-24T11:37:51Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    initContainerStatuses:
    - containerID: containerd://581800e22407e289aac09b48b13bc7f3026371264d17ae4140dce60090fbaaba
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:d55631c7a740d355eabf3bdc9e48c1e91d0f08d10d861c8928b4df4b3b96b4c9
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://581800e22407e289aac09b48b13bc7f3026371264d17ae4140dce60090fbaaba
          exitCode: 0
          finishedAt: "2024-06-16T09:32:10Z"
          reason: Completed
          startedAt: "2024-06-16T09:32:10Z"
    phase: Running
    podIP: 10.244.26.73
    podIPs:
    - ip: 10.244.26.73
    qosClass: BestEffort
    startTime: "2024-06-16T09:32:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2024-06-16T08:51:43Z"
    generateName: prometheus-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.1
      controller-revision-hash: 66bcd5f56f
      helm.sh/chart: prometheus-node-exporter-4.36.0
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: prometheus
    name: prometheus-prometheus-node-exporter-tcg8d
    namespace: prometheus
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-prometheus-node-exporter
      uid: 856c8417-6598-4615-9c86-d8420269cd9f
    resourceVersion: "3095600"
    uid: 79892a4f-0c33-42ef-a7ba-3f5c61ea289f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pool-1em8noa5l-rw5at
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.8.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: pool-1em8noa5l-rw5at
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-prometheus-node-exporter
    serviceAccountName: prometheus-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:44Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-18T05:11:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-18T05:11:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cefdfa53123696c1116ab0a3d5e8ebeb77c9f02eae203b6ef90c6c674f5ccbdf
      image: quay.io/prometheus/node-exporter:v1.8.1
      imageID: quay.io/prometheus/node-exporter@sha256:fa7fa12a57eff607176d5c363d8bb08dfbf636b36ac3cb5613a202f3c61a6631
      lastState:
        terminated:
          containerID: containerd://dad85d27a83285ee2d3c56e09b76b9a1b96af97bc0c4e1bb12d35c49b4f00468
          exitCode: 143
          finishedAt: "2024-06-18T05:11:19Z"
          reason: Error
          startedAt: "2024-06-16T11:47:56Z"
      name: node-exporter
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2024-06-18T05:11:21Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.104.0.7
    podIPs:
    - ip: 10.104.0.7
    qosClass: BestEffort
    startTime: "2024-06-16T08:51:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2024-06-16T08:51:43Z"
    generateName: prometheus-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.1
      controller-revision-hash: 66bcd5f56f
      helm.sh/chart: prometheus-node-exporter-4.36.0
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: prometheus
    name: prometheus-prometheus-node-exporter-xxf84
    namespace: prometheus
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-prometheus-node-exporter
      uid: 856c8417-6598-4615-9c86-d8420269cd9f
    resourceVersion: "5341036"
    uid: 660d484b-ed60-4299-a185-0fd6497e6d82
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pool-1em8noa5l-rwr1p
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.8.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: pool-1em8noa5l-rwr1p
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-prometheus-node-exporter
    serviceAccountName: prometheus-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:44Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T11:37:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-24T11:37:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-16T08:51:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://faa367c9aa8296d2d52d1f8c262363de1484d41d1c4bed276f9cd5289462d80f
      image: quay.io/prometheus/node-exporter:v1.8.1
      imageID: quay.io/prometheus/node-exporter@sha256:fa7fa12a57eff607176d5c363d8bb08dfbf636b36ac3cb5613a202f3c61a6631
      lastState:
        terminated:
          containerID: containerd://0fa9ac5e583cf7178fabab86bc0bc64fdc55132e3c0b441c0a2b83dd6099f5b1
          exitCode: 143
          finishedAt: "2024-06-24T11:37:47Z"
          reason: Error
          startedAt: "2024-06-19T18:00:16Z"
      name: node-exporter
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2024-06-24T11:37:48Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    phase: Running
    podIP: 10.104.0.3
    podIPs:
    - ip: 10.104.0.3
    qosClass: BestEffort
    startTime: "2024-06-16T08:51:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "10254"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-06-28T21:04:03Z"
    generateName: spark-spark-operator-f88695599-
    labels:
      app.kubernetes.io/instance: spark
      app.kubernetes.io/name: spark-operator
      pod-template-hash: f88695599
    name: spark-spark-operator-f88695599-g4chb
    namespace: spark
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: spark-spark-operator-f88695599
      uid: 5820e2b8-0ca7-4c0d-90f2-842186082ec6
    resourceVersion: "6888681"
    uid: 677d040b-a0ef-42df-bd1b-7348c6fb4d4e
  spec:
    containers:
    - args:
      - -v=2
      - -logtostderr
      - -namespace=
      - -enable-ui-service=true
      - -ingress-url-format=spark-app.minhtuyenvp02.id.vn/{{$appNamespace}}/{{$appName}}
      - -controller-threads=10
      - -resync-interval=30
      - -enable-batch-scheduler=false
      - -label-selector-filter=
      - -enable-metrics=true
      - -metrics-labels=app_type
      - -metrics-port=10254
      - -metrics-endpoint=/metrics
      - -metrics-prefix=
      - -enable-webhook=true
      - -webhook-secret-name=spark-spark-operator-webhook-certs
      - -webhook-secret-namespace=spark
      - -webhook-svc-name=spark-spark-operator-webhook-svc
      - -webhook-svc-namespace=spark
      - -webhook-config-name=spark-spark-operator-webhook-config
      - -webhook-port=8080
      - -webhook-timeout=30
      - -webhook-namespace-selector=
      - -enable-resource-quota-enforcement=false
      - -leader-election=true
      - -leader-election-lock-namespace=spark
      - -leader-election-lock-name=spark-operator-lock
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: docker.io/kubeflow/spark-operator:v1beta2-1.6.0-3.5.0
      imagePullPolicy: IfNotPresent
      name: spark-operator
      ports:
      - containerPort: 10254
        name: metrics
        protocol: TCP
      - containerPort: 8080
        name: webhook
        protocol: TCP
      resources: {}
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x2jms
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: spark-spark-operator
    serviceAccountName: spark-spark-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-x2jms
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T21:04:04Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T21:04:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T21:04:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T21:04:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T21:04:03Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c8cf3e3cf4289490b58271a5f1ed859ef2efe171af42db19168ee870d38323b1
      image: docker.io/kubeflow/spark-operator:v1beta2-1.6.0-3.5.0
      imageID: docker.io/kubeflow/spark-operator@sha256:5006ee0bc49954ea5a69771e08aebae0216fc5c27799cb82362d0b90554e0bf1
      lastState: {}
      name: spark-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-28T21:04:04Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.244.0.25
    podIPs:
    - ip: 10.244.0.25
    qosClass: BestEffort
    startTime: "2024-06-28T21:04:03Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "10254"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-07-01T07:28:24Z"
    generateName: spark-spark-operator-f88695599-
    labels:
      app.kubernetes.io/instance: spark
      app.kubernetes.io/name: spark-operator
      pod-template-hash: f88695599
    name: spark-spark-operator-f88695599-lt2vm
    namespace: spark
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: spark-spark-operator-f88695599
      uid: 5820e2b8-0ca7-4c0d-90f2-842186082ec6
    resourceVersion: "7816601"
    uid: 5c0d35e4-033f-4436-a664-53a5d2e9e340
  spec:
    containers:
    - args:
      - -v=2
      - -logtostderr
      - -namespace=
      - -enable-ui-service=true
      - -ingress-url-format=spark-app.minhtuyenvp02.id.vn/{{$appNamespace}}/{{$appName}}
      - -controller-threads=10
      - -resync-interval=30
      - -enable-batch-scheduler=false
      - -label-selector-filter=
      - -enable-metrics=true
      - -metrics-labels=app_type
      - -metrics-port=10254
      - -metrics-endpoint=/metrics
      - -metrics-prefix=
      - -enable-webhook=true
      - -webhook-secret-name=spark-spark-operator-webhook-certs
      - -webhook-secret-namespace=spark
      - -webhook-svc-name=spark-spark-operator-webhook-svc
      - -webhook-svc-namespace=spark
      - -webhook-config-name=spark-spark-operator-webhook-config
      - -webhook-port=8080
      - -webhook-timeout=30
      - -webhook-namespace-selector=
      - -enable-resource-quota-enforcement=false
      - -leader-election=true
      - -leader-election-lock-namespace=spark
      - -leader-election-lock-name=spark-operator-lock
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: docker.io/kubeflow/spark-operator:v1beta2-1.6.0-3.5.0
      imagePullPolicy: IfNotPresent
      name: spark-operator
      ports:
      - containerPort: 10254
        name: metrics
        protocol: TCP
      - containerPort: 8080
        name: webhook
        protocol: TCP
      resources: {}
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9vq69
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pool-1em8noa5l-rwr1p
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: spark-spark-operator
    serviceAccountName: spark-spark-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-9vq69
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:25Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:24Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:25Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:25Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://98d779ed1ca2494d7199ec43e21fbea6c187e772b62bda0fa35901b4b2f2c90e
      image: docker.io/kubeflow/spark-operator:v1beta2-1.6.0-3.5.0
      imageID: docker.io/kubeflow/spark-operator@sha256:5006ee0bc49954ea5a69771e08aebae0216fc5c27799cb82362d0b90554e0bf1
      lastState: {}
      name: spark-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-07-01T07:28:24Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    phase: Running
    podIP: 10.244.26.95
    podIPs:
    - ip: 10.244.26.95
    qosClass: BestEffort
    startTime: "2024-07-01T07:28:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "10254"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-06-28T21:04:03Z"
    generateName: spark-spark-operator-f88695599-
    labels:
      app.kubernetes.io/instance: spark
      app.kubernetes.io/name: spark-operator
      pod-template-hash: f88695599
    name: spark-spark-operator-f88695599-lvgmp
    namespace: spark
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: spark-spark-operator-f88695599
      uid: 5820e2b8-0ca7-4c0d-90f2-842186082ec6
    resourceVersion: "6888694"
    uid: 36a6e797-9749-4693-855a-b7a0795f32cd
  spec:
    containers:
    - args:
      - -v=2
      - -logtostderr
      - -namespace=
      - -enable-ui-service=true
      - -ingress-url-format=spark-app.minhtuyenvp02.id.vn/{{$appNamespace}}/{{$appName}}
      - -controller-threads=10
      - -resync-interval=30
      - -enable-batch-scheduler=false
      - -label-selector-filter=
      - -enable-metrics=true
      - -metrics-labels=app_type
      - -metrics-port=10254
      - -metrics-endpoint=/metrics
      - -metrics-prefix=
      - -enable-webhook=true
      - -webhook-secret-name=spark-spark-operator-webhook-certs
      - -webhook-secret-namespace=spark
      - -webhook-svc-name=spark-spark-operator-webhook-svc
      - -webhook-svc-namespace=spark
      - -webhook-config-name=spark-spark-operator-webhook-config
      - -webhook-port=8080
      - -webhook-timeout=30
      - -webhook-namespace-selector=
      - -enable-resource-quota-enforcement=false
      - -leader-election=true
      - -leader-election-lock-namespace=spark
      - -leader-election-lock-name=spark-operator-lock
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: docker.io/kubeflow/spark-operator:v1beta2-1.6.0-3.5.0
      imagePullPolicy: IfNotPresent
      name: spark-operator
      ports:
      - containerPort: 10254
        name: metrics
        protocol: TCP
      - containerPort: 8080
        name: webhook
        protocol: TCP
      resources: {}
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gqvks
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pool-1em8noa5l-rwr1p
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: spark-spark-operator
    serviceAccountName: spark-spark-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-gqvks
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T21:04:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T21:04:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T21:04:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T21:04:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T21:04:03Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4316301058d74c4c0db663d761e0ac73a86bd7785256bdf7aa0261a6e4173c47
      image: docker.io/kubeflow/spark-operator:v1beta2-1.6.0-3.5.0
      imageID: docker.io/kubeflow/spark-operator@sha256:5006ee0bc49954ea5a69771e08aebae0216fc5c27799cb82362d0b90554e0bf1
      lastState: {}
      name: spark-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-28T21:04:04Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    phase: Running
    podIP: 10.244.26.48
    podIPs:
    - ip: 10.244.26.48
    qosClass: BestEffort
    startTime: "2024-06-28T21:04:03Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configOverrides: 216350f2670d8588217c1f8fb1cd345561207481b104501b0c79aa441ffc6f81
      checksum/configOverridesFiles: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      checksum/connections: 2d41a2c51ec2fc809be84fc0ea0e603c946d529b4a0e43f7257e617909561a75
      checksum/extraConfigs: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      checksum/extraSecretEnv: 72fbd486f368264fd746d16cc7e0316a6fc004ed1483dda1e4b8bde8662b6217
      checksum/extraSecrets: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      checksum/superset_bootstrap.sh: db546af547fe8beae35fb3461d8ee461b0e6fd06b9c289da0ba745bc34b6f35f
      checksum/superset_config.py: a02e301df0b06bb7adf5b4a4f13f965d4de2467d0006cd6c2728c2e4f7426ee1
      checksum/superset_init.sh: e6b1e8eac1f7a79a07a6c72a0e2ee6e09654eeb439c6bbe61bfd676917c41e02
    creationTimestamp: "2024-06-28T11:18:08Z"
    generateName: superset-67dbcb86b6-
    labels:
      app: superset
      pod-template-hash: 67dbcb86b6
      release: superset
    name: superset-67dbcb86b6-lbvpm
    namespace: superset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: superset-67dbcb86b6
      uid: d795c1fd-de04-4d09-902a-50eba644ace5
    resourceVersion: "6737958"
    uid: 9e63fdab-8ba3-4a5c-98dc-59e45aa87d99
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - . /app/pythonpath/superset_bootstrap.sh; /usr/bin/run-server.sh
      env:
      - name: SUPERSET_PORT
        value: "8088"
      envFrom:
      - secretRef:
          name: superset-env
      image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 1
      name: superset
      ports:
      - containerPort: 8088
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/pythonpath
        name: superset-config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-v4r82
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/sh
      - -c
      - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -timeout 120s
      envFrom:
      - secretRef:
          name: superset-env
      image: apache/superset:dockerize
      imagePullPolicy: IfNotPresent
      name: wait-for-postgres
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-v4r82
        readOnly: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsUser: 0
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: superset-config
      secret:
        defaultMode: 420
        secretName: superset-config
    - name: kube-api-access-v4r82
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T11:18:09Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T11:18:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T11:19:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T11:19:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T11:18:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b82b95ccf2574d704576fcaa0acaf5c18fcfdb007e4e417bc075bcb10dcc68c5
      image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
      imageID: apachesuperset.docker.scarf.sh/apache/superset@sha256:ab9467fd712cbe3738d1a7d574d6230c0b04a88af5e5e11f3fbc38e2af305162
      lastState: {}
      name: superset
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-06-28T11:18:10Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    initContainerStatuses:
    - containerID: containerd://01d9af8d97a8e61b494c88cabf9fd43c3cb69f8472f678374456c8de377dd572
      image: docker.io/apache/superset:dockerize
      imageID: docker.io/apache/superset@sha256:afe59523a6c8774c3b16d0f44146b2e52f327a7d26a47b4cc63b904fcdedf057
      lastState: {}
      name: wait-for-postgres
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://01d9af8d97a8e61b494c88cabf9fd43c3cb69f8472f678374456c8de377dd572
          exitCode: 0
          finishedAt: "2024-06-28T11:18:09Z"
          reason: Completed
          startedAt: "2024-06-28T11:18:09Z"
    phase: Running
    podIP: 10.244.0.45
    podIPs:
    - ip: 10.244.0.45
    qosClass: BestEffort
    startTime: "2024-06-28T11:18:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-07-01T07:28:57Z"
    generateName: superset-postgresql-
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: superset
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: superset-postgresql-6c6cb67ff4
      helm.sh/chart: postgresql-12.1.6
      statefulset.kubernetes.io/pod-name: superset-postgresql-0
    name: superset-postgresql-0
    namespace: superset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: superset-postgresql
      uid: 34b79911-c3c2-4fa7-887f-1aa8d2c51bab
    resourceVersion: "7818585"
    uid: da08960b-c48a-4eff-9bc0-918aa2b84eee
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: primary
                app.kubernetes.io/instance: superset
                app.kubernetes.io/name: postgresql
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_USER
        value: superset
      - name: POSTGRES_POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: postgres-password
            name: superset-postgresql
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: superset-postgresql
      - name: POSTGRES_DB
        value: superset
      - name: POSTGRESQL_ENABLE_LDAP
        value: "no"
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "false"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit
      image: docker.io/bitnami/postgresql:14.6.0-debian-11-r13
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "superset" -d "dbname=superset" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: tcp-postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - -e
          - |
            exec pg_isready -U "superset" -d "dbname=superset" -h 127.0.0.1 -p 5432
            [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        runAsUser: 1001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dev/shm
        name: dshm
      - mountPath: /bitnami/postgresql
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x54rq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: superset-postgresql-0
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
    serviceAccount: default
    serviceAccountName: default
    subdomain: superset-postgresql-hl
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-superset-postgresql-0
    - emptyDir:
        medium: Memory
      name: dshm
    - name: kube-api-access-x54rq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:57Z"
      message: '0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes
        are available: 2 No preemption victims found for incoming pod.'
      reason: Unschedulable
      status: "False"
      type: PodScheduled
    phase: Pending
    qosClass: Burstable
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configmap: fe1cca2dea4020cb4b1623fc950515f7338ac9fd6e0328821274263d9e0e4d64
      checksum/health: 73f335f1a98f6f244d0ef2e3f93af8046efda1328e0421e6099d13241de96558
      checksum/scripts: 9790c77d02ac85c0f4b6067139404f8bc34ddcdf1da13c05021f0904c0451a8d
      checksum/secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    creationTimestamp: "2024-07-01T07:28:27Z"
    generateName: superset-redis-master-
    labels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: superset
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: superset-redis-master-6bcdc58944
      helm.sh/chart: redis-17.9.4
      statefulset.kubernetes.io/pod-name: superset-redis-master-0
    name: superset-redis-master-0
    namespace: superset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: superset-redis-master
      uid: 4441c812-7d4e-4ab4-8ee4-5d08a881a2fe
    resourceVersion: "7817340"
    uid: 9c6680cb-8e8f-40ef-bfe6-95ba0221ca6c
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: master
                app.kubernetes.io/instance: superset
                app.kubernetes.io/name: redis
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - args:
      - -c
      - /opt/bitnami/scripts/start-scripts/start-master.sh
      command:
      - /bin/bash
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: REDIS_REPLICATION_MODE
        value: master
      - name: ALLOW_EMPTY_PASSWORD
        value: "yes"
      - name: REDIS_TLS_ENABLED
        value: "no"
      - name: REDIS_PORT
        value: "6379"
      image: docker.io/bitnami/redis:7.0.10-debian-11-r4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - /health/ping_liveness_local.sh 5
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 6
      name: redis
      ports:
      - containerPort: 6379
        name: redis
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - /health/ping_readiness_local.sh 1
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 2
      resources: {}
      securityContext:
        runAsUser: 1001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/bitnami/scripts/start-scripts
        name: start-scripts
      - mountPath: /health
        name: health
      - mountPath: /data
        name: redis-data
      - mountPath: /opt/bitnami/redis/mounted-etc
        name: config
      - mountPath: /opt/bitnami/redis/etc/
        name: redis-tmp-conf
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t565m
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: superset-redis-master-0
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
    serviceAccount: superset-redis
    serviceAccountName: superset-redis
    subdomain: superset-redis-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 493
        name: superset-redis-scripts
      name: start-scripts
    - configMap:
        defaultMode: 493
        name: superset-redis-health
      name: health
    - configMap:
        defaultMode: 420
        name: superset-redis-configuration
      name: config
    - emptyDir: {}
      name: redis-tmp-conf
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: redis-data
    - name: kube-api-access-t565m
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:40Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:29:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:29:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f4eda523f8a569131a1d84f8a99cd99696cfd85b5255103ec69af05d16d52995
      image: docker.io/bitnami/redis:7.0.10-debian-11-r4
      imageID: docker.io/bitnami/redis@sha256:224a79826b42869bdc72a70933efd840c5a5f10a70caafca68e57be6901e36fb
      lastState: {}
      name: redis
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-07-01T07:28:39Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.244.0.2
    podIPs:
    - ip: 10.244.0.2
    qosClass: BestEffort
    startTime: "2024-07-01T07:28:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configOverrides: 216350f2670d8588217c1f8fb1cd345561207481b104501b0c79aa441ffc6f81
      checksum/configOverridesFiles: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      checksum/connections: 2d41a2c51ec2fc809be84fc0ea0e603c946d529b4a0e43f7257e617909561a75
      checksum/extraConfigs: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      checksum/extraSecretEnv: 72fbd486f368264fd746d16cc7e0316a6fc004ed1483dda1e4b8bde8662b6217
      checksum/extraSecrets: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      checksum/superset_bootstrap.sh: db546af547fe8beae35fb3461d8ee461b0e6fd06b9c289da0ba745bc34b6f35f
      checksum/superset_config.py: a02e301df0b06bb7adf5b4a4f13f965d4de2467d0006cd6c2728c2e4f7426ee1
    creationTimestamp: "2024-06-28T11:17:58Z"
    generateName: superset-worker-6b94cb97f4-
    labels:
      app: superset-worker
      pod-template-hash: 6b94cb97f4
      release: superset
    name: superset-worker-6b94cb97f4-q8w88
    namespace: superset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: superset-worker-6b94cb97f4
      uid: b396b516-877b-4e05-8c02-1bbbe010de83
    resourceVersion: "6762414"
    uid: 09498c36-b978-421d-b84e-b9490d1aabd1
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - . /app/pythonpath/superset_bootstrap.sh; celery --app=superset.tasks.celery_app:app
        worker
      env:
      - name: SUPERSET_PORT
        value: "8088"
      envFrom:
      - secretRef:
          name: superset-env
      image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - celery -A superset.tasks.celery_app:app inspect ping -d celery@$HOSTNAME
        failureThreshold: 3
        initialDelaySeconds: 120
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 60
      name: superset
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/pythonpath
        name: superset-config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kzf2m
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/sh
      - -c
      - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -wait "tcp://$REDIS_HOST:$REDIS_PORT"
        -timeout 120s
      envFrom:
      - secretRef:
          name: superset-env
      image: apache/superset:dockerize
      imagePullPolicy: IfNotPresent
      name: wait-for-postgres-redis
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kzf2m
        readOnly: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsUser: 0
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: superset-config
      secret:
        defaultMode: 420
        secretName: superset-config
    - name: kube-api-access-kzf2m
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T12:54:46Z"
      message: 'The node was low on resource: memory. Threshold quantity: 100Mi, available:
        76284Ki. Container superset was using 717160Ki, request is 0, has larger consumption
        of memory. '
      reason: TerminationByKubelet
      status: "True"
      type: DisruptionTarget
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T12:54:46Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T11:18:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T12:54:46Z"
      reason: PodFailed
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T12:54:46Z"
      reason: PodFailed
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T11:17:58Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://351402ed5bbcfb5585cb3d1a959392a0480b59cc5ddde49e16fecefb1e813b27
      image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
      imageID: apachesuperset.docker.scarf.sh/apache/superset@sha256:ab9467fd712cbe3738d1a7d574d6230c0b04a88af5e5e11f3fbc38e2af305162
      lastState: {}
      name: superset
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://351402ed5bbcfb5585cb3d1a959392a0480b59cc5ddde49e16fecefb1e813b27
          exitCode: 137
          finishedAt: "2024-06-28T12:54:46Z"
          reason: Error
          startedAt: "2024-06-28T11:18:53Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    initContainerStatuses:
    - containerID: containerd://56856f28cffe18634e2c744b6683759e0414fae858dc6fed853dea9c04bd15f1
      image: docker.io/apache/superset:dockerize
      imageID: docker.io/apache/superset@sha256:afe59523a6c8774c3b16d0f44146b2e52f327a7d26a47b4cc63b904fcdedf057
      lastState: {}
      name: wait-for-postgres-redis
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://56856f28cffe18634e2c744b6683759e0414fae858dc6fed853dea9c04bd15f1
          exitCode: 0
          finishedAt: "2024-06-28T11:18:52Z"
          reason: Completed
          startedAt: "2024-06-28T11:17:59Z"
    message: 'The node was low on resource: memory. Threshold quantity: 100Mi, available:
      76284Ki. Container superset was using 717160Ki, request is 0, has larger consumption
      of memory. '
    phase: Failed
    podIP: 10.244.0.31
    podIPs:
    - ip: 10.244.0.31
    qosClass: BestEffort
    reason: Evicted
    startTime: "2024-06-28T11:17:58Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configOverrides: 216350f2670d8588217c1f8fb1cd345561207481b104501b0c79aa441ffc6f81
      checksum/configOverridesFiles: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      checksum/connections: 2d41a2c51ec2fc809be84fc0ea0e603c946d529b4a0e43f7257e617909561a75
      checksum/extraConfigs: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      checksum/extraSecretEnv: 72fbd486f368264fd746d16cc7e0316a6fc004ed1483dda1e4b8bde8662b6217
      checksum/extraSecrets: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      checksum/superset_bootstrap.sh: db546af547fe8beae35fb3461d8ee461b0e6fd06b9c289da0ba745bc34b6f35f
      checksum/superset_config.py: a02e301df0b06bb7adf5b4a4f13f965d4de2467d0006cd6c2728c2e4f7426ee1
    creationTimestamp: "2024-07-01T07:28:25Z"
    generateName: superset-worker-6b94cb97f4-
    labels:
      app: superset-worker
      pod-template-hash: 6b94cb97f4
      release: superset
    name: superset-worker-6b94cb97f4-sd2gw
    namespace: superset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: superset-worker-6b94cb97f4
      uid: b396b516-877b-4e05-8c02-1bbbe010de83
    resourceVersion: "7820458"
    uid: 05dbef47-2120-4b5b-bcee-74c92bd9df38
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - . /app/pythonpath/superset_bootstrap.sh; celery --app=superset.tasks.celery_app:app
        worker
      env:
      - name: SUPERSET_PORT
        value: "8088"
      envFrom:
      - secretRef:
          name: superset-env
      image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - celery -A superset.tasks.celery_app:app inspect ping -d celery@$HOSTNAME
        failureThreshold: 3
        initialDelaySeconds: 120
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 60
      name: superset
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/pythonpath
        name: superset-config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mv8wp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/sh
      - -c
      - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -wait "tcp://$REDIS_HOST:$REDIS_PORT"
        -timeout 120s
      envFrom:
      - secretRef:
          name: superset-env
      image: apache/superset:dockerize
      imagePullPolicy: IfNotPresent
      name: wait-for-postgres-redis
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mv8wp
        readOnly: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsUser: 0
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: superset-config
      secret:
        defaultMode: 420
        secretName: superset-config
    - name: kube-api-access-mv8wp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:26Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:29:05Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:40:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:40:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:25Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b0d224d6f828c111f6ba1dfed5c3094c0992d5297759a5f3c12ea66e60927369
      image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
      imageID: apachesuperset.docker.scarf.sh/apache/superset@sha256:ab9467fd712cbe3738d1a7d574d6230c0b04a88af5e5e11f3fbc38e2af305162
      lastState:
        terminated:
          containerID: containerd://a14fd7be1d34b784c890542cf87106869dd6425f567209e46e8a6d2062252dbf
          exitCode: 1
          finishedAt: "2024-07-01T07:39:23Z"
          reason: Error
          startedAt: "2024-07-01T07:37:00Z"
      name: superset
      ready: true
      restartCount: 4
      started: true
      state:
        running:
          startedAt: "2024-07-01T07:40:15Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    initContainerStatuses:
    - containerID: containerd://4405ba28db3ef1c25f55bc6be0c6934e9dec3872ca04939628172262be65b13e
      image: docker.io/apache/superset:dockerize
      imageID: docker.io/apache/superset@sha256:afe59523a6c8774c3b16d0f44146b2e52f327a7d26a47b4cc63b904fcdedf057
      lastState: {}
      name: wait-for-postgres-redis
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://4405ba28db3ef1c25f55bc6be0c6934e9dec3872ca04939628172262be65b13e
          exitCode: 0
          finishedAt: "2024-07-01T07:29:04Z"
          reason: Completed
          startedAt: "2024-07-01T07:28:26Z"
    phase: Running
    podIP: 10.244.0.63
    podIPs:
    - ip: 10.244.0.63
    qosClass: BestEffort
    startTime: "2024-07-01T07:28:25Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-07-01T07:28:54Z"
    generateName: hive-metastore-postgresql-
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hive-metastore-postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.3.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: hive-metastore-postgresql-b59c8b646
      helm.sh/chart: postgresql-15.5.1
      statefulset.kubernetes.io/pod-name: hive-metastore-postgresql-0
    name: hive-metastore-postgresql-0
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: hive-metastore-postgresql
      uid: 6c164673-2de4-4ad3-a649-998fc8ace919
    resourceVersion: "7818595"
    uid: c2714856-72f3-4117-859c-2a146d7f306d
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: primary
                app.kubernetes.io/instance: hive-metastore-postgresql
                app.kubernetes.io/name: postgresql
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_USER
        value: admin
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: hive-metastore-postgresql
      - name: POSTGRES_POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: postgres-password
            name: hive-metastore-postgresql
      - name: POSTGRES_DATABASE
        value: metastore_db
      - name: POSTGRESQL_ENABLE_LDAP
        value: "no"
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "false"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit
      image: docker.io/bitnami/postgresql:16.3.0-debian-12-r12
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "admin" -d "dbname=metastore_db" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: tcp-postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - -e
          - |
            exec pg_isready -U "admin" -d "dbname=metastore_db" -h 127.0.0.1 -p 5432
            [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 150m
          ephemeral-storage: 1Gi
          memory: 192Mi
        requests:
          cpu: 100m
          ephemeral-storage: 50Mi
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
      - mountPath: /opt/bitnami/postgresql/conf
        name: empty-dir
        subPath: app-conf-dir
      - mountPath: /opt/bitnami/postgresql/tmp
        name: empty-dir
        subPath: app-tmp-dir
      - mountPath: /dev/shm
        name: dshm
      - mountPath: /bitnami/postgresql
        name: data
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: hive-metastore-postgresql-0
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: hive-metastore-postgresql
    serviceAccountName: hive-metastore-postgresql
    subdomain: hive-metastore-postgresql-hl
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-hive-metastore-postgresql-0
    - emptyDir: {}
      name: empty-dir
    - emptyDir:
        medium: Memory
      name: dshm
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:54Z"
      message: '0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes
        are available: 2 No preemption victims found for incoming pod.'
      reason: Unschedulable
      status: "False"
      type: PodScheduled
    phase: Pending
    qosClass: Burstable
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-07-01T07:28:54Z"
    generateName: my-hive-metastore-
    labels:
      app.kubernetes.io/component: metastore
      app.kubernetes.io/instance: my-hive-metastore
      app.kubernetes.io/name: hive-metastore
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: my-hive-metastore-6cd9478865
      statefulset.kubernetes.io/pod-name: my-hive-metastore-0
    name: my-hive-metastore-0
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: my-hive-metastore
      uid: afaaefdd-02bb-4945-b75e-5d26f65c0d2a
    resourceVersion: "7817246"
    uid: 6dfb9f19-42ec-489a-a941-02de26c9c3cf
  spec:
    containers:
    - command:
      - bash
      - entrypoint/entrypoint.sh
      image: rtdl/hive-metastore:3.1.2
      imagePullPolicy: IfNotPresent
      name: metastore
      ports:
      - containerPort: 9083
        name: thrift
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/apache-hive-metastore-3.1.2-bin/conf
        name: hive-config
      - mountPath: /opt/entrypoint
        name: entrypoint
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zcgvc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: my-hive-metastore-0
    initContainers:
    - args:
      - |
        until nc -z -v -w90 hive-metastore-postgresql 5432; do
          echo "Waiting for Hive Metastore DB to be ready..."
          sleep 5
        done
      command:
      - /bin/sh
      - -c
      image: busybox:latest
      imagePullPolicy: IfNotPresent
      name: init-wait-db
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zcgvc
        readOnly: true
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    subdomain: my-hive-metastore
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: my-hive-metastore
      name: hive-config
    - configMap:
        defaultMode: 420
        name: my-hive-metastore-entrypoint
      name: entrypoint
    - name: kube-api-access-zcgvc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:29:01Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:54Z"
      message: 'containers with incomplete status: [init-wait-db]'
      reason: ContainersNotInitialized
      status: "False"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:54Z"
      message: 'containers with unready status: [metastore]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:54Z"
      message: 'containers with unready status: [metastore]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: rtdl/hive-metastore:3.1.2
      imageID: ""
      lastState: {}
      name: metastore
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    initContainerStatuses:
    - containerID: containerd://6d840635eb22e17777b43055a7b32fd402b9ee58e7f6821c5bf84c59b6fcbbc0
      image: docker.io/library/busybox:latest
      imageID: docker.io/library/busybox@sha256:9ae97d36d26566ff84e8893c64a6dc4fe8ca6d1144bf5b87b2b85a32def253c7
      lastState: {}
      name: init-wait-db
      ready: false
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-07-01T07:29:00Z"
    phase: Pending
    podIP: 10.244.0.96
    podIPs:
    - ip: 10.244.0.96
    qosClass: BestEffort
    startTime: "2024-07-01T07:28:54Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
      checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
      checksum/scripts: 0e449b4bbf9dd08eea4509bc20671b359bfe0fae25ee5e966be27cb3a1659993
      checksum/secret: 9b054bc7354510843a124643c05d7230876d508e6d4c33eed0f7073787f8b50f
    creationTimestamp: "2024-07-01T07:28:27Z"
    generateName: my-redis-master-
    labels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: my-redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.5
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: my-redis-master-677fb497bc
      helm.sh/chart: redis-19.5.0
      statefulset.kubernetes.io/pod-name: my-redis-master-0
    name: my-redis-master-0
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: my-redis-master
      uid: bed2c503-0e52-4108-bf7d-dc1cc90fd067
    resourceVersion: "7818612"
    uid: df8a3ce5-2b96-41b1-9f6c-3b8b94089c9a
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: master
                app.kubernetes.io/instance: my-redis
                app.kubernetes.io/name: redis
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - args:
      - -c
      - /opt/bitnami/scripts/start-scripts/start-master.sh
      command:
      - /bin/bash
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: REDIS_REPLICATION_MODE
        value: master
      - name: ALLOW_EMPTY_PASSWORD
        value: "no"
      - name: REDIS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: redis-password
            name: my-redis
      - name: REDIS_TLS_ENABLED
        value: "no"
      - name: REDIS_PORT
        value: "6379"
      image: docker.io/bitnami/redis:7.2.5-debian-12-r0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - /health/ping_liveness_local.sh 5
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 6
      name: redis
      ports:
      - containerPort: 6379
        name: redis
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - /health/ping_readiness_local.sh 1
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        limits:
          cpu: 150m
          ephemeral-storage: 1Gi
          memory: 192Mi
        requests:
          cpu: 100m
          ephemeral-storage: 50Mi
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/bitnami/scripts/start-scripts
        name: start-scripts
      - mountPath: /health
        name: health
      - mountPath: /data
        name: redis-data
      - mountPath: /opt/bitnami/redis/mounted-etc
        name: config
      - mountPath: /opt/bitnami/redis/etc/
        name: empty-dir
        subPath: app-conf-dir
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: my-redis-master-0
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: my-redis-master
    serviceAccountName: my-redis-master
    subdomain: my-redis-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: redis-data
      persistentVolumeClaim:
        claimName: redis-data-my-redis-master-0
    - configMap:
        defaultMode: 493
        name: my-redis-scripts
      name: start-scripts
    - configMap:
        defaultMode: 493
        name: my-redis-health
      name: health
    - configMap:
        defaultMode: 420
        name: my-redis-configuration
      name: config
    - emptyDir: {}
      name: empty-dir
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:27Z"
      message: '0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes
        are available: 2 No preemption victims found for incoming pod.'
      reason: Unschedulable
      status: "False"
      type: PodScheduled
    phase: Pending
    qosClass: Burstable
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
      checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
      checksum/scripts: 0e449b4bbf9dd08eea4509bc20671b359bfe0fae25ee5e966be27cb3a1659993
      checksum/secret: 9b054bc7354510843a124643c05d7230876d508e6d4c33eed0f7073787f8b50f
    creationTimestamp: "2024-07-01T07:28:26Z"
    generateName: my-redis-replicas-
    labels:
      app.kubernetes.io/component: replica
      app.kubernetes.io/instance: my-redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.5
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: my-redis-replicas-dd87c5698
      helm.sh/chart: redis-19.5.0
      statefulset.kubernetes.io/pod-name: my-redis-replicas-0
    name: my-redis-replicas-0
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: my-redis-replicas
      uid: 408cf3b1-2206-4f31-9632-334dcf42e34f
    resourceVersion: "7820133"
    uid: 55fb7e44-3f61-4dda-a0fe-71ff3af74c0c
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: replica
                app.kubernetes.io/instance: my-redis
                app.kubernetes.io/name: redis
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - args:
      - -c
      - /opt/bitnami/scripts/start-scripts/start-replica.sh
      command:
      - /bin/bash
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: REDIS_REPLICATION_MODE
        value: replica
      - name: REDIS_MASTER_HOST
        value: my-redis-master-0.my-redis-headless.trino.svc.cluster.local
      - name: REDIS_MASTER_PORT_NUMBER
        value: "6379"
      - name: ALLOW_EMPTY_PASSWORD
        value: "no"
      - name: REDIS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: redis-password
            name: my-redis
      - name: REDIS_MASTER_PASSWORD
        valueFrom:
          secretKeyRef:
            key: redis-password
            name: my-redis
      - name: REDIS_TLS_ENABLED
        value: "no"
      - name: REDIS_PORT
        value: "6379"
      image: docker.io/bitnami/redis:7.2.5-debian-12-r0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - /health/ping_liveness_local_and_master.sh 5
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 6
      name: redis
      ports:
      - containerPort: 6379
        name: redis
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - /health/ping_readiness_local_and_master.sh 1
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        limits:
          cpu: 150m
          ephemeral-storage: 1Gi
          memory: 192Mi
        requests:
          cpu: 100m
          ephemeral-storage: 50Mi
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      startupProbe:
        failureThreshold: 22
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: redis
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/bitnami/scripts/start-scripts
        name: start-scripts
      - mountPath: /health
        name: health
      - mountPath: /data
        name: redis-data
      - mountPath: /opt/bitnami/redis/mounted-etc
        name: config
      - mountPath: /opt/bitnami/redis/etc
        name: empty-dir
        subPath: app-conf-dir
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: my-redis-replicas-0
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: my-redis-replica
    serviceAccountName: my-redis-replica
    subdomain: my-redis-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: redis-data
      persistentVolumeClaim:
        claimName: redis-data-my-redis-replicas-0
    - configMap:
        defaultMode: 493
        name: my-redis-scripts
      name: start-scripts
    - configMap:
        defaultMode: 493
        name: my-redis-health
      name: health
    - configMap:
        defaultMode: 420
        name: my-redis-configuration
      name: config
    - emptyDir: {}
      name: empty-dir
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:43Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:26Z"
      message: 'containers with unready status: [redis]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:26Z"
      message: 'containers with unready status: [redis]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://13570a23d16c7189c8ba47fc667a81e2ac74e45de4aa45ed4f75f92aed4ffc43
      image: docker.io/bitnami/redis:7.2.5-debian-12-r0
      imageID: docker.io/bitnami/redis@sha256:5261cae9e4076b75d114e6bb032a0699c50b004ea06a680a5304c4c08d286adb
      lastState:
        terminated:
          containerID: containerd://13570a23d16c7189c8ba47fc667a81e2ac74e45de4aa45ed4f75f92aed4ffc43
          exitCode: 0
          finishedAt: "2024-07-01T07:38:56Z"
          reason: Completed
          startedAt: "2024-07-01T07:38:14Z"
      name: redis
      ready: false
      restartCount: 7
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=redis pod=my-redis-replicas-0_trino(55fb7e44-3f61-4dda-a0fe-71ff3af74c0c)
          reason: CrashLoopBackOff
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.244.0.31
    podIPs:
    - ip: 10.244.0.31
    qosClass: Burstable
    startTime: "2024-07-01T07:28:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
      checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
      checksum/scripts: 0e449b4bbf9dd08eea4509bc20671b359bfe0fae25ee5e966be27cb3a1659993
      checksum/secret: 9b054bc7354510843a124643c05d7230876d508e6d4c33eed0f7073787f8b50f
    creationTimestamp: "2024-06-28T09:49:44Z"
    generateName: my-redis-replicas-
    labels:
      app.kubernetes.io/component: replica
      app.kubernetes.io/instance: my-redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.5
      apps.kubernetes.io/pod-index: "1"
      controller-revision-hash: my-redis-replicas-dd87c5698
      helm.sh/chart: redis-19.5.0
      statefulset.kubernetes.io/pod-name: my-redis-replicas-1
    name: my-redis-replicas-1
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: my-redis-replicas
      uid: 408cf3b1-2206-4f31-9632-334dcf42e34f
    resourceVersion: "7819743"
    uid: bdfbc91c-57ab-47f8-bad3-427bbbfd3323
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: replica
                app.kubernetes.io/instance: my-redis
                app.kubernetes.io/name: redis
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - args:
      - -c
      - /opt/bitnami/scripts/start-scripts/start-replica.sh
      command:
      - /bin/bash
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: REDIS_REPLICATION_MODE
        value: replica
      - name: REDIS_MASTER_HOST
        value: my-redis-master-0.my-redis-headless.trino.svc.cluster.local
      - name: REDIS_MASTER_PORT_NUMBER
        value: "6379"
      - name: ALLOW_EMPTY_PASSWORD
        value: "no"
      - name: REDIS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: redis-password
            name: my-redis
      - name: REDIS_MASTER_PASSWORD
        valueFrom:
          secretKeyRef:
            key: redis-password
            name: my-redis
      - name: REDIS_TLS_ENABLED
        value: "no"
      - name: REDIS_PORT
        value: "6379"
      image: docker.io/bitnami/redis:7.2.5-debian-12-r0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - /health/ping_liveness_local_and_master.sh 5
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 6
      name: redis
      ports:
      - containerPort: 6379
        name: redis
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - /health/ping_readiness_local_and_master.sh 1
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        limits:
          cpu: 150m
          ephemeral-storage: 1Gi
          memory: 192Mi
        requests:
          cpu: 100m
          ephemeral-storage: 50Mi
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      startupProbe:
        failureThreshold: 22
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: redis
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/bitnami/scripts/start-scripts
        name: start-scripts
      - mountPath: /health
        name: health
      - mountPath: /data
        name: redis-data
      - mountPath: /opt/bitnami/redis/mounted-etc
        name: config
      - mountPath: /opt/bitnami/redis/etc
        name: empty-dir
        subPath: app-conf-dir
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: my-redis-replicas-1
    nodeName: pool-1em8noa5l-rwr1p
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: my-redis-replica
    serviceAccountName: my-redis-replica
    subdomain: my-redis-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: redis-data
      persistentVolumeClaim:
        claimName: redis-data-my-redis-replicas-1
    - configMap:
        defaultMode: 493
        name: my-redis-scripts
      name: start-scripts
    - configMap:
        defaultMode: 493
        name: my-redis-health
      name: health
    - configMap:
        defaultMode: 420
        name: my-redis-configuration
      name: config
    - emptyDir: {}
      name: empty-dir
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T09:49:52Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T09:49:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:46Z"
      message: 'containers with unready status: [redis]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:46Z"
      message: 'containers with unready status: [redis]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T09:49:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://55670573e83ef4bc5900a8efb314908173d5623729136ddd6fa0fa9e05d73f75
      image: docker.io/bitnami/redis:7.2.5-debian-12-r0
      imageID: docker.io/bitnami/redis@sha256:5261cae9e4076b75d114e6bb032a0699c50b004ea06a680a5304c4c08d286adb
      lastState:
        terminated:
          containerID: containerd://55670573e83ef4bc5900a8efb314908173d5623729136ddd6fa0fa9e05d73f75
          exitCode: 0
          finishedAt: "2024-07-01T07:37:21Z"
          reason: Completed
          startedAt: "2024-07-01T07:36:38Z"
      name: redis
      ready: false
      restartCount: 14
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=redis pod=my-redis-replicas-1_trino(bdfbc91c-57ab-47f8-bad3-427bbbfd3323)
          reason: CrashLoopBackOff
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    phase: Running
    podIP: 10.244.26.17
    podIPs:
    - ip: 10.244.26.17
    qosClass: Burstable
    startTime: "2024-06-28T09:49:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
      checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
      checksum/scripts: 0e449b4bbf9dd08eea4509bc20671b359bfe0fae25ee5e966be27cb3a1659993
      checksum/secret: 9b054bc7354510843a124643c05d7230876d508e6d4c33eed0f7073787f8b50f
    creationTimestamp: "2024-06-28T11:18:20Z"
    generateName: my-redis-replicas-
    labels:
      app.kubernetes.io/component: replica
      app.kubernetes.io/instance: my-redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.5
      apps.kubernetes.io/pod-index: "2"
      controller-revision-hash: my-redis-replicas-dd87c5698
      helm.sh/chart: redis-19.5.0
      statefulset.kubernetes.io/pod-name: my-redis-replicas-2
    name: my-redis-replicas-2
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: my-redis-replicas
      uid: 408cf3b1-2206-4f31-9632-334dcf42e34f
    resourceVersion: "7819811"
    uid: 47bd6b2f-0aba-4c4f-ba66-223984fcd7a7
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: replica
                app.kubernetes.io/instance: my-redis
                app.kubernetes.io/name: redis
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - args:
      - -c
      - /opt/bitnami/scripts/start-scripts/start-replica.sh
      command:
      - /bin/bash
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: REDIS_REPLICATION_MODE
        value: replica
      - name: REDIS_MASTER_HOST
        value: my-redis-master-0.my-redis-headless.trino.svc.cluster.local
      - name: REDIS_MASTER_PORT_NUMBER
        value: "6379"
      - name: ALLOW_EMPTY_PASSWORD
        value: "no"
      - name: REDIS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: redis-password
            name: my-redis
      - name: REDIS_MASTER_PASSWORD
        valueFrom:
          secretKeyRef:
            key: redis-password
            name: my-redis
      - name: REDIS_TLS_ENABLED
        value: "no"
      - name: REDIS_PORT
        value: "6379"
      image: docker.io/bitnami/redis:7.2.5-debian-12-r0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - /health/ping_liveness_local_and_master.sh 5
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 6
      name: redis
      ports:
      - containerPort: 6379
        name: redis
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - /health/ping_readiness_local_and_master.sh 1
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        limits:
          cpu: 150m
          ephemeral-storage: 1Gi
          memory: 192Mi
        requests:
          cpu: 100m
          ephemeral-storage: 50Mi
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      startupProbe:
        failureThreshold: 22
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: redis
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/bitnami/scripts/start-scripts
        name: start-scripts
      - mountPath: /health
        name: health
      - mountPath: /data
        name: redis-data
      - mountPath: /opt/bitnami/redis/mounted-etc
        name: config
      - mountPath: /opt/bitnami/redis/etc
        name: empty-dir
        subPath: app-conf-dir
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: my-redis-replicas-2
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: my-redis-replica
    serviceAccountName: my-redis-replica
    subdomain: my-redis-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: redis-data
      persistentVolumeClaim:
        claimName: redis-data-my-redis-replicas-2
    - configMap:
        defaultMode: 493
        name: my-redis-scripts
      name: start-scripts
    - configMap:
        defaultMode: 493
        name: my-redis-health
      name: health
    - configMap:
        defaultMode: 420
        name: my-redis-configuration
      name: config
    - emptyDir: {}
      name: empty-dir
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T11:18:40Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T11:18:20Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:49Z"
      message: 'containers with unready status: [redis]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:49Z"
      message: 'containers with unready status: [redis]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-28T11:18:20Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9d08bcd1a4818fcae8f90643b34c39a37d77dc16bb716c50418d58bdb1dbaa6f
      image: docker.io/bitnami/redis:7.2.5-debian-12-r0
      imageID: docker.io/bitnami/redis@sha256:5261cae9e4076b75d114e6bb032a0699c50b004ea06a680a5304c4c08d286adb
      lastState:
        terminated:
          containerID: containerd://9d08bcd1a4818fcae8f90643b34c39a37d77dc16bb716c50418d58bdb1dbaa6f
          exitCode: 0
          finishedAt: "2024-07-01T07:37:39Z"
          reason: Completed
          startedAt: "2024-07-01T07:36:54Z"
      name: redis
      ready: false
      restartCount: 7
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=redis pod=my-redis-replicas-2_trino(47bd6b2f-0aba-4c4f-ba66-223984fcd7a7)
          reason: CrashLoopBackOff
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.244.0.114
    podIPs:
    - ip: 10.244.0.114
    qosClass: Burstable
    startTime: "2024-06-28T11:18:20Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/catalog-config: 8d01709a0b96dbd56c7133d24548de777a150b769cb13ff95da7b9e77c4fff76
      checksum/coordinator-config: cfbddf70ff23b7be16459b6b2eb01610aab047b44f783a998c63b4df23045e5d
    creationTimestamp: "2024-07-01T07:26:02Z"
    generateName: my-trino-trino-coordinator-6dc547c5c-
    labels:
      app.kubernetes.io/component: coordinator
      app.kubernetes.io/instance: my-trino
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: trino
      app.kubernetes.io/version: "449"
      helm.sh/chart: trino-0.24.0
      pod-template-hash: 6dc547c5c
    name: my-trino-trino-coordinator-6dc547c5c-vz8t4
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: my-trino-trino-coordinator-6dc547c5c
      uid: f2df840b-c487-4390-b49e-90faa7fd1f06
    resourceVersion: "7815820"
    uid: e5065e8d-0e7e-4fe3-bfce-5a42a2e5a2b4
  spec:
    containers:
    - image: trinodb/trino:449
      imagePullPolicy: IfNotPresent
      lifecycle: {}
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /v1/info
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: trino-coordinator
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /usr/lib/trino/bin/health-check
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: "1"
          memory: 1Gi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/trino
        name: config-volume
      - mountPath: /etc/trino/catalog
        name: catalog-volume
      - mountPath: /etc/trino/schemas
        name: schemas-volume
      - mountPath: /etc/redis
        name: redis-table-schema-volumn
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s8ssf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: registry-credentials
    nodeName: pool-1em8noa5l-rwr1p
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: my-trino-trino-coordinator
      name: config-volume
    - configMap:
        defaultMode: 420
        name: my-trino-trino-catalog
      name: catalog-volume
    - configMap:
        defaultMode: 420
        name: my-trino-trino-schemas-volume-coordinator
      name: schemas-volume
    - name: redis-table-schema-volumn
      secret:
        defaultMode: 420
        secretName: redis-table-definition
    - name: kube-api-access-s8ssf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:26:03Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:26:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:26:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:26:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:26:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2ef2bb4642b83660a220d898052a9e618dd4561663aacf2000c0c167dbf22f6d
      image: docker.io/trinodb/trino:449
      imageID: docker.io/trinodb/trino@sha256:8c0b85610dc4f3d172aaa6c8a5c8a195fb482dd42d0154f6dc9220c4047e75ca
      lastState: {}
      name: trino-coordinator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-07-01T07:26:03Z"
    hostIP: 10.104.0.3
    hostIPs:
    - ip: 10.104.0.3
    phase: Running
    podIP: 10.244.26.52
    podIPs:
    - ip: 10.244.26.52
    qosClass: Burstable
    startTime: "2024-07-01T07:26:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/worker-config: ddb69b7237b4ad8c576018f14ee85dac71a80333e4729219c40eaaee8f99f09d
    creationTimestamp: "2024-07-01T07:28:22Z"
    generateName: my-trino-trino-worker-56d895b7fb-
    labels:
      app.kubernetes.io/component: worker
      app.kubernetes.io/instance: my-trino
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: trino
      app.kubernetes.io/version: "449"
      helm.sh/chart: trino-0.24.0
      pod-template-hash: 56d895b7fb
    name: my-trino-trino-worker-56d895b7fb-68g8n
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: my-trino-trino-worker-56d895b7fb
      uid: 85ab12d6-f606-49e9-8c29-42da1c20aa4a
    resourceVersion: "7818599"
    uid: 6460af25-e83a-45ec-a86c-55995ace3069
  spec:
    containers:
    - image: trinodb/trino:449
      imagePullPolicy: IfNotPresent
      lifecycle: {}
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /v1/info
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: trino-worker
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /usr/lib/trino/bin/health-check
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 3Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/trino
        name: config-volume
      - mountPath: /etc/trino/catalog
        name: catalog-volume
      - mountPath: /etc/trino/schemas
        name: schemas-volume
      - mountPath: /etc/redis
        name: redis-table-schema-volumn
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nvstm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: registry-credentials
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: my-trino-trino-worker
      name: config-volume
    - configMap:
        defaultMode: 420
        name: my-trino-trino-catalog
      name: catalog-volume
    - configMap:
        defaultMode: 420
        name: my-trino-trino-schemas-volume-worker
      name: schemas-volume
    - name: redis-table-schema-volumn
      secret:
        defaultMode: 420
        secretName: redis-table-definition
    - name: kube-api-access-nvstm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T07:28:22Z"
      message: '0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes
        are available: 2 No preemption victims found for incoming pod.'
      reason: Unschedulable
      status: "False"
      type: PodScheduled
    phase: Pending
    qosClass: Burstable
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/worker-config: ddb69b7237b4ad8c576018f14ee85dac71a80333e4729219c40eaaee8f99f09d
    creationTimestamp: "2024-07-01T02:27:28Z"
    generateName: my-trino-trino-worker-56d895b7fb-
    labels:
      app.kubernetes.io/component: worker
      app.kubernetes.io/instance: my-trino
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: trino
      app.kubernetes.io/version: "449"
      helm.sh/chart: trino-0.24.0
      pod-template-hash: 56d895b7fb
    name: my-trino-trino-worker-56d895b7fb-74cmg
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: my-trino-trino-worker-56d895b7fb
      uid: 85ab12d6-f606-49e9-8c29-42da1c20aa4a
    resourceVersion: "7735835"
    uid: cfff4d91-b51c-49d5-a2f0-4492914d47e1
  spec:
    containers:
    - image: trinodb/trino:449
      imagePullPolicy: IfNotPresent
      lifecycle: {}
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /v1/info
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: trino-worker
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /usr/lib/trino/bin/health-check
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 3Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/trino
        name: config-volume
      - mountPath: /etc/trino/catalog
        name: catalog-volume
      - mountPath: /etc/trino/schemas
        name: schemas-volume
      - mountPath: /etc/redis
        name: redis-table-schema-volumn
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2d54t
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: registry-credentials
    nodeName: pool-1em8noa5l-rw5at
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: my-trino-trino-worker
      name: config-volume
    - configMap:
        defaultMode: 420
        name: my-trino-trino-catalog
      name: catalog-volume
    - configMap:
        defaultMode: 420
        name: my-trino-trino-schemas-volume-worker
      name: schemas-volume
    - name: redis-table-schema-volumn
      secret:
        defaultMode: 420
        secretName: redis-table-definition
    - name: kube-api-access-2d54t
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T02:27:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T02:27:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T02:28:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T02:28:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-07-01T02:27:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2c2c61451eb7df6611dcd02182d390720805c30092dec9de3cb217eeadcf65d4
      image: docker.io/trinodb/trino:449
      imageID: docker.io/trinodb/trino@sha256:8c0b85610dc4f3d172aaa6c8a5c8a195fb482dd42d0154f6dc9220c4047e75ca
      lastState: {}
      name: trino-worker
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-07-01T02:27:30Z"
    hostIP: 10.104.0.7
    hostIPs:
    - ip: 10.104.0.7
    phase: Running
    podIP: 10.244.0.51
    podIPs:
    - ip: 10.244.0.51
    qosClass: Burstable
    startTime: "2024-07-01T02:27:29Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-06-15T23:03:40Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: airflow
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-13.2.24
    name: airflow-postgresql
    namespace: airflow
    resourceVersion: "2245224"
    uid: 91b5a554-3eba-4fe1-bb0d-c41e1eb81dc7
  spec:
    clusterIP: 10.245.20.19
    clusterIPs:
    - 10.245.20.19
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: airflow
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    creationTimestamp: "2024-06-15T23:03:40Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: airflow
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-13.2.24
    name: airflow-postgresql-hl
    namespace: airflow
    resourceVersion: "2245230"
    uid: 15bce803-b372-4883-ade9-eda095acbbbe
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: airflow
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
      prometheus.io/port: "9102"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-06-15T23:03:40Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chart: airflow-1.13.1
      component: statsd
      heritage: Helm
      release: airflow
      tier: airflow
    name: airflow-statsd
    namespace: airflow
    resourceVersion: "2245222"
    uid: 4c4595a2-2ae4-4cd6-8a4f-f0055b305ac5
  spec:
    clusterIP: 10.245.19.36
    clusterIPs:
    - 10.245.19.36
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: statsd-ingest
      port: 9125
      protocol: UDP
      targetPort: 9125
    - name: statsd-scrape
      port: 9102
      protocol: TCP
      targetPort: 9102
    selector:
      component: statsd
      release: airflow
      tier: airflow
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-06-15T23:03:40Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chart: airflow-1.13.1
      component: triggerer
      heritage: Helm
      release: airflow
      tier: airflow
    name: airflow-triggerer
    namespace: airflow
    resourceVersion: "2245229"
    uid: 69e2bc79-e898-4eec-a722-763e5aa317d3
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: triggerer-logs
      port: 8794
      protocol: TCP
      targetPort: 8794
    selector:
      component: triggerer
      release: airflow
      tier: airflow
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-06-15T23:03:40Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chart: airflow-1.13.1
      component: webserver
      heritage: Helm
      release: airflow
      tier: airflow
    name: airflow-webserver
    namespace: airflow
    resourceVersion: "2245237"
    uid: 2aa1c53e-ac51-438a-b160-b1047a902e43
  spec:
    clusterIP: 10.245.218.183
    clusterIPs:
    - 10.245.218.183
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: airflow-ui
      nodePort: 30082
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      component: webserver
      release: airflow
      tier: airflow
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-09T16:02:30Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "191"
    uid: d054fe0e-3f97-4230-b82b-8d180a2ee4ca
  spec:
    clusterIP: 10.245.0.1
    clusterIPs:
    - 10.245.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      external-dns.alpha.kubernetes.io/hostname: minhtuyenvp02.id.vn.
      kubernetes.digitalocean.com/load-balancer-id: e0397a79-cd18-4098-be5c-6e947dfecf85
      meta.helm.sh/release-name: ingress
      meta.helm.sh/release-namespace: ingress
    creationTimestamp: "2024-06-14T20:03:16Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.10.1
      helm.sh/chart: ingress-nginx-4.10.1
    name: ingress-ingress-nginx-controller
    namespace: ingress
    resourceVersion: "1843717"
    uid: 9adcfef4-46aa-4792-a91b-287aa92177be
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.245.16.200
    clusterIPs:
    - 10.245.16.200
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: http
      name: http
      nodePort: 32557
      port: 80
      protocol: TCP
      targetPort: http
    - appProtocol: https
      name: https
      nodePort: 31653
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 139.59.223.165
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: ingress
      meta.helm.sh/release-namespace: ingress
    creationTimestamp: "2024-06-14T20:03:16Z"
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.10.1
      helm.sh/chart: ingress-nginx-4.10.1
    name: ingress-ingress-nginx-controller-admission
    namespace: ingress
    resourceVersion: "1842639"
    uid: 830104fc-2fec-4a9f-bc5a-663c8573cef1
  spec:
    clusterIP: 10.245.227.10
    clusterIPs:
    - 10.245.227.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: https
      name: https-webhook
      port: 443
      protocol: TCP
      targetPort: webhook
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kafka
      meta.helm.sh/release-namespace: kafka
    creationTimestamp: "2024-06-13T13:51:30Z"
    labels:
      app.kubernetes.io/component: kafka
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/version: 3.7.0
      helm.sh/chart: kafka-29.3.2
    name: kafka
    namespace: kafka
    resourceVersion: "1864136"
    uid: 14480dd3-cdf7-4a63-98a8-a3461822f5d0
  spec:
    clusterIP: 10.245.230.116
    clusterIPs:
    - 10.245.230.116
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-client
      nodePort: 32599
      port: 9092
      protocol: TCP
      targetPort: client
    selector:
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kafka
      meta.helm.sh/release-namespace: kafka
    creationTimestamp: "2024-06-13T13:51:30Z"
    labels:
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
      app.kubernetes.io/version: 3.7.0
      helm.sh/chart: kafka-29.3.2
    name: kafka-controller-headless
    namespace: kafka
    resourceVersion: "1864129"
    uid: 4b206da3-428b-438b-9d77-7436db4c3b18
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-interbroker
      port: 9094
      protocol: TCP
      targetPort: interbroker
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: client
    - name: tcp-controller
      port: 9093
      protocol: TCP
      targetPort: controller
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kafka
      meta.helm.sh/release-namespace: kafka
      prometheus.io/path: /
      prometheus.io/port: "5556"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-06-13T13:51:30Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/version: 3.7.0
      helm.sh/chart: kafka-29.3.2
    name: kafka-jmx-metrics
    namespace: kafka
    resourceVersion: "1864133"
    uid: d5393b7c-a6bc-4d9b-8701-115c73e29203
  spec:
    clusterIP: 10.245.248.29
    clusterIPs:
    - 10.245.248.29
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 5556
      protocol: TCP
      targetPort: metrics
    selector:
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kafka-metrics
      meta.helm.sh/release-namespace: kafka
    creationTimestamp: "2024-06-16T09:10:45Z"
    labels:
      app: prometheus-kafka-exporter
      app.kubernetes.io/managed-by: Helm
      chart: prometheus-kafka-exporter-2.10.0
      heritage: Helm
      release: kafka-metrics
    name: kafka-metrics-prometheus-kafka-exporter
    namespace: kafka
    resourceVersion: "2398502"
    uid: 1502371b-a12d-471c-b4f6-ce7a5b2e4048
  spec:
    clusterIP: 10.245.58.3
    clusterIPs:
    - 10.245.58.3
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: exporter-port
      port: 9308
      protocol: TCP
      targetPort: exporter-port
    selector:
      app: prometheus-kafka-exporter
      release: kafka-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-09T16:11:02Z"
    labels:
      app.kubernetes.io/name: hubble-peer
      app.kubernetes.io/part-of: cilium
      c3.doks.digitalocean.com/component: cilium
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: cilium
    name: hubble-peer
    namespace: kube-system
    resourceVersion: "891"
    uid: c34672e3-6ae2-4fe9-b37d-f264ed51c281
  spec:
    clusterIP: 10.245.95.146
    clusterIPs:
    - 10.245.95.146
    internalTrafficPolicy: Local
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: peer-service
      port: 443
      protocol: TCP
      targetPort: 4244
    selector:
      k8s-app: cilium
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-09T16:11:02Z"
    labels:
      app.kubernetes.io/name: hubble-relay
      app.kubernetes.io/part-of: cilium
      c3.doks.digitalocean.com/component: cilium
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-relay
    name: hubble-relay
    namespace: kube-system
    resourceVersion: "895"
    uid: 57d85075-836b-4c43-9c10-ccc12be0346d
  spec:
    clusterIP: 10.245.21.173
    clusterIPs:
    - 10.245.21.173
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 80
      protocol: TCP
      targetPort: 4245
    selector:
      k8s-app: hubble-relay
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-09T16:11:02Z"
    labels:
      app.kubernetes.io/name: hubble-ui
      app.kubernetes.io/part-of: cilium
      c3.doks.digitalocean.com/component: cilium
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-ui
    name: hubble-ui
    namespace: kube-system
    resourceVersion: "899"
    uid: eb387765-79e5-4aac-ac4c-5ce995a6ff8a
  spec:
    clusterIP: 10.245.12.95
    clusterIPs:
    - 10.245.12.95
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8081
    selector:
      k8s-app: hubble-ui
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-06-09T16:25:34Z"
    labels:
      c3.doks.digitalocean.com/component: coredns
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "5326"
    uid: 71821563-beb0-4bbc-b43f-3180dba8819d
  spec:
    clusterIP: 10.245.0.10
    clusterIPs:
    - 10.245.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-11T05:40:10Z"
    labels:
      app: kube-prometheus-stack-coredns
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 60.1.0
      chart: kube-prometheus-stack-60.1.0
      heritage: Helm
      jobLabel: coredns
      release: prometheus
    name: prometheus-kube-prometheus-coredns
    namespace: kube-system
    resourceVersion: "1881976"
    uid: 42893163-5567-4d10-854c-d25161f9bb86
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-11T05:40:10Z"
    labels:
      app: kube-prometheus-stack-kube-controller-manager
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 60.1.0
      chart: kube-prometheus-stack-60.1.0
      heritage: Helm
      jobLabel: kube-controller-manager
      release: prometheus
    name: prometheus-kube-prometheus-kube-controller-manager
    namespace: kube-system
    resourceVersion: "1881979"
    uid: dcd1dd00-f700-4635-aeb0-75d9f1d0bae6
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10257
      protocol: TCP
      targetPort: 10257
    selector:
      component: kube-controller-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-11T05:40:10Z"
    labels:
      app: kube-prometheus-stack-kube-etcd
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 60.1.0
      chart: kube-prometheus-stack-60.1.0
      heritage: Helm
      jobLabel: kube-etcd
      release: prometheus
    name: prometheus-kube-prometheus-kube-etcd
    namespace: kube-system
    resourceVersion: "1881982"
    uid: 46e5f8df-ea67-4660-8b21-37c84e5883cf
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 2381
      protocol: TCP
      targetPort: 2381
    selector:
      component: etcd
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-11T05:40:10Z"
    labels:
      app: kube-prometheus-stack-kube-proxy
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 60.1.0
      chart: kube-prometheus-stack-60.1.0
      heritage: Helm
      jobLabel: kube-proxy
      release: prometheus
    name: prometheus-kube-prometheus-kube-proxy
    namespace: kube-system
    resourceVersion: "1881985"
    uid: c2672539-d70a-4a3f-8fe5-655b5dc9e8e3
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10249
      protocol: TCP
      targetPort: 10249
    selector:
      k8s-app: kube-proxy
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-11T05:40:10Z"
    labels:
      app: kube-prometheus-stack-kube-scheduler
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 60.1.0
      chart: kube-prometheus-stack-60.1.0
      heritage: Helm
      jobLabel: kube-scheduler
      release: prometheus
    name: prometheus-kube-prometheus-kube-scheduler
    namespace: kube-system
    resourceVersion: "1881990"
    uid: fff6c1d9-9b63-465e-a6ab-1c83b306d6a9
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10259
      protocol: TCP
      targetPort: 10259
    selector:
      component: kube-scheduler
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-11T05:40:16Z"
    labels:
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: kubelet
      k8s-app: kubelet
    name: prometheus-kube-prometheus-kubelet
    namespace: kube-system
    resourceVersion: "549083"
    uid: 4e922808-530d-46a6-8b8b-9db8bafc2597
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    - IPv6
    ipFamilyPolicy: RequireDualStack
    ports:
    - name: https-metrics
      port: 10250
      protocol: TCP
      targetPort: 10250
    - name: http-metrics
      port: 10255
      protocol: TCP
      targetPort: 10255
    - name: cadvisor
      port: 4194
      protocol: TCP
      targetPort: 4194
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: minio
      meta.helm.sh/release-namespace: minio
    creationTimestamp: "2024-06-14T12:43:48Z"
    labels:
      app.kubernetes.io/instance: minio
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: minio
      app.kubernetes.io/version: 2024.6.13
      helm.sh/chart: minio-14.6.11
    name: minio
    namespace: minio
    resourceVersion: "3715433"
    uid: ac354670-d198-40dc-addf-f669878de288
  spec:
    clusterIP: 10.245.24.65
    clusterIPs:
    - 10.245.24.65
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: minio-api
      nodePort: 30090
      port: 9000
      protocol: TCP
      targetPort: minio-api
    - name: minio-console
      nodePort: 30091
      port: 9001
      protocol: TCP
      targetPort: minio-console
    selector:
      app.kubernetes.io/instance: minio
      app.kubernetes.io/name: minio
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: minio
      meta.helm.sh/release-namespace: minio
    creationTimestamp: "2024-06-14T12:43:48Z"
    labels:
      app.kubernetes.io/instance: minio
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: minio
      app.kubernetes.io/version: 2024.6.13
      helm.sh/chart: minio-14.6.11
    name: minio-headless
    namespace: minio
    resourceVersion: "3715426"
    uid: 52c2e916-a791-400f-ac86-1e6703c4b6e4
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: minio-api
      port: 9000
      protocol: TCP
      targetPort: minio-api
    - name: minio-console
      port: 9001
      protocol: TCP
      targetPort: minio-console
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/instance: minio
      app.kubernetes.io/name: minio
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-16T08:51:51Z"
    labels:
      managed-by: prometheus-operator
      operated-alertmanager: "true"
    name: alertmanager-operated
    namespace: prometheus
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Alertmanager
      name: prometheus-kube-prometheus-alertmanager
      uid: 2739cf3b-2557-45be-bcbd-4bae6a842853
    resourceVersion: "2393495"
    uid: cbbdb6e8-55cb-4de3-b5d5-77352ac0fcce
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: http-web
    - name: tcp-mesh
      port: 9094
      protocol: TCP
      targetPort: 9094
    - name: udp-mesh
      port: 9094
      protocol: UDP
      targetPort: 9094
    selector:
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-16T08:51:38Z"
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 11.0.0
      helm.sh/chart: grafana-8.0.1
    name: prometheus-grafana
    namespace: prometheus
    resourceVersion: "2393275"
    uid: f1d419ba-5203-4ca9-9bae-4543cab98e38
  spec:
    clusterIP: 10.245.237.152
    clusterIPs:
    - 10.245.237.152
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: grafana
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-16T08:51:39Z"
    labels:
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 60.1.0
      chart: kube-prometheus-stack-60.1.0
      heritage: Helm
      release: prometheus
      self-monitor: "true"
    name: prometheus-kube-prometheus-alertmanager
    namespace: prometheus
    resourceVersion: "2393295"
    uid: 2d142202-00f0-4775-bd2e-b3a34290265b
  spec:
    clusterIP: 10.245.16.130
    clusterIPs:
    - 10.245.16.130
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: 9093
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      alertmanager: prometheus-kube-prometheus-alertmanager
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-16T08:51:42Z"
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 60.1.0
      chart: kube-prometheus-stack-60.1.0
      heritage: Helm
      release: prometheus
    name: prometheus-kube-prometheus-operator
    namespace: prometheus
    resourceVersion: "2393307"
    uid: d73ee81f-c077-4a5c-9b8a-548ea2e9262b
  spec:
    clusterIP: 10.245.188.6
    clusterIPs:
    - 10.245.188.6
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app: kube-prometheus-stack-operator
      release: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-16T08:51:43Z"
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 60.1.0
      chart: kube-prometheus-stack-60.1.0
      heritage: Helm
      release: prometheus
      self-monitor: "true"
    name: prometheus-kube-prometheus-prometheus
    namespace: prometheus
    resourceVersion: "2401411"
    uid: dd26a44e-5d8d-4ac5-8e4a-eebc42d96a8c
  spec:
    clusterIP: 10.245.241.201
    clusterIPs:
    - 10.245.241.201
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: 9090
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      app.kubernetes.io/name: prometheus
      operator.prometheus.io/name: prometheus-kube-prometheus-prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-06-16T08:51:38Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.12.0
      helm.sh/chart: kube-state-metrics-5.20.0
      release: prometheus
    name: prometheus-kube-state-metrics
    namespace: prometheus
    resourceVersion: "2393281"
    uid: 7391e9a2-c2b5-43f1-9c15-df0998d02d2d
  spec:
    clusterIP: 10.245.44.79
    clusterIPs:
    - 10.245.44.79
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: kube-state-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-16T08:51:51Z"
    labels:
      managed-by: prometheus-operator
      operated-prometheus: "true"
    name: prometheus-operated
    namespace: prometheus
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Prometheus
      name: prometheus-kube-prometheus-prometheus
      uid: f4cb9b67-49dd-41ec-91fa-6be5c504f4ec
    resourceVersion: "2393500"
    uid: 06fbd5dd-f693-4fcd-b35d-c2e7156d9a12
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: http-web
    selector:
      app.kubernetes.io/name: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-06-16T08:51:39Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.1
      helm.sh/chart: prometheus-node-exporter-4.36.0
      jobLabel: node-exporter
      release: prometheus
    name: prometheus-prometheus-node-exporter
    namespace: prometheus
    resourceVersion: "2393288"
    uid: 6ec34665-ee92-400d-8dea-7911df2bff3d
  spec:
    clusterIP: 10.245.226.137
    clusterIPs:
    - 10.245.226.137
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus-node-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T07:14:02Z"
    labels:
      sparkoperator.k8s.io/app-name: gold-create-scd0-6nzu0zlp
    name: gold-create-scd0-6nzu0zlp-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: gold-create-scd0-6nzu0zlp
      uid: 108edd8f-3092-4021-93c1-b234420b1bf7
    resourceVersion: "6672771"
    uid: 6495f583-ed7e-4932-8028-9f60f36d1f11
  spec:
    clusterIP: 10.245.107.54
    clusterIPs:
    - 10.245.107.54
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: gold-create-scd0-6nzu0zlp
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T07:02:26Z"
    labels:
      sparkoperator.k8s.io/app-name: gold-create-scd0-70bnxsd4
    name: gold-create-scd0-70bnxsd4-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: gold-create-scd0-70bnxsd4
      uid: b8d9c994-a3b0-4b67-aa26-3519a504acc7
    resourceVersion: "6669775"
    uid: 4790bcbf-506e-4990-a3ef-adf9e7f789ce
  spec:
    clusterIP: 10.245.193.131
    clusterIPs:
    - 10.245.193.131
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: gold-create-scd0-70bnxsd4
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-16T14:11:54Z"
    labels:
      sparkoperator.k8s.io/app-name: gold-create-scd0-81opx1cu
    name: gold-create-scd0-81opx1cu-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: gold-create-scd0-81opx1cu
      uid: 2179b1b2-e379-4314-bc03-4ecb52c01314
    resourceVersion: "2474168"
    uid: 1f4a2b9a-3012-4482-b8d0-1c075d258eaa
  spec:
    clusterIP: 10.245.210.210
    clusterIPs:
    - 10.245.210.210
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: gold-create-scd0-81opx1cu
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T07:25:39Z"
    labels:
      sparkoperator.k8s.io/app-name: gold-create-scd0-dslr57tw
    name: gold-create-scd0-dslr57tw-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: gold-create-scd0-dslr57tw
      uid: d6a3ff07-6b97-4bb9-a137-ed683563260a
    resourceVersion: "6675766"
    uid: 4153a96b-030d-4ac1-86a2-15f815cea7ec
  spec:
    clusterIP: 10.245.123.1
    clusterIPs:
    - 10.245.123.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: gold-create-scd0-dslr57tw
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T08:35:23Z"
    labels:
      sparkoperator.k8s.io/app-name: silver-transform-silver-fhvhv-transform-2xo41mt3
    name: silver-transform-silver-fhvhv-transform-2xo41mt3-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: silver-transform-silver-fhvhv-transform-2xo41mt3
      uid: 9b9f02b7-37a4-41f5-a75e-7ac759ba7015
    resourceVersion: "6693572"
    uid: 96a3805b-e247-4d19-b19b-1ed884f4a61d
  spec:
    clusterIP: 10.245.162.133
    clusterIPs:
    - 10.245.162.133
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: silver-transform-silver-fhvhv-transform-2xo41mt3
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T08:23:46Z"
    labels:
      sparkoperator.k8s.io/app-name: silver-transform-silver-fhvhv-transform-erxbl555
    name: silver-transform-silver-fhvhv-transform-erxbl555-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: silver-transform-silver-fhvhv-transform-erxbl555
      uid: 154ffadc-fb83-4619-8af6-5d9ad3f67531
    resourceVersion: "6690581"
    uid: eada9c34-e7cb-4cbb-a6c3-86d476fb0f62
  spec:
    clusterIP: 10.245.247.45
    clusterIPs:
    - 10.245.247.45
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: silver-transform-silver-fhvhv-transform-erxbl555
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T08:00:33Z"
    labels:
      sparkoperator.k8s.io/app-name: silver-transform-silver-fhvhv-transform-r50jap2r
    name: silver-transform-silver-fhvhv-transform-r50jap2r-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: silver-transform-silver-fhvhv-transform-r50jap2r
      uid: 09340096-1bd7-4073-9ff1-9225ce12ce19
    resourceVersion: "6684620"
    uid: c252b6ff-3213-4c1e-ac20-f5375540badb
  spec:
    clusterIP: 10.245.140.57
    clusterIPs:
    - 10.245.140.57
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: silver-transform-silver-fhvhv-transform-r50jap2r
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T08:12:10Z"
    labels:
      sparkoperator.k8s.io/app-name: silver-transform-silver-fhvhv-transform-vgd656jy
    name: silver-transform-silver-fhvhv-transform-vgd656jy-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: silver-transform-silver-fhvhv-transform-vgd656jy
      uid: b190097b-9b82-4f1b-a0f3-ca47975ec85f
    resourceVersion: "6687594"
    uid: 51123fdc-121e-4375-9548-4a6a61abb2c2
  spec:
    clusterIP: 10.245.8.217
    clusterIPs:
    - 10.245.8.217
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: silver-transform-silver-fhvhv-transform-vgd656jy
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/name":"spark-operator"},"name":"spark-operator-metrics","namespace":"spark"},"spec":{"ports":[{"name":"metrics","port":10254,"targetPort":10254}],"selector":{"app.kubernetes.io/name":"spark-operator"}}}
    creationTimestamp: "2024-06-16T11:09:26Z"
    labels:
      app.kubernetes.io/name: spark-operator
    name: spark-operator-metrics
    namespace: spark
    resourceVersion: "2426759"
    uid: 0762a60a-f2e8-4166-a42a-e9aa71d5e521
  spec:
    clusterIP: 10.245.40.210
    clusterIPs:
    - 10.245.40.210
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics
      port: 10254
      protocol: TCP
      targetPort: 10254
    selector:
      app.kubernetes.io/name: spark-operator
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: spark
      meta.helm.sh/release-namespace: spark
    creationTimestamp: "2024-06-14T12:47:56Z"
    labels:
      app.kubernetes.io/instance: spark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: spark-operator
      app.kubernetes.io/version: v1beta2-1.6.0-3.5.0
      helm.sh/chart: spark-operator-1.4.0
    name: spark-spark-operator-webhook-svc
    namespace: spark
    resourceVersion: "1724880"
    uid: 0557b6c2-5d3b-40dd-b1f4-6cf18a470cb2
  spec:
    clusterIP: 10.245.18.50
    clusterIPs:
    - 10.245.18.50
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: webhook
      port: 443
      protocol: TCP
      targetPort: webhook
    selector:
      app.kubernetes.io/instance: spark
      app.kubernetes.io/name: spark-operator
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T05:45:48Z"
    labels:
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-1d83n3tn
    name: trip-consuming-stream-fhvhv-to-bronze-1d83n3tn-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: trip-consuming-stream-fhvhv-to-bronze-1d83n3tn
      uid: 64141fd0-d8ea-4865-8913-2c38aba50d19
    resourceVersion: "6650377"
    uid: c59be2be-1cc8-4c80-8b87-ae52b9827717
  spec:
    clusterIP: 10.245.204.41
    clusterIPs:
    - 10.245.204.41
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-1d83n3tn
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T07:17:16Z"
    labels:
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-5p3zlgss
    name: trip-consuming-stream-fhvhv-to-bronze-5p3zlgss-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: trip-consuming-stream-fhvhv-to-bronze-5p3zlgss
      uid: 56002a91-1fcf-458d-b513-f3cb265edc61
    resourceVersion: "6673622"
    uid: 86b6a6d4-f562-4a16-bbf3-5dc8bcedc4a7
  spec:
    clusterIP: 10.245.119.205
    clusterIPs:
    - 10.245.119.205
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-5p3zlgss
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T05:56:29Z"
    labels:
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-93hmpaa5
    name: trip-consuming-stream-fhvhv-to-bronze-93hmpaa5-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: trip-consuming-stream-fhvhv-to-bronze-93hmpaa5
      uid: cffa3546-b14e-46dd-bc82-e087e944122c
    resourceVersion: "6653065"
    uid: dd0ca3b1-a302-4279-942d-776f17917b2f
  spec:
    clusterIP: 10.245.211.122
    clusterIPs:
    - 10.245.211.122
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-93hmpaa5
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T06:45:17Z"
    labels:
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-fks880nz
    name: trip-consuming-stream-fhvhv-to-bronze-fks880nz-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: trip-consuming-stream-fhvhv-to-bronze-fks880nz
      uid: d7f5c1cb-953a-47fd-9319-aa6f37225d82
    resourceVersion: "6665393"
    uid: 9d27e7b0-6468-448b-b447-a7c600e417cc
  spec:
    clusterIP: 10.245.23.245
    clusterIPs:
    - 10.245.23.245
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-fks880nz
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T07:06:36Z"
    labels:
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-oczk1i23
    name: trip-consuming-stream-fhvhv-to-bronze-oczk1i23-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: trip-consuming-stream-fhvhv-to-bronze-oczk1i23
      uid: e54b1628-8ab0-45c5-94e8-9e317ef47a38
    resourceVersion: "6670862"
    uid: 2f825320-56fb-4de6-b239-2271c0371bf2
  spec:
    clusterIP: 10.245.149.18
    clusterIPs:
    - 10.245.149.18
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-oczk1i23
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T05:35:08Z"
    labels:
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-sdgzeiub
    name: trip-consuming-stream-fhvhv-to-bronze-sdgzeiub-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: trip-consuming-stream-fhvhv-to-bronze-sdgzeiub
      uid: 8e5f1b5e-633d-4d13-87ca-6ad6e97f0f28
    resourceVersion: "6647685"
    uid: 1801091a-4ffc-4a14-bfc8-827461aa66b0
  spec:
    clusterIP: 10.245.211.49
    clusterIPs:
    - 10.245.211.49
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-sdgzeiub
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T09:03:34Z"
    labels:
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-t2ymwfzw
    name: trip-consuming-stream-fhvhv-to-bronze-t2ymwfzw-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: trip-consuming-stream-fhvhv-to-bronze-t2ymwfzw
      uid: bd6f5276-6ce3-44be-9b56-eee125473b4d
    resourceVersion: "6700763"
    uid: af09da47-d414-4d6e-9616-2ae92c440b08
  spec:
    clusterIP: 10.245.119.232
    clusterIPs:
    - 10.245.119.232
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-t2ymwfzw
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T06:07:08Z"
    labels:
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-uti2jge5
    name: trip-consuming-stream-fhvhv-to-bronze-uti2jge5-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: trip-consuming-stream-fhvhv-to-bronze-uti2jge5
      uid: f70bf3a4-d124-4107-9344-b4e9d6a4cce1
    resourceVersion: "6655760"
    uid: 1b1f95c3-d54d-4afa-ba9e-f3147f16cd7a
  spec:
    clusterIP: 10.245.84.4
    clusterIPs:
    - 10.245.84.4
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-uti2jge5
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-06-28T06:55:59Z"
    labels:
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-vxucpli4
    name: trip-consuming-stream-fhvhv-to-bronze-vxucpli4-ui-svc
    namespace: spark
    ownerReferences:
    - apiVersion: sparkoperator.k8s.io/v1beta2
      controller: true
      kind: SparkApplication
      name: trip-consuming-stream-fhvhv-to-bronze-vxucpli4
      uid: f1fca2c5-691a-4a86-a502-2db0e4e51c44
    resourceVersion: "6668103"
    uid: 96d2b406-384e-40bd-8a42-595c2105aac7
  spec:
    clusterIP: 10.245.93.41
    clusterIPs:
    - 10.245.93.41
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: spark-driver-ui-port
      port: 4040
      protocol: TCP
      targetPort: 4040
    selector:
      spark-role: driver
      sparkoperator.k8s.io/app-name: trip-consuming-stream-fhvhv-to-bronze-vxucpli4
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-09T21:32:38Z"
    labels:
      app: superset
      app.kubernetes.io/managed-by: Helm
      chart: superset-0.12.11
      heritage: Helm
      release: superset
    name: superset
    namespace: superset
    resourceVersion: "2386542"
    uid: 1ffceace-1151-4c33-b9a5-efcf85f0e3db
  spec:
    clusterIP: 10.245.5.206
    clusterIPs:
    - 10.245.5.206
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 30088
      port: 8088
      protocol: TCP
      targetPort: http
    selector:
      app: superset
      release: superset
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-09T21:32:38Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: superset
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      helm.sh/chart: postgresql-12.1.6
    name: superset-postgresql
    namespace: superset
    resourceVersion: "85947"
    uid: 973b969e-91a6-4e0c-b4b7-793058871660
  spec:
    clusterIP: 10.245.139.120
    clusterIPs:
    - 10.245.139.120
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: superset
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-09T21:32:38Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: superset
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      helm.sh/chart: postgresql-12.1.6
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    name: superset-postgresql-hl
    namespace: superset
    resourceVersion: "85944"
    uid: 371a3048-941e-4303-bc53-ec3268d068c1
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: superset
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-09T21:32:38Z"
    labels:
      app.kubernetes.io/instance: superset
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      helm.sh/chart: redis-17.9.4
    name: superset-redis-headless
    namespace: superset
    resourceVersion: "85946"
    uid: a0687966-0907-4590-a044-ce303efcefa5
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-redis
      port: 6379
      protocol: TCP
      targetPort: redis
    selector:
      app.kubernetes.io/instance: superset
      app.kubernetes.io/name: redis
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-09T21:32:38Z"
    labels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: superset
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      helm.sh/chart: redis-17.9.4
    name: superset-redis-master
    namespace: superset
    resourceVersion: "85949"
    uid: 9c074a83-621a-420d-b72d-db4723e275da
  spec:
    clusterIP: 10.245.58.78
    clusterIPs:
    - 10.245.58.78
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-redis
      port: 6379
      protocol: TCP
      targetPort: redis
    selector:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: superset
      app.kubernetes.io/name: redis
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hive-metastore-postgresql
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-09T16:26:01Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hive-metastore-postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.3.0
      helm.sh/chart: postgresql-15.5.1
    name: hive-metastore-postgresql
    namespace: trino
    resourceVersion: "5616"
    uid: 78150123-8274-4dbe-b729-921df0003582
  spec:
    clusterIP: 10.245.123.250
    clusterIPs:
    - 10.245.123.250
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hive-metastore-postgresql
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hive-metastore-postgresql
      meta.helm.sh/release-namespace: trino
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    creationTimestamp: "2024-06-09T16:26:01Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hive-metastore-postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.3.0
      helm.sh/chart: postgresql-15.5.1
    name: hive-metastore-postgresql-hl
    namespace: trino
    resourceVersion: "5612"
    uid: 74b3976b-c525-42bc-a3bd-401577a9d3e3
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hive-metastore-postgresql
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: my-hive-metastore
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-09T16:26:03Z"
    labels:
      app.kubernetes.io/component: metastore
      app.kubernetes.io/instance: my-hive-metastore
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: hive-metastore
      app.kubernetes.io/part-of: hive-metastore
      app.kubernetes.io/version: 3.1.2
      helm.sh/chart: hive-metastore-0.0.1
    name: my-hive-metastore
    namespace: trino
    resourceVersion: "5648"
    uid: 5445a5c3-fb9b-423f-a1f8-1ac88b396f6d
  spec:
    clusterIP: 10.245.62.172
    clusterIPs:
    - 10.245.62.172
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: thrift
      port: 9083
      protocol: TCP
      targetPort: 9083
    selector:
      app.kubernetes.io/component: metastore
      app.kubernetes.io/instance: my-hive-metastore
      app.kubernetes.io/name: hive-metastore
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: my-redis
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-09T16:26:09Z"
    labels:
      app.kubernetes.io/instance: my-redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.5
      helm.sh/chart: redis-19.5.0
    name: my-redis-headless
    namespace: trino
    resourceVersion: "5785"
    uid: ef359239-fd84-4a21-8eed-1a52d45985ce
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-redis
      port: 6379
      protocol: TCP
      targetPort: redis
    selector:
      app.kubernetes.io/instance: my-redis
      app.kubernetes.io/name: redis
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: my-redis
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-09T16:26:09Z"
    labels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: my-redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.5
      helm.sh/chart: redis-19.5.0
    name: my-redis-master
    namespace: trino
    resourceVersion: "5789"
    uid: 76df97b9-7868-47a7-8006-07f95481e465
  spec:
    clusterIP: 10.245.53.178
    clusterIPs:
    - 10.245.53.178
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-redis
      port: 6379
      protocol: TCP
      targetPort: redis
    selector:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: my-redis
      app.kubernetes.io/name: redis
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: my-redis
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-09T16:26:09Z"
    labels:
      app.kubernetes.io/component: replica
      app.kubernetes.io/instance: my-redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.5
      helm.sh/chart: redis-19.5.0
    name: my-redis-replicas
    namespace: trino
    resourceVersion: "5793"
    uid: 812f4e54-3265-4671-997d-2406983d61b4
  spec:
    clusterIP: 10.245.168.45
    clusterIPs:
    - 10.245.168.45
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-redis
      port: 6379
      protocol: TCP
      targetPort: redis
    selector:
      app.kubernetes.io/component: replica
      app.kubernetes.io/instance: my-redis
      app.kubernetes.io/name: redis
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: my-trino
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-09T16:26:37Z"
    labels:
      app.kubernetes.io/instance: my-trino
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: trino
      app.kubernetes.io/version: "449"
      helm.sh/chart: trino-0.24.0
    name: my-trino-trino
    namespace: trino
    resourceVersion: "7732714"
    uid: 6e7595d2-3fa3-4586-a0f1-563e1d38299e
  spec:
    clusterIP: 10.245.204.127
    clusterIPs:
    - 10.245.204.127
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/component: coordinator
      app.kubernetes.io/instance: my-trino
      app.kubernetes.io/name: trino
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2024-06-09T16:11:02Z"
    generation: 1
    labels:
      app.kubernetes.io/name: cilium-agent
      app.kubernetes.io/part-of: cilium
      c3.doks.digitalocean.com/component: cilium
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: cilium
      kubernetes.io/cluster-service: "true"
    name: cilium
    namespace: kube-system
    resourceVersion: "7820334"
    uid: 338cdf62-638a-4469-8517-3c4fe503a2e8
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: cilium
        kubernetes.io/cluster-service: "true"
    template:
      metadata:
        annotations:
          clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
          container.apparmor.security.beta.kubernetes.io/apply-sysctl-overwrites: unconfined
          container.apparmor.security.beta.kubernetes.io/cilium-agent: unconfined
          container.apparmor.security.beta.kubernetes.io/clean-cilium-state: unconfined
          container.apparmor.security.beta.kubernetes.io/mount-cgroup: unconfined
          kubectl.kubernetes.io/default-container: cilium-agent
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: cilium-agent
          app.kubernetes.io/part-of: cilium
          doks.digitalocean.com/managed: "true"
          k8s-app: cilium
          kubernetes.io/cluster-service: "true"
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: true
        containers:
        - args:
          - --config-dir=/tmp/cilium/config-map
          - --k8s-api-server=https://f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
          - --ipv4-native-routing-cidr=10.244.0.0/16
          command:
          - cilium-agent
          env:
          - name: K8S_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CILIUM_K8S_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CILIUM_CLUSTERMESH_CONFIG
            value: /var/lib/cilium/clustermesh/
          - name: KUBERNETES_SERVICE_HOST
            value: f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
          imagePullPolicy: IfNotPresent
          lifecycle:
            postStart:
              exec:
                command:
                - bash
                - -c
                - |
                  set -o errexit
                  set -o pipefail
                  set -o nounset

                  # When running in AWS ENI mode, it's likely that 'aws-node' has
                  # had a chance to install SNAT iptables rules. These can result
                  # in dropped traffic, so we should attempt to remove them.
                  # We do it using a 'postStart' hook since this may need to run
                  # for nodes which might have already been init'ed but may still
                  # have dangling rules. This is safe because there are no
                  # dependencies on anything that is part of the startup script
                  # itself, and can be safely run multiple times per node (e.g. in
                  # case of a restart).
                  if [[ "$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')" != "0" ]];
                  then
                      echo 'Deleting iptables rules created by the AWS CNI VPC plugin'
                      iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore
                  fi
                  echo 'Done!'
            preStop:
              exec:
                command:
                - /cni-uninstall.sh
          livenessProbe:
            failureThreshold: 10
            httpGet:
              host: 127.0.0.1
              httpHeaders:
              - name: brief
                value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            initialDelaySeconds: 120
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: cilium-agent
          ports:
          - containerPort: 4244
            hostPort: 4244
            name: peer-service
            protocol: TCP
          - containerPort: 9090
            hostPort: 9090
            name: prometheus
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              httpHeaders:
              - name: brief
                value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 300m
              memory: 300Mi
          securityContext:
            capabilities:
              add:
              - CHOWN
              - KILL
              - NET_ADMIN
              - NET_RAW
              - IPC_LOCK
              - SYS_MODULE
              - SYS_ADMIN
              - SYS_RESOURCE
              - DAC_OVERRIDE
              - FOWNER
              - SETGID
              - SETUID
              drop:
              - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          startupProbe:
            failureThreshold: 105
            httpGet:
              host: 127.0.0.1
              httpHeaders:
              - name: brief
                value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /host/proc/sys/net
            name: host-proc-sys-net
          - mountPath: /host/proc/sys/kernel
            name: host-proc-sys-kernel
          - mountPath: /sys/fs/bpf
            mountPropagation: HostToContainer
            name: bpf-maps
          - mountPath: /var/run/cilium
            name: cilium-run
          - mountPath: /host/etc/cni/net.d
            name: etc-cni-netd
          - mountPath: /var/lib/cilium/clustermesh
            name: clustermesh-secrets
            readOnly: true
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /var/lib/cilium/tls/hubble
            name: hubble-tls
            readOnly: true
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - command:
          - bash
          - -e
          - -c
          - |
            # This will get the node object for the local node and search through
            # the assigned addresses in the object in order to check whether CCM
            # already set the internal AND external IP since cilium needs both
            # for a clean startup.
            # The grep matches regardless of the order of IPs.
            until /host/usr/bin/kubectl get node ${HOSTNAME} -o jsonpath="{.status.addresses[*].type}" | grep -E "InternalIP.*ExternalIP|ExternalIP.*InternalIP"; do echo "waiting for CCM to store internal and external IP addresses in node object: ${HOSTNAME}" && sleep 3; done;
          env:
          - name: KUBERNETES_SERVICE_HOST
            value: f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
          imagePullPolicy: IfNotPresent
          name: delay-cilium-for-ccm
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/usr/bin/kubectl
            name: host-kubectl
        - command:
          - cilium
          - build-config
          - --source=config-map:cilium-config
          env:
          - name: K8S_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CILIUM_K8S_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: KUBERNETES_SERVICE_HOST
            value: f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
          imagePullPolicy: IfNotPresent
          name: config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        - command:
          - sh
          - -ec
          - |
            cp /usr/bin/cilium-mount /hostbin/cilium-mount;
            nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
            rm /hostbin/cilium-mount
          env:
          - name: CGROUP_ROOT
            value: /run/cilium/cgroupv2
          - name: BIN_PATH
            value: /opt/cni/bin
          image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
          imagePullPolicy: IfNotPresent
          name: mount-cgroup
          resources: {}
          securityContext:
            capabilities:
              add:
              - SYS_ADMIN
              - SYS_CHROOT
              - SYS_PTRACE
              drop:
              - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /hostproc
            name: hostproc
          - mountPath: /hostbin
            name: cni-path
        - command:
          - sh
          - -ec
          - |
            cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;
            nsenter --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-sysctlfix";
            rm /hostbin/cilium-sysctlfix
          env:
          - name: BIN_PATH
            value: /opt/cni/bin
          image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
          imagePullPolicy: IfNotPresent
          name: apply-sysctl-overwrites
          resources: {}
          securityContext:
            capabilities:
              add:
              - SYS_ADMIN
              - SYS_CHROOT
              - SYS_PTRACE
              drop:
              - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /hostproc
            name: hostproc
          - mountPath: /hostbin
            name: cni-path
        - args:
          - mount | grep "/sys/fs/bpf type bpf" || mount -t bpf bpf /sys/fs/bpf
          command:
          - /bin/bash
          - -c
          - --
          image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
          imagePullPolicy: IfNotPresent
          name: mount-bpf-fs
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /sys/fs/bpf
            mountPropagation: Bidirectional
            name: bpf-maps
        - command:
          - /init-container.sh
          env:
          - name: CILIUM_ALL_STATE
            valueFrom:
              configMapKeyRef:
                key: clean-cilium-state
                name: cilium-config
                optional: true
          - name: CILIUM_BPF_STATE
            valueFrom:
              configMapKeyRef:
                key: clean-cilium-bpf-state
                name: cilium-config
                optional: true
          - name: KUBERNETES_SERVICE_HOST
            value: f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
          imagePullPolicy: IfNotPresent
          name: clean-cilium-state
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
              - SYS_MODULE
              - SYS_ADMIN
              - SYS_RESOURCE
              drop:
              - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /sys/fs/bpf
            name: bpf-maps
          - mountPath: /run/cilium/cgroupv2
            mountPropagation: HostToContainer
            name: cilium-cgroup
          - mountPath: /var/run/cilium
            name: cilium-run
        - command:
          - /install-plugin.sh
          image: docker.io/cilium/cilium:v1.14.10@sha256:0a1bcd2859c6d18d60dba6650cca8c707101716a3e47b126679040cbd621c031
          imagePullPolicy: IfNotPresent
          name: install-cni-binaries
          resources:
            requests:
              cpu: 100m
              memory: 10Mi
          securityContext:
            capabilities:
              drop:
              - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /host/opt/cni/bin
            name: cni-path
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cilium
        serviceAccountName: cilium
        terminationGracePeriodSeconds: 1
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /usr/bin/kubectl
            type: File
          name: host-kubectl
        - emptyDir: {}
          name: tmp
        - hostPath:
            path: /var/run/cilium
            type: DirectoryOrCreate
          name: cilium-run
        - hostPath:
            path: /sys/fs/bpf
            type: DirectoryOrCreate
          name: bpf-maps
        - hostPath:
            path: /proc
            type: Directory
          name: hostproc
        - hostPath:
            path: /run/cilium/cgroupv2
            type: DirectoryOrCreate
          name: cilium-cgroup
        - hostPath:
            path: /opt/cni/bin
            type: DirectoryOrCreate
          name: cni-path
        - hostPath:
            path: /etc/cni/net.d
            type: DirectoryOrCreate
          name: etc-cni-netd
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - name: clustermesh-secrets
          projected:
            defaultMode: 256
            sources:
            - secret:
                name: cilium-clustermesh
                optional: true
            - secret:
                items:
                - key: tls.key
                  path: common-etcd-client.key
                - key: tls.crt
                  path: common-etcd-client.crt
                - key: ca.crt
                  path: common-etcd-client-ca.crt
                name: clustermesh-apiserver-remote-cert
                optional: true
        - hostPath:
            path: /proc/sys/net
            type: Directory
          name: host-proc-sys-net
        - hostPath:
            path: /proc/sys/kernel
            type: Directory
          name: host-proc-sys-kernel
        - name: hubble-tls
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: tls.crt
                  path: server.crt
                - key: tls.key
                  path: server.key
                - key: ca.crt
                  path: client-ca.crt
                name: hubble-server-certs
                optional: true
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 10%
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2024-06-09T16:25:19Z"
    generation: 1
    labels:
      app: cpc-bridge-proxy
      c3.doks.digitalocean.com/component: cpc-bridge-proxy
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
    name: cpc-bridge-proxy
    namespace: kube-system
    resourceVersion: "7820339"
    uid: ed5c6c96-c2b0-4db2-81e2-228ec8de77e2
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: cpc-bridge-proxy
    template:
      metadata:
        annotations:
          clusterlint.digitalocean.com/disabled-checks: resource-requirements
        creationTimestamp: null
        labels:
          app: cpc-bridge-proxy
          doks.digitalocean.com/managed: "true"
      spec:
        automountServiceAccountToken: false
        containers:
        - image: digitalocean/cpbridge:1.25.3
          imagePullPolicy: IfNotPresent
          name: cpc-bridge-proxy
          resources:
            requests:
              cpu: 100m
              memory: 75Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/nginx
            name: cpc-bridge-proxy-config
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - command:
          - /bin/bash
          - -c
          - |
            set -o errexit
            set -o pipefail
            set -o nounset
            ipt_nat="iptables-legacy -t nat"
            # Avoid racing with kube-proxy on the initial iptables rules population which makes the rule order indeterministic.
            until ${ipt_nat} --list KUBE-SERVICES > /dev/null; do echo "waiting for kube-proxy to populate iptables rules"; sleep 3; done
            ipt_output_args="OUTPUT -p tcp -d 10.245.0.1/32 --dport 443 -j DNAT --to-destination 100.65.52.51:16443"
            ipt_prerouting_args="PREROUTING -p tcp -d 100.65.52.51 --dport 443 -j DNAT --to-destination 100.65.52.51:16443"
            ${ipt_nat} --check ${ipt_output_args} || ${ipt_nat} --insert ${ipt_output_args}
            ${ipt_nat} --check ${ipt_prerouting_args} || ${ipt_nat} --insert ${ipt_prerouting_args}
          image: digitalocean/cpbridge:1.25.3
          imagePullPolicy: IfNotPresent
          name: init-iptables
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: cpc-bridge-proxy-config
          name: cpc-bridge-proxy-config
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "2"
    creationTimestamp: "2024-06-09T16:25:45Z"
    generation: 2
    labels:
      c3.doks.digitalocean.com/component: csi-node-service
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
    name: csi-do-node
    namespace: kube-system
    resourceVersion: "7820340"
    uid: b7a14168-e8bb-439d-aff4-78856303073b
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-do-node
    template:
      metadata:
        annotations:
          clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
          kubectl.kubernetes.io/default-container: csi-do-plugin
        creationTimestamp: null
        labels:
          app: csi-do-node
          doks.digitalocean.com/managed: "true"
          role: csi-do
      spec:
        containers:
        - args:
          - --v=5
          - --csi-address=$(ADDRESS)
          - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: DRIVER_REG_SOCK_PATH
            value: /var/lib/kubelet/plugins/dobs.csi.digitalocean.com/csi.sock
          - name: KUBE_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.3
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/sh
                - -c
                - rm -rf /registration/dobs.csi.digitalocean.com /registration/dobs.csi.digitalocean.com-reg.sock
          name: csi-node-driver-registrar
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: plugin-dir
          - mountPath: /registration/
            name: registration-dir
        - args:
          - --endpoint=$(CSI_ENDPOINT)
          - --validate-attachment=true
          - --url=https://api.digitalocean.com
          - --driver-name=dobs.csi.digitalocean.com
          env:
          - name: CSI_ENDPOINT
            value: unix:///csi/csi.sock
          image: docker.io/digitalocean/do-csi-plugin:v4.10.0
          imagePullPolicy: Always
          name: csi-do-plugin
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /var/lib/kubelet
            mountPropagation: Bidirectional
            name: pods-mount-dir
          - mountPath: /dev
            name: device-dir
        dnsPolicy: ClusterFirst
        hostNetwork: true
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: csi-do-node-sa
        serviceAccountName: csi-do-node-sa
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: DirectoryOrCreate
          name: registration-dir
        - hostPath:
            path: /var/lib/kubelet/plugins/dobs.csi.digitalocean.com
            type: DirectoryOrCreate
          name: plugin-dir
        - hostPath:
            path: /var/lib/kubelet
            type: Directory
          name: pods-mount-dir
        - hostPath:
            path: /dev
            type: ""
          name: device-dir
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 10%
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 2
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2024-06-09T16:26:05Z"
    generation: 1
    labels:
      app: do-node-agent
      c3.doks.digitalocean.com/component: do-node-agent
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
    name: do-node-agent
    namespace: kube-system
    resourceVersion: "7820341"
    uid: 1a45765d-3c7e-442b-8f76-11d21f5c353c
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: do-node-agent
    template:
      metadata:
        annotations:
          clusterlint.digitalocean.com/disabled-checks: hostpath-volume
        creationTimestamp: null
        labels:
          app: do-node-agent
          doks.digitalocean.com/managed: "true"
      spec:
        containers:
        - args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --k8s-metrics-path=http://kube-state-metrics.kube-system.svc.cluster.local:8080/metrics
          - --additional-label=kubernetes_cluster_uuid:f9962841-03c0-44b1-bde7-37fc077b7777
          command:
          - /bin/do-agent
          image: docker.io/digitalocean/do-agent:3.16.7
          imagePullPolicy: IfNotPresent
          name: do-node-agent
          resources:
            limits:
              memory: 300Mi
            requests:
              cpu: 102m
              memory: 80Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: do-agent
        serviceAccountName: do-agent
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2024-06-09T16:24:53Z"
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      c3.doks.digitalocean.com/component: konnectivity-agent
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: konnectivity-agent
    name: konnectivity-agent
    namespace: kube-system
    resourceVersion: "7820338"
    uid: 4283b747-52bc-4c3d-9853-6c9ce8e24d7e
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: konnectivity-agent
    template:
      metadata:
        creationTimestamp: null
        labels:
          doks.digitalocean.com/managed: "true"
          k8s-app: konnectivity-agent
      spec:
        containers:
        - args:
          - --logtostderr=true
          - --ca-cert=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          - --proxy-server-port=8132
          - --admin-server-port=8133
          - --health-server-port=8134
          - --keepalive-time=5m
          - --service-account-token-path=/var/run/secrets/tokens/konnectivity-agent-token
          - --proxy-server-host=f9962841-03c0-44b1-bde7-37fc077b7777.k8s.ondigitalocean.com
          command:
          - /proxy-agent
          image: registry.k8s.io/kas-network-proxy/proxy-agent:v0.29.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8134
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: konnectivity-agent
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/run/secrets/tokens
            name: konnectivity-agent-token
        dnsPolicy: Default
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: konnectivity-agent
        serviceAccountName: konnectivity-agent
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - name: konnectivity-agent-token
          projected:
            defaultMode: 420
            sources:
            - serviceAccountToken:
                audience: system:konnectivity-server
                expirationSeconds: 3600
                path: konnectivity-agent-token
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2024-06-09T16:10:51Z"
    generation: 1
    labels:
      c3.doks.digitalocean.com/component: kube-proxy
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: kube-proxy
      tier: node
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "7820333"
    uid: af07a04a-b595-4e65-a81d-fdf7fbb57537
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-proxy
        tier: node
    template:
      metadata:
        annotations:
          clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
        creationTimestamp: null
        labels:
          doks.digitalocean.com/managed: "true"
          k8s-app: kube-proxy
          tier: node
      spec:
        containers:
        - command:
          - kube-proxy
          - --config=/etc/kubernetes/config/kube-proxy-config.yaml
          image: registry.k8s.io/kube-proxy:v1.29.5
          imagePullPolicy: IfNotPresent
          name: kube-proxy
          resources:
            requests:
              memory: 125Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/kubernetes
            name: kube-proxy-kubeconfig
            readOnly: true
          - mountPath: /etc/kubernetes/config
            name: kube-proxy-config
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-proxy
        serviceAccountName: kube-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - name: kube-proxy-kubeconfig
          secret:
            defaultMode: 420
            secretName: kube-proxy
        - configMap:
            defaultMode: 420
            name: kube-proxy
          name: kube-proxy-config
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-16T08:51:43Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.1
      helm.sh/chart: prometheus-node-exporter-4.36.0
      jobLabel: node-exporter
      release: prometheus
    name: prometheus-prometheus-node-exporter
    namespace: prometheus
    resourceVersion: "7818627"
    uid: 856c8417-6598-4615-9c86-d8420269cd9f
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-node-exporter
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-node-exporter
          app.kubernetes.io/part-of: prometheus-node-exporter
          app.kubernetes.io/version: 1.8.1
          helm.sh/chart: prometheus-node-exporter-4.36.0
          jobLabel: node-exporter
          release: prometheus
      spec:
        automountServiceAccountToken: false
        containers:
        - args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --path.rootfs=/host/root
          - --path.udev.data=/host/root/run/udev/data
          - --web.listen-address=[$(HOST_IP)]:9100
          - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
          - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
          env:
          - name: HOST_IP
            value: 0.0.0.0
          image: quay.io/prometheus/node-exporter:v1.8.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: node-exporter
          ports:
          - containerPort: 9100
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-node-exporter
        serviceAccountName: prometheus-prometheus-node-exporter
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-06-15T23:03:40Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
      chart: airflow-1.13.1
      component: scheduler
      executor: KubernetesExecutor
      heritage: Helm
      release: airflow
      tier: airflow
    name: airflow-scheduler
    namespace: airflow
    resourceVersion: "7816371"
    uid: 5b2e4d39-5868-478f-ba2a-773d2405d6e8
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        component: scheduler
        release: airflow
        tier: airflow
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/airflow-config: cdff98e8c3f0bebc19523bf34fd5a6eb9ca62c9c556e71201f3b5386fd6127b2
          checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8
          checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827
          checksum/metadata-secret: 1527346545415bf13f4c9ad69470086eb90d854f2b83594d78ec1badb5e13eb0
          checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688
          checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          component: scheduler
          release: airflow
          tier: airflow
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    component: scheduler
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - bash
          - -c
          - exec airflow scheduler
          env:
          - name: AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW_VAR_SLACK_WEB_HOOK
            valueFrom:
              secretKeyRef:
                key: SLACK_WEB_HOOK
                name: webhook-secret
          - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
            value: webhook-secret=SLACK_WEB_HOOK
          - name: AIRFLOW__CORE__FERNET_KEY
            valueFrom:
              secretKeyRef:
                key: fernet-key
                name: airflow-fernet-key
          - name: AIRFLOW_HOME
            value: /opt/airflow
          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW_CONN_AIRFLOW_DB
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__WEBSERVER__SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: webserver-secret-key
                name: airflow-webserver-secret-key
          image: docker.io/minhtuyenvp02/airflow-spark:pr-49
          imagePullPolicy: Always
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - |
                CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
                airflow jobs check --job-type SchedulerJob --local
            failureThreshold: 30
            initialDelaySeconds: 120
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: scheduler
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          startupProbe:
            exec:
              command:
              - sh
              - -c
              - |
                CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
                airflow jobs check --job-type SchedulerJob --local
            failureThreshold: 6
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 20
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/pod_templates/pod_template_file.yaml
            name: config
            readOnly: true
            subPath: pod_template_file.yaml
          - mountPath: /opt/airflow/logs
            name: logs
          - mountPath: /opt/airflow/airflow.cfg
            name: config
            readOnly: true
            subPath: airflow.cfg
          - mountPath: /opt/airflow/config/airflow_local_settings.py
            name: config
            readOnly: true
            subPath: airflow_local_settings.py
          - mountPath: /opt/airflow/dags
            name: dags
            readOnly: true
        - env:
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/ssh
          - name: GITSYNC_SSH_KEY_FILE
            value: /etc/git-secret/ssh
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GITSYNC_SSH
            value: "true"
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: GITSYNC_SSH_KNOWN_HOSTS
            value: "false"
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GITSYNC_REF
            value: v2-2-stable
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REPO
            value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
          - name: GITSYNC_REPO
            value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GITSYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_ROOT
            value: /git
          - name: GITSYNC_ROOT
            value: /git
          - name: GIT_SYNC_DEST
            value: repo
          - name: GITSYNC_LINK
            value: repo
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GITSYNC_ADD_USER
            value: "true"
          - name: GITSYNC_PERIOD
            value: 5s
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GITSYNC_MAX_FAILURES
            value: "0"
          image: registry.k8s.io/git-sync/git-sync:v4.1.0
          imagePullPolicy: IfNotPresent
          name: git-sync
          resources: {}
          securityContext:
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /git
            name: dags
          - mountPath: /etc/git-secret/ssh
            name: git-sync-ssh-key
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - /clean-logs
          env:
          - name: AIRFLOW__LOG_RETENTION_DAYS
            value: "15"
          - name: AIRFLOW_HOME
            value: /opt/airflow
          image: docker.io/minhtuyenvp02/airflow-spark:pr-49
          imagePullPolicy: Always
          name: scheduler-log-groomer
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/logs
            name: logs
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - airflow
          - db
          - check-migrations
          - --migration-wait-timeout=60
          env:
          - name: AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW_VAR_SLACK_WEB_HOOK
            valueFrom:
              secretKeyRef:
                key: SLACK_WEB_HOOK
                name: webhook-secret
          - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
            value: webhook-secret=SLACK_WEB_HOOK
          - name: AIRFLOW__CORE__FERNET_KEY
            valueFrom:
              secretKeyRef:
                key: fernet-key
                name: airflow-fernet-key
          - name: AIRFLOW_HOME
            value: /opt/airflow
          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW_CONN_AIRFLOW_DB
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__WEBSERVER__SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: webserver-secret-key
                name: airflow-webserver-secret-key
          image: docker.io/minhtuyenvp02/airflow-spark:pr-49
          imagePullPolicy: Always
          name: wait-for-airflow-migrations
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/airflow.cfg
            name: config
            readOnly: true
            subPath: airflow.cfg
          - mountPath: /opt/airflow/config/airflow_local_settings.py
            name: config
            readOnly: true
            subPath: airflow_local_settings.py
        - env:
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/ssh
          - name: GITSYNC_SSH_KEY_FILE
            value: /etc/git-secret/ssh
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GITSYNC_SSH
            value: "true"
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: GITSYNC_SSH_KNOWN_HOSTS
            value: "false"
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GITSYNC_REF
            value: v2-2-stable
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REPO
            value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
          - name: GITSYNC_REPO
            value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GITSYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_ROOT
            value: /git
          - name: GITSYNC_ROOT
            value: /git
          - name: GIT_SYNC_DEST
            value: repo
          - name: GITSYNC_LINK
            value: repo
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GITSYNC_ADD_USER
            value: "true"
          - name: GITSYNC_PERIOD
            value: 5s
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GITSYNC_MAX_FAILURES
            value: "0"
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GITSYNC_ONE_TIME
            value: "true"
          image: registry.k8s.io/git-sync/git-sync:v4.1.0
          imagePullPolicy: IfNotPresent
          name: git-sync-init
          resources: {}
          securityContext:
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /git
            name: dags
          - mountPath: /etc/git-secret/ssh
            name: git-sync-ssh-key
            readOnly: true
            subPath: gitSshKey
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
          runAsUser: 50000
        serviceAccount: airflow-scheduler
        serviceAccountName: airflow-scheduler
        terminationGracePeriodSeconds: 10
        volumes:
        - configMap:
            defaultMode: 420
            name: airflow-config
          name: config
        - emptyDir: {}
          name: dags
        - name: git-sync-ssh-key
          secret:
            defaultMode: 288
            secretName: airflow-ssh-secret
        - emptyDir: {}
          name: logs
  status:
    conditions:
    - lastTransitionTime: "2024-06-15T23:03:40Z"
      lastUpdateTime: "2024-06-15T23:05:13Z"
      message: ReplicaSet "airflow-scheduler-64b495fdcd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-07-01T07:28:22Z"
      lastUpdateTime: "2024-07-01T07:28:22Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    observedGeneration: 1
    replicas: 1
    unavailableReplicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-06-15T23:03:40Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
      chart: airflow-1.13.1
      component: statsd
      heritage: Helm
      release: airflow
      tier: airflow
    name: airflow-statsd
    namespace: airflow
    resourceVersion: "7816870"
    uid: 15d26764-597b-45ba-a689-2ee8b3cab236
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        component: statsd
        release: airflow
        tier: airflow
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          component: statsd
          release: airflow
          tier: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml
          image: quay.io/prometheus/statsd-exporter:v0.26.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: 9102
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: statsd
          ports:
          - containerPort: 9125
            name: statsd-ingest
            protocol: UDP
          - containerPort: 9102
            name: statsd-scrape
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: 9102
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/statsd-exporter/mappings.yml
            name: config
            subPath: mappings.yml
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 65534
        serviceAccount: airflow-statsd
        serviceAccountName: airflow-statsd
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: airflow-statsd
          name: config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-06-15T23:03:40Z"
      lastUpdateTime: "2024-06-15T23:04:00Z"
      message: ReplicaSet "airflow-statsd-5667dd85ff" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-07-01T07:28:33Z"
      lastUpdateTime: "2024-07-01T07:28:33Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-06-16T19:59:10Z"
    generation: 3
    labels:
      app.kubernetes.io/managed-by: Helm
      chart: airflow-1.13.1
      component: webserver
      heritage: Helm
      release: airflow
      tier: airflow
    name: airflow-webserver
    namespace: airflow
    resourceVersion: "7816369"
    uid: ab3c3831-f8f9-4be5-a0e5-a332ac0366b4
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 1
    selector:
      matchLabels:
        component: webserver
        release: airflow
        tier: airflow
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/airflow-config: cdff98e8c3f0bebc19523bf34fd5a6eb9ca62c9c556e71201f3b5386fd6127b2
          checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8
          checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827
          checksum/metadata-secret: 1527346545415bf13f4c9ad69470086eb90d854f2b83594d78ec1badb5e13eb0
          checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688
          checksum/webserver-config: 2f3fdfd294a37094d2abee43b2b09888a5c195ee03414996bf99a4681658af94
          checksum/webserver-secret-key: c2e7d741976bcca47534b96b419058c6ee4a482ca2ee5adad27507770148319b
        creationTimestamp: null
        labels:
          component: webserver
          release: airflow
          tier: airflow
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    component: webserver
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - bash
          - -c
          - exec airflow webserver
          env:
          - name: AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW_VAR_SLACK_WEB_HOOK
            valueFrom:
              secretKeyRef:
                key: SLACK_WEB_HOOK
                name: webhook-secret
          - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
            value: webhook-secret=SLACK_WEB_HOOK
          - name: AIRFLOW__CORE__FERNET_KEY
            valueFrom:
              secretKeyRef:
                key: fernet-key
                name: airflow-fernet-key
          - name: AIRFLOW_HOME
            value: /opt/airflow
          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW_CONN_AIRFLOW_DB
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__WEBSERVER__SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: webserver-secret-key
                name: airflow-webserver-secret-key
          image: docker.io/minhtuyenvp02/airflow-spark:pr-49
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: webserver
          ports:
          - containerPort: 8080
            name: airflow-ui
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "3"
              memory: 5Gi
            requests:
              cpu: "2"
              memory: 2Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          startupProbe:
            failureThreshold: 6
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/pod_templates/pod_template_file.yaml
            name: config
            readOnly: true
            subPath: pod_template_file.yaml
          - mountPath: /opt/airflow/airflow.cfg
            name: config
            readOnly: true
            subPath: airflow.cfg
          - mountPath: /opt/airflow/config/airflow_local_settings.py
            name: config
            readOnly: true
            subPath: airflow_local_settings.py
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - airflow
          - db
          - check-migrations
          - --migration-wait-timeout=60
          env:
          - name: AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW_VAR_SLACK_WEB_HOOK
            valueFrom:
              secretKeyRef:
                key: SLACK_WEB_HOOK
                name: webhook-secret
          - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
            value: webhook-secret=SLACK_WEB_HOOK
          - name: AIRFLOW__CORE__FERNET_KEY
            valueFrom:
              secretKeyRef:
                key: fernet-key
                name: airflow-fernet-key
          - name: AIRFLOW_HOME
            value: /opt/airflow
          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW_CONN_AIRFLOW_DB
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__WEBSERVER__SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: webserver-secret-key
                name: airflow-webserver-secret-key
          image: docker.io/minhtuyenvp02/airflow-spark:pr-49
          imagePullPolicy: Always
          name: wait-for-airflow-migrations
          resources:
            limits:
              cpu: "3"
              memory: 5Gi
            requests:
              cpu: "2"
              memory: 2Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/airflow.cfg
            name: config
            readOnly: true
            subPath: airflow.cfg
          - mountPath: /opt/airflow/config/airflow_local_settings.py
            name: config
            readOnly: true
            subPath: airflow_local_settings.py
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
          runAsUser: 50000
        serviceAccount: airflow-webserver
        serviceAccountName: airflow-webserver
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: airflow-config
          name: config
  status:
    conditions:
    - lastTransitionTime: "2024-06-16T19:59:10Z"
      lastUpdateTime: "2024-06-16T20:23:07Z"
      message: ReplicaSet "airflow-webserver-676ff8f7b9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-07-01T07:28:22Z"
      lastUpdateTime: "2024-07-01T07:28:22Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    observedGeneration: 3
    replicas: 1
    unavailableReplicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: ingress
      meta.helm.sh/release-namespace: ingress
    creationTimestamp: "2024-06-14T20:03:17Z"
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.10.1
      helm.sh/chart: ingress-nginx-4.10.1
    name: ingress-ingress-nginx-controller
    namespace: ingress
    resourceVersion: "5341035"
    uid: ea46797a-d902-41ce-aaa5-ed2d4a329053
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress
        app.kubernetes.io/name: ingress-nginx
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: ingress
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.10.1
          helm.sh/chart: ingress-nginx-4.10.1
      spec:
        containers:
        - args:
          - /nginx-ingress-controller
          - --publish-service=$(POD_NAMESPACE)/ingress-ingress-nginx-controller
          - --election-id=ingress-ingress-nginx-leader
          - --controller-class=k8s.io/ingress-nginx
          - --ingress-class=nginx
          - --configmap=$(POD_NAMESPACE)/ingress-ingress-nginx-controller
          - --validating-webhook=:8443
          - --validating-webhook-certificate=/usr/local/certificates/cert
          - --validating-webhook-key=/usr/local/certificates/key
          - --enable-metrics=false
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_PRELOAD
            value: /usr/local/lib/libmimalloc.so
          image: registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /wait-shutdown
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 80
            hostPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            hostPort: 443
            name: https
            protocol: TCP
          - containerPort: 8443
            name: webhook
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 90Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 101
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/local/certificates/
            name: webhook-cert
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-ingress-nginx
        serviceAccountName: ingress-ingress-nginx
        terminationGracePeriodSeconds: 300
        volumes:
        - name: webhook-cert
          secret:
            defaultMode: 420
            secretName: ingress-ingress-nginx-admission
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-06-14T20:21:28Z"
      lastUpdateTime: "2024-06-14T20:21:39Z"
      message: ReplicaSet "ingress-ingress-nginx-controller-6b4df77c7b" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-06-24T11:37:49Z"
      lastUpdateTime: "2024-06-24T11:37:49Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kafka-metrics
      meta.helm.sh/release-namespace: kafka
    creationTimestamp: "2024-06-16T09:10:46Z"
    generation: 1
    labels:
      app: prometheus-kafka-exporter
      app.kubernetes.io/managed-by: Helm
      chart: prometheus-kafka-exporter-2.10.0
      heritage: Helm
      release: kafka-metrics
    name: kafka-metrics-prometheus-kafka-exporter
    namespace: kafka
    resourceVersion: "2398537"
    uid: 7dad9b5b-2901-4e8f-b665-c8f804dfcd7a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: prometheus-kafka-exporter
        release: kafka-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: prometheus-kafka-exporter
          release: kafka-metrics
      spec:
        containers:
        - args:
          - --verbosity=0
          - --kafka.server=kafka-controller-headless.kafka.svc.cluster.local:9092
          - --kafka.version=3.6.1
          image: danielqsj/kafka-exporter:v1.7.0
          imagePullPolicy: IfNotPresent
          name: prometheus-kafka-exporter
          ports:
          - containerPort: 9308
            name: exporter-port
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kafka-metrics-prometheus-kafka-exporter
        serviceAccountName: kafka-metrics-prometheus-kafka-exporter
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-06-16T09:10:47Z"
      lastUpdateTime: "2024-06-16T09:10:47Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-06-16T09:10:46Z"
      lastUpdateTime: "2024-06-16T09:10:47Z"
      message: ReplicaSet "kafka-metrics-prometheus-kafka-exporter-86b48d7b76" has
        successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-06-09T16:25:34Z"
    generation: 1
    labels:
      c3.doks.digitalocean.com/component: coredns
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: kube-dns
      kubernetes.io/name: CoreDNS
    name: coredns
    namespace: kube-system
    resourceVersion: "7816837"
    uid: c81dd5ab-d846-4cee-a158-1169375eb657
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 100%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          doks.digitalocean.com/managed: "true"
          k8s-app: kube-dns
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: docker.io/coredns/coredns:1.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 150M
            requests:
              cpu: 100m
              memory: 150M
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2024-06-09T16:25:34Z"
      lastUpdateTime: "2024-06-09T16:25:43Z"
      message: ReplicaSet "coredns-6857f5494d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-06-14T13:39:03Z"
      lastUpdateTime: "2024-06-14T13:39:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-06-09T16:11:02Z"
    generation: 1
    labels:
      app.kubernetes.io/name: hubble-relay
      app.kubernetes.io/part-of: cilium
      c3.doks.digitalocean.com/component: cilium
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-relay
    name: hubble-relay
    namespace: kube-system
    resourceVersion: "7816855"
    uid: fecc7bc7-8e4a-4178-b6ac-2417f2b16483
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: hubble-relay
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: hubble-relay
          app.kubernetes.io/part-of: cilium
          doks.digitalocean.com/managed: "true"
          k8s-app: hubble-relay
      spec:
        affinity:
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: false
        containers:
        - args:
          - serve
          command:
          - hubble-relay
          image: quay.io/cilium/hubble-relay:v1.14.10@sha256:c156c4fc2da520d2876142ea17490440b95431a1be755d2050e72115a495cfd0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: grpc
            timeoutSeconds: 1
          name: hubble-relay
          ports:
          - containerPort: 4245
            name: grpc
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: grpc
            timeoutSeconds: 1
          resources: {}
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/hubble-relay
            name: config
            readOnly: true
          - mountPath: /var/lib/hubble-relay/tls
            name: tls
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65532
        serviceAccount: hubble-relay
        serviceAccountName: hubble-relay
        terminationGracePeriodSeconds: 1
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: config.yaml
              path: config.yaml
            name: hubble-relay-config
          name: config
        - name: tls
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: tls.crt
                  path: client.crt
                - key: tls.key
                  path: client.key
                - key: ca.crt
                  path: hubble-server-ca.crt
                name: hubble-relay-client-certs
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-06-09T16:11:02Z"
      lastUpdateTime: "2024-06-09T16:11:02Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-06-09T16:11:02Z"
      lastUpdateTime: "2024-06-09T16:13:12Z"
      message: ReplicaSet "hubble-relay-6b67bbc75f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-06-09T16:24:33Z"
    generation: 1
    labels:
      app.kubernetes.io/name: hubble-ui
      app.kubernetes.io/part-of: cilium
      c3.doks.digitalocean.com/component: cilium
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-ui
    name: hubble-ui
    namespace: kube-system
    resourceVersion: "7816974"
    uid: 24f50e9e-78e2-4335-96dc-43f8a0dfc763
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: hubble-ui
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: hubble-ui
          app.kubernetes.io/part-of: cilium
          doks.digitalocean.com/managed: "true"
          k8s-app: hubble-ui
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/cilium/hubble-ui:v0.13.0@sha256:7d663dc16538dd6e29061abd1047013a645e6e69c115e008bee9ea9fef9a6666
          imagePullPolicy: IfNotPresent
          name: frontend
          ports:
          - containerPort: 8081
            name: http
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/nginx/conf.d/default.conf
            name: hubble-ui-nginx-conf
            subPath: nginx.conf
          - mountPath: /tmp
            name: tmp-dir
        - env:
          - name: EVENTS_SERVER_PORT
            value: "8090"
          - name: FLOWS_API_ADDR
            value: hubble-relay:80
          image: quay.io/cilium/hubble-ui-backend:v0.13.0@sha256:1e7657d997c5a48253bb8dc91ecee75b63018d16ff5e5797e5af367336bc8803
          imagePullPolicy: IfNotPresent
          name: backend
          ports:
          - containerPort: 8090
            name: grpc
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hubble-ui
        serviceAccountName: hubble-ui
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: hubble-ui-nginx
          name: hubble-ui-nginx-conf
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-06-09T16:24:33Z"
      lastUpdateTime: "2024-06-09T16:24:33Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-06-09T16:24:33Z"
      lastUpdateTime: "2024-06-09T16:24:48Z"
      message: ReplicaSet "hubble-ui-776986f894" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-16T08:51:43Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 11.0.0
      helm.sh/chart: grafana-8.0.1
    name: prometheus-grafana
    namespace: prometheus
    resourceVersion: "3095582"
    uid: ead49d65-5ef5-4726-8414-5bad15553e78
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: grafana
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: d8d9157e07caef7c3a90b40c8373d900f3c676a242c290ba75f122344fe1a0cd
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: grafana
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.26.1
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.26.1
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:11.0.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: prometheus-grafana
        serviceAccountName: prometheus-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: prometheus-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-06-16T08:51:43Z"
      lastUpdateTime: "2024-06-16T09:32:21Z"
      message: ReplicaSet "prometheus-grafana-7c4bd5956d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-06-18T05:11:41Z"
      lastUpdateTime: "2024-06-18T05:11:41Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-16T08:51:44Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 60.1.0
      chart: kube-prometheus-stack-60.1.0
      heritage: Helm
      release: prometheus
    name: prometheus-kube-prometheus-operator
    namespace: prometheus
    resourceVersion: "2393490"
    uid: fbcffa0c-2ae5-4b68-b606-fc74050e31b3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        release: prometheus
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 60.1.0
          chart: kube-prometheus-stack-60.1.0
          heritage: Helm
          release: prometheus
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/prometheus-kube-prometheus-kubelet
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.35.1
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.74.0
          imagePullPolicy: IfNotPresent
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-prometheus-operator
        serviceAccountName: prometheus-kube-prometheus-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: prometheus-kube-prometheus-admission
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-06-16T08:51:51Z"
      lastUpdateTime: "2024-06-16T08:51:51Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-06-16T08:51:44Z"
      lastUpdateTime: "2024-06-16T08:51:51Z"
      message: ReplicaSet "prometheus-kube-prometheus-operator-64776975f5" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-16T08:51:44Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.12.0
      helm.sh/chart: kube-state-metrics-5.20.0
      release: prometheus
    name: prometheus-kube-state-metrics
    namespace: prometheus
    resourceVersion: "3095608"
    uid: 95a86be8-dda8-4d70-a29e-c912f12eb0ac
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.12.0
          helm.sh/chart: kube-state-metrics-5.20.0
          release: prometheus
      spec:
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-06-16T08:51:44Z"
      lastUpdateTime: "2024-06-16T08:52:14Z"
      message: ReplicaSet "prometheus-kube-state-metrics-6c97d9f546" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-06-18T05:11:44Z"
      lastUpdateTime: "2024-06-18T05:11:44Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: spark
      meta.helm.sh/release-namespace: spark
    creationTimestamp: "2024-06-14T12:47:56Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: spark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: spark-operator
      app.kubernetes.io/version: v1beta2-1.6.0-3.5.0
      helm.sh/chart: spark-operator-1.4.0
    name: spark-spark-operator
    namespace: spark
    resourceVersion: "7816611"
    uid: 0b1287d2-2acd-4156-85fa-e43f9a7b58d5
  spec:
    progressDeadlineSeconds: 600
    replicas: 3
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: spark
        app.kubernetes.io/name: spark-operator
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "10254"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: spark
          app.kubernetes.io/name: spark-operator
      spec:
        containers:
        - args:
          - -v=2
          - -logtostderr
          - -namespace=
          - -enable-ui-service=true
          - -ingress-url-format=spark-app.minhtuyenvp02.id.vn/{{$appNamespace}}/{{$appName}}
          - -controller-threads=10
          - -resync-interval=30
          - -enable-batch-scheduler=false
          - -label-selector-filter=
          - -enable-metrics=true
          - -metrics-labels=app_type
          - -metrics-port=10254
          - -metrics-endpoint=/metrics
          - -metrics-prefix=
          - -enable-webhook=true
          - -webhook-secret-name=spark-spark-operator-webhook-certs
          - -webhook-secret-namespace=spark
          - -webhook-svc-name=spark-spark-operator-webhook-svc
          - -webhook-svc-namespace=spark
          - -webhook-config-name=spark-spark-operator-webhook-config
          - -webhook-port=8080
          - -webhook-timeout=30
          - -webhook-namespace-selector=
          - -enable-resource-quota-enforcement=false
          - -leader-election=true
          - -leader-election-lock-namespace=spark
          - -leader-election-lock-name=spark-operator-lock
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: docker.io/kubeflow/spark-operator:v1beta2-1.6.0-3.5.0
          imagePullPolicy: IfNotPresent
          name: spark-operator
          ports:
          - containerPort: 10254
            name: metrics
            protocol: TCP
          - containerPort: 8080
            name: webhook
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: spark-spark-operator
        serviceAccountName: spark-spark-operator
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 3
    conditions:
    - lastTransitionTime: "2024-06-14T12:47:56Z"
      lastUpdateTime: "2024-06-28T21:04:05Z"
      message: ReplicaSet "spark-spark-operator-f88695599" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-07-01T07:28:25Z"
      lastUpdateTime: "2024-07-01T07:28:25Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 3
    replicas: 3
    updatedReplicas: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-09T21:32:39Z"
    generation: 3
    labels:
      app: superset
      app.kubernetes.io/managed-by: Helm
      chart: superset-0.12.11
      heritage: Helm
      release: superset
    name: superset
    namespace: superset
    resourceVersion: "6737962"
    uid: 231fdd41-4fd3-4899-8444-25cb7506304f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: superset
        release: superset
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/configOverrides: 216350f2670d8588217c1f8fb1cd345561207481b104501b0c79aa441ffc6f81
          checksum/configOverridesFiles: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/connections: 2d41a2c51ec2fc809be84fc0ea0e603c946d529b4a0e43f7257e617909561a75
          checksum/extraConfigs: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/extraSecretEnv: 72fbd486f368264fd746d16cc7e0316a6fc004ed1483dda1e4b8bde8662b6217
          checksum/extraSecrets: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/superset_bootstrap.sh: db546af547fe8beae35fb3461d8ee461b0e6fd06b9c289da0ba745bc34b6f35f
          checksum/superset_config.py: a02e301df0b06bb7adf5b4a4f13f965d4de2467d0006cd6c2728c2e4f7426ee1
          checksum/superset_init.sh: e6b1e8eac1f7a79a07a6c72a0e2ee6e09654eeb439c6bbe61bfd676917c41e02
        creationTimestamp: null
        labels:
          app: superset
          release: superset
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - . /app/pythonpath/superset_bootstrap.sh; /usr/bin/run-server.sh
          env:
          - name: SUPERSET_PORT
            value: "8088"
          envFrom:
          - secretRef:
              name: superset-env
          image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          name: superset
          ports:
          - containerPort: 8088
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/pythonpath
            name: superset-config
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/sh
          - -c
          - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -timeout 120s
          envFrom:
          - secretRef:
              name: superset-env
          image: apache/superset:dockerize
          imagePullPolicy: IfNotPresent
          name: wait-for-postgres
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        terminationGracePeriodSeconds: 30
        volumes:
        - name: superset-config
          secret:
            defaultMode: 420
            secretName: superset-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-06-09T21:52:46Z"
      lastUpdateTime: "2024-06-10T10:55:48Z"
      message: ReplicaSet "superset-67dbcb86b6" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-06-28T11:19:39Z"
      lastUpdateTime: "2024-06-28T11:19:39Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-09T21:32:39Z"
    generation: 3
    labels:
      app: superset-worker
      app.kubernetes.io/managed-by: Helm
      chart: superset-0.12.11
      heritage: Helm
      release: superset
    name: superset-worker
    namespace: superset
    resourceVersion: "7820461"
    uid: e0b403f2-d415-4087-a767-2b11c83f01df
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: superset-worker
        release: superset
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/configOverrides: 216350f2670d8588217c1f8fb1cd345561207481b104501b0c79aa441ffc6f81
          checksum/configOverridesFiles: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/connections: 2d41a2c51ec2fc809be84fc0ea0e603c946d529b4a0e43f7257e617909561a75
          checksum/extraConfigs: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/extraSecretEnv: 72fbd486f368264fd746d16cc7e0316a6fc004ed1483dda1e4b8bde8662b6217
          checksum/extraSecrets: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/superset_bootstrap.sh: db546af547fe8beae35fb3461d8ee461b0e6fd06b9c289da0ba745bc34b6f35f
          checksum/superset_config.py: a02e301df0b06bb7adf5b4a4f13f965d4de2467d0006cd6c2728c2e4f7426ee1
        creationTimestamp: null
        labels:
          app: superset-worker
          release: superset
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - . /app/pythonpath/superset_bootstrap.sh; celery --app=superset.tasks.celery_app:app
            worker
          env:
          - name: SUPERSET_PORT
            value: "8088"
          envFrom:
          - secretRef:
              name: superset-env
          image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - celery -A superset.tasks.celery_app:app inspect ping -d celery@$HOSTNAME
            failureThreshold: 3
            initialDelaySeconds: 120
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 60
          name: superset
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/pythonpath
            name: superset-config
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/sh
          - -c
          - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -wait "tcp://$REDIS_HOST:$REDIS_PORT"
            -timeout 120s
          envFrom:
          - secretRef:
              name: superset-env
          image: apache/superset:dockerize
          imagePullPolicy: IfNotPresent
          name: wait-for-postgres-redis
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        terminationGracePeriodSeconds: 30
        volumes:
        - name: superset-config
          secret:
            defaultMode: 420
            secretName: superset-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-06-09T21:32:39Z"
      lastUpdateTime: "2024-06-10T10:55:30Z"
      message: ReplicaSet "superset-worker-6b94cb97f4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-07-01T07:40:16Z"
      lastUpdateTime: "2024-07-01T07:40:16Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: my-trino
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-09T16:26:37Z"
    generation: 4
    labels:
      app.kubernetes.io/component: coordinator
      app.kubernetes.io/instance: my-trino
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: trino
      app.kubernetes.io/version: "449"
      helm.sh/chart: trino-0.24.0
    name: my-trino-trino-coordinator
    namespace: trino
    resourceVersion: "7815824"
    uid: cdefe2a3-d11d-4167-9526-9ee3d954da33
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: coordinator
        app.kubernetes.io/instance: my-trino
        app.kubernetes.io/name: trino
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/catalog-config: 8d01709a0b96dbd56c7133d24548de777a150b769cb13ff95da7b9e77c4fff76
          checksum/coordinator-config: cfbddf70ff23b7be16459b6b2eb01610aab047b44f783a998c63b4df23045e5d
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: coordinator
          app.kubernetes.io/instance: my-trino
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: trino
          app.kubernetes.io/version: "449"
          helm.sh/chart: trino-0.24.0
      spec:
        containers:
        - image: trinodb/trino:449
          imagePullPolicy: IfNotPresent
          lifecycle: {}
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /v1/info
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: trino-coordinator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /usr/lib/trino/bin/health-check
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/trino
            name: config-volume
          - mountPath: /etc/trino/catalog
            name: catalog-volume
          - mountPath: /etc/trino/schemas
            name: schemas-volume
          - mountPath: /etc/redis
            name: redis-table-schema-volumn
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: registry-credentials
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: my-trino-trino-coordinator
          name: config-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-catalog
          name: catalog-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-schemas-volume-coordinator
          name: schemas-volume
        - name: redis-table-schema-volumn
          secret:
            defaultMode: 420
            secretName: redis-table-definition
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-06-09T16:26:37Z"
      lastUpdateTime: "2024-07-01T02:19:23Z"
      message: ReplicaSet "my-trino-trino-coordinator-6dc547c5c" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-07-01T07:26:42Z"
      lastUpdateTime: "2024-07-01T07:26:42Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: my-trino
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-09T16:26:37Z"
    generation: 5
    labels:
      app.kubernetes.io/component: worker
      app.kubernetes.io/instance: my-trino
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: trino
      app.kubernetes.io/version: "449"
      helm.sh/chart: trino-0.24.0
    name: my-trino-trino-worker
    namespace: trino
    resourceVersion: "7816362"
    uid: 50bc6cc8-ae37-4300-83d7-61fd9651b2ba
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: worker
        app.kubernetes.io/instance: my-trino
        app.kubernetes.io/name: trino
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/worker-config: ddb69b7237b4ad8c576018f14ee85dac71a80333e4729219c40eaaee8f99f09d
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: worker
          app.kubernetes.io/instance: my-trino
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: trino
          app.kubernetes.io/version: "449"
          helm.sh/chart: trino-0.24.0
      spec:
        containers:
        - image: trinodb/trino:449
          imagePullPolicy: IfNotPresent
          lifecycle: {}
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /v1/info
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: trino-worker
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /usr/lib/trino/bin/health-check
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 3Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/trino
            name: config-volume
          - mountPath: /etc/trino/catalog
            name: catalog-volume
          - mountPath: /etc/trino/schemas
            name: schemas-volume
          - mountPath: /etc/redis
            name: redis-table-schema-volumn
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: registry-credentials
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: my-trino-trino-worker
          name: config-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-catalog
          name: catalog-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-schemas-volume-worker
          name: schemas-volume
        - name: redis-table-schema-volumn
          secret:
            defaultMode: 420
            secretName: redis-table-definition
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-07-01T02:27:28Z"
      lastUpdateTime: "2024-07-01T02:28:20Z"
      message: ReplicaSet "my-trino-trino-worker-56d895b7fb" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-07-01T07:28:22Z"
      lastUpdateTime: "2024-07-01T07:28:22Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    observedGeneration: 5
    readyReplicas: 1
    replicas: 2
    unavailableReplicas: 1
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-06-15T23:03:40Z"
    generation: 1
    labels:
      component: scheduler
      pod-template-hash: 64b495fdcd
      release: airflow
      tier: airflow
    name: airflow-scheduler-64b495fdcd
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-scheduler
      uid: 5b2e4d39-5868-478f-ba2a-773d2405d6e8
    resourceVersion: "7816370"
    uid: cfd15427-3847-4af4-b649-a6698237bcd0
  spec:
    replicas: 1
    selector:
      matchLabels:
        component: scheduler
        pod-template-hash: 64b495fdcd
        release: airflow
        tier: airflow
    template:
      metadata:
        annotations:
          checksum/airflow-config: cdff98e8c3f0bebc19523bf34fd5a6eb9ca62c9c556e71201f3b5386fd6127b2
          checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8
          checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827
          checksum/metadata-secret: 1527346545415bf13f4c9ad69470086eb90d854f2b83594d78ec1badb5e13eb0
          checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688
          checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          component: scheduler
          pod-template-hash: 64b495fdcd
          release: airflow
          tier: airflow
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    component: scheduler
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - bash
          - -c
          - exec airflow scheduler
          env:
          - name: AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW_VAR_SLACK_WEB_HOOK
            valueFrom:
              secretKeyRef:
                key: SLACK_WEB_HOOK
                name: webhook-secret
          - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
            value: webhook-secret=SLACK_WEB_HOOK
          - name: AIRFLOW__CORE__FERNET_KEY
            valueFrom:
              secretKeyRef:
                key: fernet-key
                name: airflow-fernet-key
          - name: AIRFLOW_HOME
            value: /opt/airflow
          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW_CONN_AIRFLOW_DB
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__WEBSERVER__SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: webserver-secret-key
                name: airflow-webserver-secret-key
          image: docker.io/minhtuyenvp02/airflow-spark:pr-49
          imagePullPolicy: Always
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - |
                CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
                airflow jobs check --job-type SchedulerJob --local
            failureThreshold: 30
            initialDelaySeconds: 120
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: scheduler
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          startupProbe:
            exec:
              command:
              - sh
              - -c
              - |
                CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
                airflow jobs check --job-type SchedulerJob --local
            failureThreshold: 6
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 20
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/pod_templates/pod_template_file.yaml
            name: config
            readOnly: true
            subPath: pod_template_file.yaml
          - mountPath: /opt/airflow/logs
            name: logs
          - mountPath: /opt/airflow/airflow.cfg
            name: config
            readOnly: true
            subPath: airflow.cfg
          - mountPath: /opt/airflow/config/airflow_local_settings.py
            name: config
            readOnly: true
            subPath: airflow_local_settings.py
          - mountPath: /opt/airflow/dags
            name: dags
            readOnly: true
        - env:
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/ssh
          - name: GITSYNC_SSH_KEY_FILE
            value: /etc/git-secret/ssh
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GITSYNC_SSH
            value: "true"
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: GITSYNC_SSH_KNOWN_HOSTS
            value: "false"
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GITSYNC_REF
            value: v2-2-stable
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REPO
            value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
          - name: GITSYNC_REPO
            value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GITSYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_ROOT
            value: /git
          - name: GITSYNC_ROOT
            value: /git
          - name: GIT_SYNC_DEST
            value: repo
          - name: GITSYNC_LINK
            value: repo
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GITSYNC_ADD_USER
            value: "true"
          - name: GITSYNC_PERIOD
            value: 5s
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GITSYNC_MAX_FAILURES
            value: "0"
          image: registry.k8s.io/git-sync/git-sync:v4.1.0
          imagePullPolicy: IfNotPresent
          name: git-sync
          resources: {}
          securityContext:
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /git
            name: dags
          - mountPath: /etc/git-secret/ssh
            name: git-sync-ssh-key
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - /clean-logs
          env:
          - name: AIRFLOW__LOG_RETENTION_DAYS
            value: "15"
          - name: AIRFLOW_HOME
            value: /opt/airflow
          image: docker.io/minhtuyenvp02/airflow-spark:pr-49
          imagePullPolicy: Always
          name: scheduler-log-groomer
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/logs
            name: logs
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - airflow
          - db
          - check-migrations
          - --migration-wait-timeout=60
          env:
          - name: AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW_VAR_SLACK_WEB_HOOK
            valueFrom:
              secretKeyRef:
                key: SLACK_WEB_HOOK
                name: webhook-secret
          - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
            value: webhook-secret=SLACK_WEB_HOOK
          - name: AIRFLOW__CORE__FERNET_KEY
            valueFrom:
              secretKeyRef:
                key: fernet-key
                name: airflow-fernet-key
          - name: AIRFLOW_HOME
            value: /opt/airflow
          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW_CONN_AIRFLOW_DB
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__WEBSERVER__SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: webserver-secret-key
                name: airflow-webserver-secret-key
          image: docker.io/minhtuyenvp02/airflow-spark:pr-49
          imagePullPolicy: Always
          name: wait-for-airflow-migrations
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/airflow.cfg
            name: config
            readOnly: true
            subPath: airflow.cfg
          - mountPath: /opt/airflow/config/airflow_local_settings.py
            name: config
            readOnly: true
            subPath: airflow_local_settings.py
        - env:
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/ssh
          - name: GITSYNC_SSH_KEY_FILE
            value: /etc/git-secret/ssh
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GITSYNC_SSH
            value: "true"
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: GITSYNC_SSH_KNOWN_HOSTS
            value: "false"
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GITSYNC_REF
            value: v2-2-stable
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REPO
            value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
          - name: GITSYNC_REPO
            value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GITSYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_ROOT
            value: /git
          - name: GITSYNC_ROOT
            value: /git
          - name: GIT_SYNC_DEST
            value: repo
          - name: GITSYNC_LINK
            value: repo
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GITSYNC_ADD_USER
            value: "true"
          - name: GITSYNC_PERIOD
            value: 5s
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GITSYNC_MAX_FAILURES
            value: "0"
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GITSYNC_ONE_TIME
            value: "true"
          image: registry.k8s.io/git-sync/git-sync:v4.1.0
          imagePullPolicy: IfNotPresent
          name: git-sync-init
          resources: {}
          securityContext:
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /git
            name: dags
          - mountPath: /etc/git-secret/ssh
            name: git-sync-ssh-key
            readOnly: true
            subPath: gitSshKey
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
          runAsUser: 50000
        serviceAccount: airflow-scheduler
        serviceAccountName: airflow-scheduler
        terminationGracePeriodSeconds: 10
        volumes:
        - configMap:
            defaultMode: 420
            name: airflow-config
          name: config
        - emptyDir: {}
          name: dags
        - name: git-sync-ssh-key
          secret:
            defaultMode: 288
            secretName: airflow-ssh-secret
        - emptyDir: {}
          name: logs
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-06-15T23:03:40Z"
    generation: 1
    labels:
      component: statsd
      pod-template-hash: 5667dd85ff
      release: airflow
      tier: airflow
    name: airflow-statsd-5667dd85ff
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-statsd
      uid: 15d26764-597b-45ba-a689-2ee8b3cab236
    resourceVersion: "7816869"
    uid: df8b04b0-a5c7-4980-b95d-894b81655e69
  spec:
    replicas: 1
    selector:
      matchLabels:
        component: statsd
        pod-template-hash: 5667dd85ff
        release: airflow
        tier: airflow
    template:
      metadata:
        creationTimestamp: null
        labels:
          component: statsd
          pod-template-hash: 5667dd85ff
          release: airflow
          tier: airflow
      spec:
        affinity: {}
        containers:
        - args:
          - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml
          image: quay.io/prometheus/statsd-exporter:v0.26.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: 9102
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: statsd
          ports:
          - containerPort: 9125
            name: statsd-ingest
            protocol: UDP
          - containerPort: 9102
            name: statsd-scrape
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: 9102
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/statsd-exporter/mappings.yml
            name: config
            subPath: mappings.yml
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 65534
        serviceAccount: airflow-statsd
        serviceAccountName: airflow-statsd
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: airflow-statsd
          name: config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-06-16T20:12:25Z"
    generation: 1
    labels:
      component: webserver
      pod-template-hash: 676ff8f7b9
      release: airflow
      tier: airflow
    name: airflow-webserver-676ff8f7b9
    namespace: airflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: airflow-webserver
      uid: ab3c3831-f8f9-4be5-a0e5-a332ac0366b4
    resourceVersion: "7816365"
    uid: a067f510-1d45-47e4-a3ae-1f30b5361634
  spec:
    replicas: 1
    selector:
      matchLabels:
        component: webserver
        pod-template-hash: 676ff8f7b9
        release: airflow
        tier: airflow
    template:
      metadata:
        annotations:
          checksum/airflow-config: cdff98e8c3f0bebc19523bf34fd5a6eb9ca62c9c556e71201f3b5386fd6127b2
          checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8
          checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827
          checksum/metadata-secret: 1527346545415bf13f4c9ad69470086eb90d854f2b83594d78ec1badb5e13eb0
          checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688
          checksum/webserver-config: 2f3fdfd294a37094d2abee43b2b09888a5c195ee03414996bf99a4681658af94
          checksum/webserver-secret-key: c2e7d741976bcca47534b96b419058c6ee4a482ca2ee5adad27507770148319b
        creationTimestamp: null
        labels:
          component: webserver
          pod-template-hash: 676ff8f7b9
          release: airflow
          tier: airflow
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    component: webserver
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - bash
          - -c
          - exec airflow webserver
          env:
          - name: AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW_VAR_SLACK_WEB_HOOK
            valueFrom:
              secretKeyRef:
                key: SLACK_WEB_HOOK
                name: webhook-secret
          - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
            value: webhook-secret=SLACK_WEB_HOOK
          - name: AIRFLOW__CORE__FERNET_KEY
            valueFrom:
              secretKeyRef:
                key: fernet-key
                name: airflow-fernet-key
          - name: AIRFLOW_HOME
            value: /opt/airflow
          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW_CONN_AIRFLOW_DB
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__WEBSERVER__SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: webserver-secret-key
                name: airflow-webserver-secret-key
          image: docker.io/minhtuyenvp02/airflow-spark:pr-49
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: webserver
          ports:
          - containerPort: 8080
            name: airflow-ui
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "3"
              memory: 5Gi
            requests:
              cpu: "2"
              memory: 2Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          startupProbe:
            failureThreshold: 6
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/pod_templates/pod_template_file.yaml
            name: config
            readOnly: true
            subPath: pod_template_file.yaml
          - mountPath: /opt/airflow/airflow.cfg
            name: config
            readOnly: true
            subPath: airflow.cfg
          - mountPath: /opt/airflow/config/airflow_local_settings.py
            name: config
            readOnly: true
            subPath: airflow_local_settings.py
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - airflow
          - db
          - check-migrations
          - --migration-wait-timeout=60
          env:
          - name: AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW_VAR_SLACK_WEB_HOOK
            valueFrom:
              secretKeyRef:
                key: SLACK_WEB_HOOK
                name: webhook-secret
          - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
            value: webhook-secret=SLACK_WEB_HOOK
          - name: AIRFLOW__CORE__FERNET_KEY
            valueFrom:
              secretKeyRef:
                key: fernet-key
                name: airflow-fernet-key
          - name: AIRFLOW_HOME
            value: /opt/airflow
          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW_CONN_AIRFLOW_DB
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__WEBSERVER__SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: webserver-secret-key
                name: airflow-webserver-secret-key
          image: docker.io/minhtuyenvp02/airflow-spark:pr-49
          imagePullPolicy: Always
          name: wait-for-airflow-migrations
          resources:
            limits:
              cpu: "3"
              memory: 5Gi
            requests:
              cpu: "2"
              memory: 2Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/airflow.cfg
            name: config
            readOnly: true
            subPath: airflow.cfg
          - mountPath: /opt/airflow/config/airflow_local_settings.py
            name: config
            readOnly: true
            subPath: airflow_local_settings.py
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
          runAsUser: 50000
        serviceAccount: airflow-webserver
        serviceAccountName: airflow-webserver
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: airflow-config
          name: config
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: ingress
      meta.helm.sh/release-namespace: ingress
    creationTimestamp: "2024-06-14T20:03:17Z"
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.10.1
      helm.sh/chart: ingress-nginx-4.10.1
      pod-template-hash: 6b4df77c7b
    name: ingress-ingress-nginx-controller-6b4df77c7b
    namespace: ingress
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ingress-ingress-nginx-controller
      uid: ea46797a-d902-41ce-aaa5-ed2d4a329053
    resourceVersion: "5341034"
    uid: 534442d0-7fe1-48d6-8e76-7d2c69cb23ec
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress
        app.kubernetes.io/name: ingress-nginx
        pod-template-hash: 6b4df77c7b
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: ingress
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.10.1
          helm.sh/chart: ingress-nginx-4.10.1
          pod-template-hash: 6b4df77c7b
      spec:
        containers:
        - args:
          - /nginx-ingress-controller
          - --publish-service=$(POD_NAMESPACE)/ingress-ingress-nginx-controller
          - --election-id=ingress-ingress-nginx-leader
          - --controller-class=k8s.io/ingress-nginx
          - --ingress-class=nginx
          - --configmap=$(POD_NAMESPACE)/ingress-ingress-nginx-controller
          - --validating-webhook=:8443
          - --validating-webhook-certificate=/usr/local/certificates/cert
          - --validating-webhook-key=/usr/local/certificates/key
          - --enable-metrics=false
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_PRELOAD
            value: /usr/local/lib/libmimalloc.so
          image: registry.k8s.io/ingress-nginx/controller:v1.10.1@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /wait-shutdown
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 80
            hostPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            hostPort: 443
            name: https
            protocol: TCP
          - containerPort: 8443
            name: webhook
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 90Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 101
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/local/certificates/
            name: webhook-cert
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-ingress-nginx
        serviceAccountName: ingress-ingress-nginx
        terminationGracePeriodSeconds: 300
        volumes:
        - name: webhook-cert
          secret:
            defaultMode: 420
            secretName: ingress-ingress-nginx-admission
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kafka-metrics
      meta.helm.sh/release-namespace: kafka
    creationTimestamp: "2024-06-16T09:10:46Z"
    generation: 1
    labels:
      app: prometheus-kafka-exporter
      pod-template-hash: 86b48d7b76
      release: kafka-metrics
    name: kafka-metrics-prometheus-kafka-exporter-86b48d7b76
    namespace: kafka
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kafka-metrics-prometheus-kafka-exporter
      uid: 7dad9b5b-2901-4e8f-b665-c8f804dfcd7a
    resourceVersion: "2398536"
    uid: f8fc8e47-6bfa-4a37-b041-623666a497e4
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: prometheus-kafka-exporter
        pod-template-hash: 86b48d7b76
        release: kafka-metrics
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: prometheus-kafka-exporter
          pod-template-hash: 86b48d7b76
          release: kafka-metrics
      spec:
        containers:
        - args:
          - --verbosity=0
          - --kafka.server=kafka-controller-headless.kafka.svc.cluster.local:9092
          - --kafka.version=3.6.1
          image: danielqsj/kafka-exporter:v1.7.0
          imagePullPolicy: IfNotPresent
          name: prometheus-kafka-exporter
          ports:
          - containerPort: 9308
            name: exporter-port
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kafka-metrics-prometheus-kafka-exporter
        serviceAccountName: kafka-metrics-prometheus-kafka-exporter
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "4"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-06-09T16:25:34Z"
    generation: 1
    labels:
      doks.digitalocean.com/managed: "true"
      k8s-app: kube-dns
      pod-template-hash: 6857f5494d
    name: coredns-6857f5494d
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: c81dd5ab-d846-4cee-a158-1169375eb657
    resourceVersion: "7816836"
    uid: 82f373ac-9d1b-4c43-a223-554312c1a6ec
  spec:
    replicas: 2
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 6857f5494d
    template:
      metadata:
        creationTimestamp: null
        labels:
          doks.digitalocean.com/managed: "true"
          k8s-app: kube-dns
          pod-template-hash: 6857f5494d
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: docker.io/coredns/coredns:1.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 150M
            requests:
              cpu: 100m
              memory: 150M
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-06-09T16:11:02Z"
    generation: 1
    labels:
      app.kubernetes.io/name: hubble-relay
      app.kubernetes.io/part-of: cilium
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-relay
      pod-template-hash: 6b67bbc75f
    name: hubble-relay-6b67bbc75f
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hubble-relay
      uid: fecc7bc7-8e4a-4178-b6ac-2417f2b16483
    resourceVersion: "7816854"
    uid: d180e92f-dc60-4a5a-8a58-733c44ff6b60
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: hubble-relay
        pod-template-hash: 6b67bbc75f
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: hubble-relay
          app.kubernetes.io/part-of: cilium
          doks.digitalocean.com/managed: "true"
          k8s-app: hubble-relay
          pod-template-hash: 6b67bbc75f
      spec:
        affinity:
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: false
        containers:
        - args:
          - serve
          command:
          - hubble-relay
          image: quay.io/cilium/hubble-relay:v1.14.10@sha256:c156c4fc2da520d2876142ea17490440b95431a1be755d2050e72115a495cfd0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: grpc
            timeoutSeconds: 1
          name: hubble-relay
          ports:
          - containerPort: 4245
            name: grpc
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: grpc
            timeoutSeconds: 1
          resources: {}
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/hubble-relay
            name: config
            readOnly: true
          - mountPath: /var/lib/hubble-relay/tls
            name: tls
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65532
        serviceAccount: hubble-relay
        serviceAccountName: hubble-relay
        terminationGracePeriodSeconds: 1
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: config.yaml
              path: config.yaml
            name: hubble-relay-config
          name: config
        - name: tls
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: tls.crt
                  path: client.crt
                - key: tls.key
                  path: client.key
                - key: ca.crt
                  path: hubble-server-ca.crt
                name: hubble-relay-client-certs
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-06-09T16:24:33Z"
    generation: 1
    labels:
      app.kubernetes.io/name: hubble-ui
      app.kubernetes.io/part-of: cilium
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-ui
      pod-template-hash: 776986f894
    name: hubble-ui-776986f894
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hubble-ui
      uid: 24f50e9e-78e2-4335-96dc-43f8a0dfc763
    resourceVersion: "7816973"
    uid: dabbab36-afec-4941-ad57-94b7d2411c5c
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: hubble-ui
        pod-template-hash: 776986f894
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: hubble-ui
          app.kubernetes.io/part-of: cilium
          doks.digitalocean.com/managed: "true"
          k8s-app: hubble-ui
          pod-template-hash: 776986f894
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/cilium/hubble-ui:v0.13.0@sha256:7d663dc16538dd6e29061abd1047013a645e6e69c115e008bee9ea9fef9a6666
          imagePullPolicy: IfNotPresent
          name: frontend
          ports:
          - containerPort: 8081
            name: http
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/nginx/conf.d/default.conf
            name: hubble-ui-nginx-conf
            subPath: nginx.conf
          - mountPath: /tmp
            name: tmp-dir
        - env:
          - name: EVENTS_SERVER_PORT
            value: "8090"
          - name: FLOWS_API_ADDR
            value: hubble-relay:80
          image: quay.io/cilium/hubble-ui-backend:v0.13.0@sha256:1e7657d997c5a48253bb8dc91ecee75b63018d16ff5e5797e5af367336bc8803
          imagePullPolicy: IfNotPresent
          name: backend
          ports:
          - containerPort: 8090
            name: grpc
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hubble-ui
        serviceAccountName: hubble-ui
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: hubble-ui-nginx
          name: hubble-ui-nginx-conf
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-16T08:51:43Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: grafana
      pod-template-hash: 78fbf8577c
    name: prometheus-grafana-78fbf8577c
    namespace: prometheus
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-grafana
      uid: ead49d65-5ef5-4726-8414-5bad15553e78
    resourceVersion: "2403929"
    uid: 1e99f75e-cec5-4098-8fb7-1ee1db2915a7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: grafana
        pod-template-hash: 78fbf8577c
    template:
      metadata:
        annotations:
          checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: grafana
          pod-template-hash: 78fbf8577c
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.26.1
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.26.1
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:11.0.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: prometheus-grafana
        serviceAccountName: prometheus-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: prometheus-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-16T09:32:00Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: grafana
      pod-template-hash: 7c4bd5956d
    name: prometheus-grafana-7c4bd5956d
    namespace: prometheus
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-grafana
      uid: ead49d65-5ef5-4726-8414-5bad15553e78
    resourceVersion: "3095581"
    uid: 3fe9dd25-4cba-4390-b9de-1a5584cec52b
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: grafana
        pod-template-hash: 7c4bd5956d
    template:
      metadata:
        annotations:
          checksum/config: d8d9157e07caef7c3a90b40c8373d900f3c676a242c290ba75f122344fe1a0cd
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: grafana
          pod-template-hash: 7c4bd5956d
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.26.1
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.26.1
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:11.0.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: prometheus-grafana
        serviceAccountName: prometheus-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: prometheus-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-16T08:51:44Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 60.1.0
      chart: kube-prometheus-stack-60.1.0
      heritage: Helm
      pod-template-hash: 64776975f5
      release: prometheus
    name: prometheus-kube-prometheus-operator-64776975f5
    namespace: prometheus
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-prometheus-operator
      uid: fbcffa0c-2ae5-4b68-b606-fc74050e31b3
    resourceVersion: "2393487"
    uid: 8cc744fd-aab7-49b0-b142-ad9fb47f2349
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        pod-template-hash: 64776975f5
        release: prometheus
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 60.1.0
          chart: kube-prometheus-stack-60.1.0
          heritage: Helm
          pod-template-hash: 64776975f5
          release: prometheus
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/prometheus-kube-prometheus-kubelet
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.35.1
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.74.0
          imagePullPolicy: IfNotPresent
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-prometheus-operator
        serviceAccountName: prometheus-kube-prometheus-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: prometheus-kube-prometheus-admission
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
    creationTimestamp: "2024-06-16T08:51:44Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.12.0
      helm.sh/chart: kube-state-metrics-5.20.0
      pod-template-hash: 6c97d9f546
      release: prometheus
    name: prometheus-kube-state-metrics-6c97d9f546
    namespace: prometheus
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: 95a86be8-dda8-4d70-a29e-c912f12eb0ac
    resourceVersion: "3095607"
    uid: ef778e80-5f95-4745-b6ad-d0308204ef68
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 6c97d9f546
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.12.0
          helm.sh/chart: kube-state-metrics-5.20.0
          pod-template-hash: 6c97d9f546
          release: prometheus
      spec:
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: spark
      meta.helm.sh/release-namespace: spark
    creationTimestamp: "2024-06-28T16:38:06Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: spark
      app.kubernetes.io/name: spark-operator
      pod-template-hash: 686d96c96b
    name: spark-spark-operator-686d96c96b
    namespace: spark
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: spark-spark-operator
      uid: 0b1287d2-2acd-4156-85fa-e43f9a7b58d5
    resourceVersion: "6888606"
    uid: 6d8db0ee-d283-4463-b7e8-bba284ad6623
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: spark
        app.kubernetes.io/name: spark-operator
        pod-template-hash: 686d96c96b
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "10254"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: spark
          app.kubernetes.io/name: spark-operator
          pod-template-hash: 686d96c96b
      spec:
        containers:
        - args:
          - -v=2
          - -logtostderr
          - -namespace=
          - -enable-ui-service=true
          - -ingress-url-format={{$appName}}.minhtuyenvp02.id.vn/{{$appNamespace}}/{{$appName}}
          - -controller-threads=10
          - -resync-interval=30
          - -enable-batch-scheduler=false
          - -label-selector-filter=
          - -enable-metrics=true
          - -metrics-labels=app_type
          - -metrics-port=10254
          - -metrics-endpoint=/metrics
          - -metrics-prefix=
          - -enable-webhook=true
          - -webhook-secret-name=spark-spark-operator-webhook-certs
          - -webhook-secret-namespace=spark
          - -webhook-svc-name=spark-spark-operator-webhook-svc
          - -webhook-svc-namespace=spark
          - -webhook-config-name=spark-spark-operator-webhook-config
          - -webhook-port=8080
          - -webhook-timeout=30
          - -webhook-namespace-selector=
          - -enable-resource-quota-enforcement=false
          - -leader-election=true
          - -leader-election-lock-namespace=spark
          - -leader-election-lock-name=spark-operator-lock
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: docker.io/kubeflow/spark-operator:v1beta2-1.6.0-3.5.0
          imagePullPolicy: IfNotPresent
          name: spark-operator
          ports:
          - containerPort: 10254
            name: metrics
            protocol: TCP
          - containerPort: 8080
            name: webhook
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: spark-spark-operator
        serviceAccountName: spark-spark-operator
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: spark
      meta.helm.sh/release-namespace: spark
    creationTimestamp: "2024-06-14T12:47:56Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: spark
      app.kubernetes.io/name: spark-operator
      pod-template-hash: 85b9754c5d
    name: spark-spark-operator-85b9754c5d
    namespace: spark
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: spark-spark-operator
      uid: 0b1287d2-2acd-4156-85fa-e43f9a7b58d5
    resourceVersion: "6821003"
    uid: d12c10a4-f01b-49c7-bce3-2ef69bb0fe5b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: spark
        app.kubernetes.io/name: spark-operator
        pod-template-hash: 85b9754c5d
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "10254"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: spark
          app.kubernetes.io/name: spark-operator
          pod-template-hash: 85b9754c5d
      spec:
        containers:
        - args:
          - -v=2
          - -logtostderr
          - -namespace=
          - -enable-ui-service=true
          - -ingress-url-format=
          - -controller-threads=10
          - -resync-interval=30
          - -enable-batch-scheduler=false
          - -label-selector-filter=
          - -enable-metrics=true
          - -metrics-labels=app_type
          - -metrics-port=10254
          - -metrics-endpoint=/metrics
          - -metrics-prefix=
          - -enable-webhook=true
          - -webhook-secret-name=spark-spark-operator-webhook-certs
          - -webhook-secret-namespace=spark
          - -webhook-svc-name=spark-spark-operator-webhook-svc
          - -webhook-svc-namespace=spark
          - -webhook-config-name=spark-spark-operator-webhook-config
          - -webhook-port=8080
          - -webhook-timeout=30
          - -webhook-namespace-selector=
          - -enable-resource-quota-enforcement=false
          - -leader-election=true
          - -leader-election-lock-namespace=spark
          - -leader-election-lock-name=spark-operator-lock
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: docker.io/kubeflow/spark-operator:v1beta2-1.6.0-3.5.0
          imagePullPolicy: IfNotPresent
          name: spark-operator
          ports:
          - containerPort: 10254
            name: metrics
            protocol: TCP
          - containerPort: 8080
            name: webhook
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: spark-spark-operator
        serviceAccountName: spark-spark-operator
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: spark
      meta.helm.sh/release-namespace: spark
    creationTimestamp: "2024-06-28T21:04:03Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: spark
      app.kubernetes.io/name: spark-operator
      pod-template-hash: f88695599
    name: spark-spark-operator-f88695599
    namespace: spark
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: spark-spark-operator
      uid: 0b1287d2-2acd-4156-85fa-e43f9a7b58d5
    resourceVersion: "7816605"
    uid: 5820e2b8-0ca7-4c0d-90f2-842186082ec6
  spec:
    replicas: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: spark
        app.kubernetes.io/name: spark-operator
        pod-template-hash: f88695599
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "10254"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: spark
          app.kubernetes.io/name: spark-operator
          pod-template-hash: f88695599
      spec:
        containers:
        - args:
          - -v=2
          - -logtostderr
          - -namespace=
          - -enable-ui-service=true
          - -ingress-url-format=spark-app.minhtuyenvp02.id.vn/{{$appNamespace}}/{{$appName}}
          - -controller-threads=10
          - -resync-interval=30
          - -enable-batch-scheduler=false
          - -label-selector-filter=
          - -enable-metrics=true
          - -metrics-labels=app_type
          - -metrics-port=10254
          - -metrics-endpoint=/metrics
          - -metrics-prefix=
          - -enable-webhook=true
          - -webhook-secret-name=spark-spark-operator-webhook-certs
          - -webhook-secret-namespace=spark
          - -webhook-svc-name=spark-spark-operator-webhook-svc
          - -webhook-svc-namespace=spark
          - -webhook-config-name=spark-spark-operator-webhook-config
          - -webhook-port=8080
          - -webhook-timeout=30
          - -webhook-namespace-selector=
          - -enable-resource-quota-enforcement=false
          - -leader-election=true
          - -leader-election-lock-namespace=spark
          - -leader-election-lock-name=spark-operator-lock
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: docker.io/kubeflow/spark-operator:v1beta2-1.6.0-3.5.0
          imagePullPolicy: IfNotPresent
          name: spark-operator
          ports:
          - containerPort: 10254
            name: metrics
            protocol: TCP
          - containerPort: 8080
            name: webhook
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: spark-spark-operator
        serviceAccountName: spark-spark-operator
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 3
    fullyLabeledReplicas: 3
    observedGeneration: 1
    readyReplicas: 3
    replicas: 3
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-09T21:52:46Z"
    generation: 2
    labels:
      app: superset
      pod-template-hash: 58d9dbf459
      release: superset
    name: superset-58d9dbf459
    namespace: superset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: superset
      uid: 231fdd41-4fd3-4899-8444-25cb7506304f
    resourceVersion: "276086"
    uid: 140d389f-27be-46c8-bd2e-160bb15aab8b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: superset
        pod-template-hash: 58d9dbf459
        release: superset
    template:
      metadata:
        annotations:
          checksum/configOverrides: 216350f2670d8588217c1f8fb1cd345561207481b104501b0c79aa441ffc6f81
          checksum/configOverridesFiles: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/connections: 2d41a2c51ec2fc809be84fc0ea0e603c946d529b4a0e43f7257e617909561a75
          checksum/extraConfigs: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/extraSecretEnv: 72fbd486f368264fd746d16cc7e0316a6fc004ed1483dda1e4b8bde8662b6217
          checksum/extraSecrets: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/superset_bootstrap.sh: dc9a47141051ced34960c313860a55e03eb48c1fa36a0ed25c03ad60cd3b5c48
          checksum/superset_config.py: a02e301df0b06bb7adf5b4a4f13f965d4de2467d0006cd6c2728c2e4f7426ee1
          checksum/superset_init.sh: e6b1e8eac1f7a79a07a6c72a0e2ee6e09654eeb439c6bbe61bfd676917c41e02
        creationTimestamp: null
        labels:
          app: superset
          pod-template-hash: 58d9dbf459
          release: superset
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - . /app/pythonpath/superset_bootstrap.sh; /usr/bin/run-server.sh
          env:
          - name: SUPERSET_PORT
            value: "8088"
          envFrom:
          - secretRef:
              name: superset-env
          image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          name: superset
          ports:
          - containerPort: 8088
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/pythonpath
            name: superset-config
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/sh
          - -c
          - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -timeout 120s
          envFrom:
          - secretRef:
              name: superset-env
          image: apache/superset:dockerize
          imagePullPolicy: IfNotPresent
          name: wait-for-postgres
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        terminationGracePeriodSeconds: 30
        volumes:
        - name: superset-config
          secret:
            defaultMode: 420
            secretName: superset-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-10T10:55:28Z"
    generation: 1
    labels:
      app: superset
      pod-template-hash: 67dbcb86b6
      release: superset
    name: superset-67dbcb86b6
    namespace: superset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: superset
      uid: 231fdd41-4fd3-4899-8444-25cb7506304f
    resourceVersion: "6737961"
    uid: d795c1fd-de04-4d09-902a-50eba644ace5
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: superset
        pod-template-hash: 67dbcb86b6
        release: superset
    template:
      metadata:
        annotations:
          checksum/configOverrides: 216350f2670d8588217c1f8fb1cd345561207481b104501b0c79aa441ffc6f81
          checksum/configOverridesFiles: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/connections: 2d41a2c51ec2fc809be84fc0ea0e603c946d529b4a0e43f7257e617909561a75
          checksum/extraConfigs: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/extraSecretEnv: 72fbd486f368264fd746d16cc7e0316a6fc004ed1483dda1e4b8bde8662b6217
          checksum/extraSecrets: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/superset_bootstrap.sh: db546af547fe8beae35fb3461d8ee461b0e6fd06b9c289da0ba745bc34b6f35f
          checksum/superset_config.py: a02e301df0b06bb7adf5b4a4f13f965d4de2467d0006cd6c2728c2e4f7426ee1
          checksum/superset_init.sh: e6b1e8eac1f7a79a07a6c72a0e2ee6e09654eeb439c6bbe61bfd676917c41e02
        creationTimestamp: null
        labels:
          app: superset
          pod-template-hash: 67dbcb86b6
          release: superset
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - . /app/pythonpath/superset_bootstrap.sh; /usr/bin/run-server.sh
          env:
          - name: SUPERSET_PORT
            value: "8088"
          envFrom:
          - secretRef:
              name: superset-env
          image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          name: superset
          ports:
          - containerPort: 8088
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/pythonpath
            name: superset-config
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/sh
          - -c
          - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -timeout 120s
          envFrom:
          - secretRef:
              name: superset-env
          image: apache/superset:dockerize
          imagePullPolicy: IfNotPresent
          name: wait-for-postgres
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        terminationGracePeriodSeconds: 30
        volumes:
        - name: superset-config
          secret:
            defaultMode: 420
            secretName: superset-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-09T21:32:39Z"
    generation: 2
    labels:
      app: superset
      pod-template-hash: 7cd74cd487
      release: superset
    name: superset-7cd74cd487
    namespace: superset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: superset
      uid: 231fdd41-4fd3-4899-8444-25cb7506304f
    resourceVersion: "91761"
    uid: a1f31120-f081-4365-9cb9-a61d920ba84f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: superset
        pod-template-hash: 7cd74cd487
        release: superset
    template:
      metadata:
        annotations:
          checksum/configOverrides: 0323182aba078c79acaa99d36193cc4388fc8b73f8a370be166ad24d7c68b450
          checksum/configOverridesFiles: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/connections: 2d41a2c51ec2fc809be84fc0ea0e603c946d529b4a0e43f7257e617909561a75
          checksum/extraConfigs: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/extraSecretEnv: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/extraSecrets: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/superset_bootstrap.sh: dc9a47141051ced34960c313860a55e03eb48c1fa36a0ed25c03ad60cd3b5c48
          checksum/superset_config.py: 3235a31c9786417a8aff43413717a62a094b1a56bf1f329f2f5da0b67cd6d3e4
          checksum/superset_init.sh: e6b1e8eac1f7a79a07a6c72a0e2ee6e09654eeb439c6bbe61bfd676917c41e02
        creationTimestamp: null
        labels:
          app: superset
          pod-template-hash: 7cd74cd487
          release: superset
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - . /app/pythonpath/superset_bootstrap.sh; /usr/bin/run-server.sh
          env:
          - name: SUPERSET_PORT
            value: "8088"
          envFrom:
          - secretRef:
              name: superset-env
          image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          name: superset
          ports:
          - containerPort: 8088
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/pythonpath
            name: superset-config
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/sh
          - -c
          - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -timeout 120s
          envFrom:
          - secretRef:
              name: superset-env
          image: apache/superset:dockerize
          imagePullPolicy: IfNotPresent
          name: wait-for-postgres
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        terminationGracePeriodSeconds: 30
        volumes:
        - name: superset-config
          secret:
            defaultMode: 420
            secretName: superset-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-09T21:32:39Z"
    generation: 2
    labels:
      app: superset-worker
      pod-template-hash: 588f4c9d59
      release: superset
    name: superset-worker-588f4c9d59
    namespace: superset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: superset-worker
      uid: e0b403f2-d415-4087-a767-2b11c83f01df
    resourceVersion: "91681"
    uid: e9d7962a-9218-4d1d-a515-07c117453593
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: superset-worker
        pod-template-hash: 588f4c9d59
        release: superset
    template:
      metadata:
        annotations:
          checksum/configOverrides: 0323182aba078c79acaa99d36193cc4388fc8b73f8a370be166ad24d7c68b450
          checksum/configOverridesFiles: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/connections: 2d41a2c51ec2fc809be84fc0ea0e603c946d529b4a0e43f7257e617909561a75
          checksum/extraConfigs: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/extraSecretEnv: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/extraSecrets: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/superset_bootstrap.sh: dc9a47141051ced34960c313860a55e03eb48c1fa36a0ed25c03ad60cd3b5c48
          checksum/superset_config.py: 3235a31c9786417a8aff43413717a62a094b1a56bf1f329f2f5da0b67cd6d3e4
        creationTimestamp: null
        labels:
          app: superset-worker
          pod-template-hash: 588f4c9d59
          release: superset
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - . /app/pythonpath/superset_bootstrap.sh; celery --app=superset.tasks.celery_app:app
            worker
          env:
          - name: SUPERSET_PORT
            value: "8088"
          envFrom:
          - secretRef:
              name: superset-env
          image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - celery -A superset.tasks.celery_app:app inspect ping -d celery@$HOSTNAME
            failureThreshold: 3
            initialDelaySeconds: 120
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 60
          name: superset
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/pythonpath
            name: superset-config
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/sh
          - -c
          - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -wait "tcp://$REDIS_HOST:$REDIS_PORT"
            -timeout 120s
          envFrom:
          - secretRef:
              name: superset-env
          image: apache/superset:dockerize
          imagePullPolicy: IfNotPresent
          name: wait-for-postgres-redis
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        terminationGracePeriodSeconds: 30
        volumes:
        - name: superset-config
          secret:
            defaultMode: 420
            secretName: superset-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-10T10:55:28Z"
    generation: 1
    labels:
      app: superset-worker
      pod-template-hash: 6b94cb97f4
      release: superset
    name: superset-worker-6b94cb97f4
    namespace: superset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: superset-worker
      uid: e0b403f2-d415-4087-a767-2b11c83f01df
    resourceVersion: "7820459"
    uid: b396b516-877b-4e05-8c02-1bbbe010de83
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: superset-worker
        pod-template-hash: 6b94cb97f4
        release: superset
    template:
      metadata:
        annotations:
          checksum/configOverrides: 216350f2670d8588217c1f8fb1cd345561207481b104501b0c79aa441ffc6f81
          checksum/configOverridesFiles: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/connections: 2d41a2c51ec2fc809be84fc0ea0e603c946d529b4a0e43f7257e617909561a75
          checksum/extraConfigs: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/extraSecretEnv: 72fbd486f368264fd746d16cc7e0316a6fc004ed1483dda1e4b8bde8662b6217
          checksum/extraSecrets: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/superset_bootstrap.sh: db546af547fe8beae35fb3461d8ee461b0e6fd06b9c289da0ba745bc34b6f35f
          checksum/superset_config.py: a02e301df0b06bb7adf5b4a4f13f965d4de2467d0006cd6c2728c2e4f7426ee1
        creationTimestamp: null
        labels:
          app: superset-worker
          pod-template-hash: 6b94cb97f4
          release: superset
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - . /app/pythonpath/superset_bootstrap.sh; celery --app=superset.tasks.celery_app:app
            worker
          env:
          - name: SUPERSET_PORT
            value: "8088"
          envFrom:
          - secretRef:
              name: superset-env
          image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - celery -A superset.tasks.celery_app:app inspect ping -d celery@$HOSTNAME
            failureThreshold: 3
            initialDelaySeconds: 120
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 60
          name: superset
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/pythonpath
            name: superset-config
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/sh
          - -c
          - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -wait "tcp://$REDIS_HOST:$REDIS_PORT"
            -timeout 120s
          envFrom:
          - secretRef:
              name: superset-env
          image: apache/superset:dockerize
          imagePullPolicy: IfNotPresent
          name: wait-for-postgres-redis
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        terminationGracePeriodSeconds: 30
        volumes:
        - name: superset-config
          secret:
            defaultMode: 420
            secretName: superset-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-09T21:52:46Z"
    generation: 2
    labels:
      app: superset-worker
      pod-template-hash: 6c9bbf4bf9
      release: superset
    name: superset-worker-6c9bbf4bf9
    namespace: superset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: superset-worker
      uid: e0b403f2-d415-4087-a767-2b11c83f01df
    resourceVersion: "275991"
    uid: ad304cbd-6dce-4804-94fe-f3f32d920b50
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: superset-worker
        pod-template-hash: 6c9bbf4bf9
        release: superset
    template:
      metadata:
        annotations:
          checksum/configOverrides: 216350f2670d8588217c1f8fb1cd345561207481b104501b0c79aa441ffc6f81
          checksum/configOverridesFiles: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/connections: 2d41a2c51ec2fc809be84fc0ea0e603c946d529b4a0e43f7257e617909561a75
          checksum/extraConfigs: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/extraSecretEnv: 72fbd486f368264fd746d16cc7e0316a6fc004ed1483dda1e4b8bde8662b6217
          checksum/extraSecrets: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
          checksum/superset_bootstrap.sh: dc9a47141051ced34960c313860a55e03eb48c1fa36a0ed25c03ad60cd3b5c48
          checksum/superset_config.py: a02e301df0b06bb7adf5b4a4f13f965d4de2467d0006cd6c2728c2e4f7426ee1
        creationTimestamp: null
        labels:
          app: superset-worker
          pod-template-hash: 6c9bbf4bf9
          release: superset
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - . /app/pythonpath/superset_bootstrap.sh; celery --app=superset.tasks.celery_app:app
            worker
          env:
          - name: SUPERSET_PORT
            value: "8088"
          envFrom:
          - secretRef:
              name: superset-env
          image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - celery -A superset.tasks.celery_app:app inspect ping -d celery@$HOSTNAME
            failureThreshold: 3
            initialDelaySeconds: 120
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 60
          name: superset
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/pythonpath
            name: superset-config
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/sh
          - -c
          - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -wait "tcp://$REDIS_HOST:$REDIS_PORT"
            -timeout 120s
          envFrom:
          - secretRef:
              name: superset-env
          image: apache/superset:dockerize
          imagePullPolicy: IfNotPresent
          name: wait-for-postgres-redis
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        terminationGracePeriodSeconds: 30
        volumes:
        - name: superset-config
          secret:
            defaultMode: 420
            secretName: superset-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: my-trino
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-09T16:26:37Z"
    generation: 2
    labels:
      app.kubernetes.io/component: coordinator
      app.kubernetes.io/instance: my-trino
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: trino
      app.kubernetes.io/version: "449"
      helm.sh/chart: trino-0.23.1
      pod-template-hash: 68fb6575d9
    name: my-trino-trino-coordinator-68fb6575d9
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-trino-trino-coordinator
      uid: cdefe2a3-d11d-4167-9526-9ee3d954da33
    resourceVersion: "5341831"
    uid: a2bfcd1d-26b5-444f-b275-fc317f1c8829
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: coordinator
        app.kubernetes.io/instance: my-trino
        app.kubernetes.io/name: trino
        pod-template-hash: 68fb6575d9
    template:
      metadata:
        annotations:
          checksum/catalog-config: 9e1134f14c720f7a0b56f58b0600cdcee5de5b7ef538639cf48cd4cd549cbd64
          checksum/coordinator-config: 50fda1bed1d3c238221bc42abac7a3f9cfa3c6474d897312ad86c20ac7a2511f
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: coordinator
          app.kubernetes.io/instance: my-trino
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: trino
          app.kubernetes.io/version: "449"
          helm.sh/chart: trino-0.23.1
          pod-template-hash: 68fb6575d9
      spec:
        containers:
        - image: trinodb/trino:449
          imagePullPolicy: IfNotPresent
          lifecycle: {}
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /v1/info
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: trino-coordinator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /usr/lib/trino/bin/health-check
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/trino
            name: config-volume
          - mountPath: /etc/trino/catalog
            name: catalog-volume
          - mountPath: /etc/trino/schemas
            name: schemas-volume
          - mountPath: /etc/redis
            name: redis-table-schema-volumn
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: registry-credentials
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: my-trino-trino-coordinator
          name: config-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-catalog
          name: catalog-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-schemas-volume-coordinator
          name: schemas-volume
        - name: redis-table-schema-volumn
          secret:
            defaultMode: 420
            secretName: redis-table-definition
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: my-trino
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-28T09:12:47Z"
    generation: 2
    labels:
      app.kubernetes.io/component: coordinator
      app.kubernetes.io/instance: my-trino
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: trino
      app.kubernetes.io/version: "449"
      helm.sh/chart: trino-0.24.0
      pod-template-hash: 6b8dd8f857
    name: my-trino-trino-coordinator-6b8dd8f857
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-trino-trino-coordinator
      uid: cdefe2a3-d11d-4167-9526-9ee3d954da33
    resourceVersion: "7733329"
    uid: 55e40178-77eb-4138-81ce-09f529932d2f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: coordinator
        app.kubernetes.io/instance: my-trino
        app.kubernetes.io/name: trino
        pod-template-hash: 6b8dd8f857
    template:
      metadata:
        annotations:
          checksum/catalog-config: 8d01709a0b96dbd56c7133d24548de777a150b769cb13ff95da7b9e77c4fff76
          checksum/coordinator-config: cfbddf70ff23b7be16459b6b2eb01610aab047b44f783a998c63b4df23045e5d
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: coordinator
          app.kubernetes.io/instance: my-trino
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: trino
          app.kubernetes.io/version: "449"
          helm.sh/chart: trino-0.24.0
          pod-template-hash: 6b8dd8f857
      spec:
        containers:
        - image: trinodb/trino:449
          imagePullPolicy: IfNotPresent
          lifecycle: {}
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /v1/info
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: trino-coordinator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /usr/lib/trino/bin/health-check
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/trino
            name: config-volume
          - mountPath: /etc/trino/catalog
            name: catalog-volume
          - mountPath: /etc/trino/schemas
            name: schemas-volume
          - mountPath: /etc/redis
            name: redis-table-schema-volumn
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: registry-credentials
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: my-trino-trino-coordinator
          name: config-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-catalog
          name: catalog-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-schemas-volume-coordinator
          name: schemas-volume
        - name: redis-table-schema-volumn
          secret:
            defaultMode: 420
            secretName: redis-table-definition
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: my-trino
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-07-01T02:17:12Z"
    generation: 1
    labels:
      app.kubernetes.io/component: coordinator
      app.kubernetes.io/instance: my-trino
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: trino
      app.kubernetes.io/version: "449"
      helm.sh/chart: trino-0.24.0
      pod-template-hash: 6dc547c5c
    name: my-trino-trino-coordinator-6dc547c5c
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-trino-trino-coordinator
      uid: cdefe2a3-d11d-4167-9526-9ee3d954da33
    resourceVersion: "7815822"
    uid: f2df840b-c487-4390-b49e-90faa7fd1f06
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: coordinator
        app.kubernetes.io/instance: my-trino
        app.kubernetes.io/name: trino
        pod-template-hash: 6dc547c5c
    template:
      metadata:
        annotations:
          checksum/catalog-config: 8d01709a0b96dbd56c7133d24548de777a150b769cb13ff95da7b9e77c4fff76
          checksum/coordinator-config: cfbddf70ff23b7be16459b6b2eb01610aab047b44f783a998c63b4df23045e5d
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: coordinator
          app.kubernetes.io/instance: my-trino
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: trino
          app.kubernetes.io/version: "449"
          helm.sh/chart: trino-0.24.0
          pod-template-hash: 6dc547c5c
      spec:
        containers:
        - image: trinodb/trino:449
          imagePullPolicy: IfNotPresent
          lifecycle: {}
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /v1/info
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: trino-coordinator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /usr/lib/trino/bin/health-check
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/trino
            name: config-volume
          - mountPath: /etc/trino/catalog
            name: catalog-volume
          - mountPath: /etc/trino/schemas
            name: schemas-volume
          - mountPath: /etc/redis
            name: redis-table-schema-volumn
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: registry-credentials
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: my-trino-trino-coordinator
          name: config-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-catalog
          name: catalog-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-schemas-volume-coordinator
          name: schemas-volume
        - name: redis-table-schema-volumn
          secret:
            defaultMode: 420
            secretName: redis-table-definition
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: my-trino
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-24T11:39:47Z"
    generation: 2
    labels:
      app.kubernetes.io/component: coordinator
      app.kubernetes.io/instance: my-trino
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: trino
      app.kubernetes.io/version: "449"
      helm.sh/chart: trino-0.24.0
      pod-template-hash: 6dd8f99484
    name: my-trino-trino-coordinator-6dd8f99484
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-trino-trino-coordinator
      uid: cdefe2a3-d11d-4167-9526-9ee3d954da33
    resourceVersion: "6703995"
    uid: 9741ec21-c468-47db-a291-d50599dca477
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: coordinator
        app.kubernetes.io/instance: my-trino
        app.kubernetes.io/name: trino
        pod-template-hash: 6dd8f99484
    template:
      metadata:
        annotations:
          checksum/catalog-config: 8d01709a0b96dbd56c7133d24548de777a150b769cb13ff95da7b9e77c4fff76
          checksum/coordinator-config: cfbddf70ff23b7be16459b6b2eb01610aab047b44f783a998c63b4df23045e5d
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: coordinator
          app.kubernetes.io/instance: my-trino
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: trino
          app.kubernetes.io/version: "449"
          helm.sh/chart: trino-0.24.0
          pod-template-hash: 6dd8f99484
      spec:
        containers:
        - image: trinodb/trino:449
          imagePullPolicy: IfNotPresent
          lifecycle: {}
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /v1/info
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: trino-coordinator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /usr/lib/trino/bin/health-check
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/trino
            name: config-volume
          - mountPath: /etc/trino/catalog
            name: catalog-volume
          - mountPath: /etc/trino/schemas
            name: schemas-volume
          - mountPath: /etc/redis
            name: redis-table-schema-volumn
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: registry-credentials
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: my-trino-trino-coordinator
          name: config-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-catalog
          name: catalog-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-schemas-volume-coordinator
          name: schemas-volume
        - name: redis-table-schema-volumn
          secret:
            defaultMode: 420
            secretName: redis-table-definition
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: my-trino
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-07-01T02:17:12Z"
    generation: 2
    labels:
      app.kubernetes.io/component: worker
      app.kubernetes.io/instance: my-trino
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: trino
      app.kubernetes.io/version: "449"
      helm.sh/chart: trino-0.24.0
      pod-template-hash: 56d895b7fb
    name: my-trino-trino-worker-56d895b7fb
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-trino-trino-worker
      uid: 50bc6cc8-ae37-4300-83d7-61fd9651b2ba
    resourceVersion: "7816352"
    uid: 85ab12d6-f606-49e9-8c29-42da1c20aa4a
  spec:
    replicas: 2
    selector:
      matchLabels:
        app.kubernetes.io/component: worker
        app.kubernetes.io/instance: my-trino
        app.kubernetes.io/name: trino
        pod-template-hash: 56d895b7fb
    template:
      metadata:
        annotations:
          checksum/worker-config: ddb69b7237b4ad8c576018f14ee85dac71a80333e4729219c40eaaee8f99f09d
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: worker
          app.kubernetes.io/instance: my-trino
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: trino
          app.kubernetes.io/version: "449"
          helm.sh/chart: trino-0.24.0
          pod-template-hash: 56d895b7fb
      spec:
        containers:
        - image: trinodb/trino:449
          imagePullPolicy: IfNotPresent
          lifecycle: {}
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /v1/info
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: trino-worker
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /usr/lib/trino/bin/health-check
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 3Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/trino
            name: config-volume
          - mountPath: /etc/trino/catalog
            name: catalog-volume
          - mountPath: /etc/trino/schemas
            name: schemas-volume
          - mountPath: /etc/redis
            name: redis-table-schema-volumn
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: registry-credentials
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: my-trino-trino-worker
          name: config-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-catalog
          name: catalog-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-schemas-volume-worker
          name: schemas-volume
        - name: redis-table-schema-volumn
          secret:
            defaultMode: 420
            secretName: redis-table-definition
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 2
    observedGeneration: 2
    readyReplicas: 1
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: my-trino
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-09T16:26:37Z"
    generation: 3
    labels:
      app.kubernetes.io/component: worker
      app.kubernetes.io/instance: my-trino
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: trino
      app.kubernetes.io/version: "449"
      helm.sh/chart: trino-0.23.1
      pod-template-hash: 74fb969588
    name: my-trino-trino-worker-74fb969588
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-trino-trino-worker
      uid: 50bc6cc8-ae37-4300-83d7-61fd9651b2ba
    resourceVersion: "5342009"
    uid: f0ef1cf1-f1f0-4adb-9296-229f8aaedcc4
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: worker
        app.kubernetes.io/instance: my-trino
        app.kubernetes.io/name: trino
        pod-template-hash: 74fb969588
    template:
      metadata:
        annotations:
          checksum/worker-config: 8af559a0593e8dfba3459cd8d758c7aef307ce68dea108e15fceb5b5c1cb7e99
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: worker
          app.kubernetes.io/instance: my-trino
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: trino
          app.kubernetes.io/version: "449"
          helm.sh/chart: trino-0.23.1
          pod-template-hash: 74fb969588
      spec:
        containers:
        - image: trinodb/trino:449
          imagePullPolicy: IfNotPresent
          lifecycle: {}
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /v1/info
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: trino-worker
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /usr/lib/trino/bin/health-check
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/trino
            name: config-volume
          - mountPath: /etc/trino/catalog
            name: catalog-volume
          - mountPath: /etc/trino/schemas
            name: schemas-volume
          - mountPath: /etc/redis
            name: redis-table-schema-volumn
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: registry-credentials
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: my-trino-trino-worker
          name: config-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-catalog
          name: catalog-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-schemas-volume-worker
          name: schemas-volume
        - name: redis-table-schema-volumn
          secret:
            defaultMode: 420
            secretName: redis-table-definition
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: my-trino
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-24T11:39:47Z"
    generation: 6
    labels:
      app.kubernetes.io/component: worker
      app.kubernetes.io/instance: my-trino
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: trino
      app.kubernetes.io/version: "449"
      helm.sh/chart: trino-0.24.0
      pod-template-hash: d7559b6b7
    name: my-trino-trino-worker-d7559b6b7
    namespace: trino
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-trino-trino-worker
      uid: 50bc6cc8-ae37-4300-83d7-61fd9651b2ba
    resourceVersion: "7735843"
    uid: 02d0d4f7-83d8-47f6-995b-0efbcadc7af2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: worker
        app.kubernetes.io/instance: my-trino
        app.kubernetes.io/name: trino
        pod-template-hash: d7559b6b7
    template:
      metadata:
        annotations:
          checksum/worker-config: ddb69b7237b4ad8c576018f14ee85dac71a80333e4729219c40eaaee8f99f09d
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: worker
          app.kubernetes.io/instance: my-trino
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: trino
          app.kubernetes.io/version: "449"
          helm.sh/chart: trino-0.24.0
          pod-template-hash: d7559b6b7
      spec:
        containers:
        - image: trinodb/trino:449
          imagePullPolicy: IfNotPresent
          lifecycle: {}
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /v1/info
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: trino-worker
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /usr/lib/trino/bin/health-check
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/trino
            name: config-volume
          - mountPath: /etc/trino/catalog
            name: catalog-volume
          - mountPath: /etc/trino/schemas
            name: schemas-volume
          - mountPath: /etc/redis
            name: redis-table-schema-volumn
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: registry-credentials
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: my-trino-trino-worker
          name: config-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-catalog
          name: catalog-volume
        - configMap:
            defaultMode: 420
            name: my-trino-trino-schemas-volume-worker
          name: schemas-volume
        - name: redis-table-schema-volumn
          secret:
            defaultMode: 420
            secretName: redis-table-definition
  status:
    observedGeneration: 6
    replicas: 0
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-06-15T23:03:40Z"
    generation: 1
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: airflow
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-13.2.24
    name: airflow-postgresql
    namespace: airflow
    resourceVersion: "7817132"
    uid: 2194de4e-9c3d-4766-91eb-a2f74afe12cf
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: airflow
        app.kubernetes.io/name: postgresql
    serviceName: airflow-postgresql-hl
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: primary
          app.kubernetes.io/instance: airflow
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: postgresql
          app.kubernetes.io/version: 16.1.0
          helm.sh/chart: postgresql-13.2.24
        name: airflow-postgresql
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: airflow
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: POSTGRESQL_PORT_NUMBER
            value: "5432"
          - name: POSTGRESQL_VOLUME_DIR
            value: /bitnami/postgresql
          - name: PGDATA
            value: /bitnami/postgresql/data
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: postgres-password
                name: airflow-postgresql
          - name: POSTGRESQL_ENABLE_LDAP
            value: "no"
          - name: POSTGRESQL_ENABLE_TLS
            value: "no"
          - name: POSTGRESQL_LOG_HOSTNAME
            value: "false"
          - name: POSTGRESQL_LOG_CONNECTIONS
            value: "false"
          - name: POSTGRESQL_LOG_DISCONNECTIONS
            value: "false"
          - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
            value: "off"
          - name: POSTGRESQL_CLIENT_MIN_MESSAGES
            value: error
          - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
            value: pgaudit
          image: docker.io/bitnami/postgresql:16.1.0-debian-11-r15
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
          - containerPort: 5432
            name: tcp-postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - -e
              - |
                exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
                [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /bitnami/postgresql
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: dshm
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 0
    collisionCount: 0
    currentReplicas: 1
    currentRevision: airflow-postgresql-59f5bcc96d
    observedGeneration: 1
    replicas: 1
    updateRevision: airflow-postgresql-59f5bcc96d
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: airflow
      meta.helm.sh/release-namespace: airflow
    creationTimestamp: "2024-06-15T23:03:40Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
      chart: airflow-1.13.1
      component: triggerer
      heritage: Helm
      release: airflow
      tier: airflow
    name: airflow-triggerer
    namespace: airflow
    resourceVersion: "3294378"
    uid: cd7a8e94-71a4-457d-934a-e103df36d833
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        component: triggerer
        release: airflow
        tier: airflow
    serviceName: airflow-triggerer
    template:
      metadata:
        annotations:
          checksum/airflow-config: cdff98e8c3f0bebc19523bf34fd5a6eb9ca62c9c556e71201f3b5386fd6127b2
          checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8
          checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827
          checksum/metadata-secret: 1527346545415bf13f4c9ad69470086eb90d854f2b83594d78ec1badb5e13eb0
          checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          component: triggerer
          release: airflow
          tier: airflow
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    component: triggerer
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - bash
          - -c
          - exec airflow triggerer
          env:
          - name: AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW_VAR_SLACK_WEB_HOOK
            valueFrom:
              secretKeyRef:
                key: SLACK_WEB_HOOK
                name: webhook-secret
          - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
            value: webhook-secret=SLACK_WEB_HOOK
          - name: AIRFLOW__CORE__FERNET_KEY
            valueFrom:
              secretKeyRef:
                key: fernet-key
                name: airflow-fernet-key
          - name: AIRFLOW_HOME
            value: /opt/airflow
          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW_CONN_AIRFLOW_DB
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__WEBSERVER__SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: webserver-secret-key
                name: airflow-webserver-secret-key
          image: docker.io/minhtuyenvp02/airflow-spark:pr-49
          imagePullPolicy: Always
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - |
                CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
                airflow jobs check --job-type TriggererJob --local
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: triggerer
          ports:
          - containerPort: 8794
            name: triggerer-logs
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/logs
            name: logs
          - mountPath: /opt/airflow/airflow.cfg
            name: config
            readOnly: true
            subPath: airflow.cfg
          - mountPath: /opt/airflow/config/airflow_local_settings.py
            name: config
            readOnly: true
            subPath: airflow_local_settings.py
          - mountPath: /opt/airflow/dags
            name: dags
            readOnly: true
        - env:
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/ssh
          - name: GITSYNC_SSH_KEY_FILE
            value: /etc/git-secret/ssh
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GITSYNC_SSH
            value: "true"
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: GITSYNC_SSH_KNOWN_HOSTS
            value: "false"
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GITSYNC_REF
            value: v2-2-stable
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REPO
            value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
          - name: GITSYNC_REPO
            value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GITSYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_ROOT
            value: /git
          - name: GITSYNC_ROOT
            value: /git
          - name: GIT_SYNC_DEST
            value: repo
          - name: GITSYNC_LINK
            value: repo
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GITSYNC_ADD_USER
            value: "true"
          - name: GITSYNC_PERIOD
            value: 5s
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GITSYNC_MAX_FAILURES
            value: "0"
          image: registry.k8s.io/git-sync/git-sync:v4.1.0
          imagePullPolicy: IfNotPresent
          name: git-sync
          resources: {}
          securityContext:
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /git
            name: dags
          - mountPath: /etc/git-secret/ssh
            name: git-sync-ssh-key
            readOnly: true
            subPath: gitSshKey
        - args:
          - bash
          - /clean-logs
          env:
          - name: AIRFLOW__LOG_RETENTION_DAYS
            value: "15"
          - name: AIRFLOW_HOME
            value: /opt/airflow
          image: docker.io/minhtuyenvp02/airflow-spark:pr-49
          imagePullPolicy: Always
          name: triggerer-log-groomer
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/logs
            name: logs
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - airflow
          - db
          - check-migrations
          - --migration-wait-timeout=60
          env:
          - name: AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ACCESS_KEY
            value: admin
          - name: AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_SECRET_KEY
            value: admin123
          - name: AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TRIP_PRODUCER_IMAGE
            value: minhtuyenvp02/trip-generator
          - name: AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_PRODUCER_SERVERS
            value: kafka-controller-headless.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_KAFKA_CONSUMER_SERVERS
            value: kafka.kafka.svc.cluster.local:9092
          - name: AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_TOPIC
            value: yellow_tripdata,fhvhv_tripdata
          - name: AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_ENDPOINT
            value: http://minio.minio.svc.cluster.local:9000
          - name: AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_LOCATION_CSV
            value: s3a://nyc-trip-bucket/nyc-data/location.csv
          - name: AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_PATH_DPC_BASE_NUM_CSV
            value: s3a://nyc-trip-bucket/nyc-data/dpc_base_num.csv
          - name: AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_SPARK_CLUSTER
            value: spark://spark-master-svc.spark.svc.cluster.local:7077
          - name: AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_DATA_DIR
            value: nyc-trip-bucket/nyc-data/2023
          - name: AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_MESSAGE_SEND_SPEED
            value: "200"
          - name: AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_VAR_S3_BUCKET_NAME
            value: nyc-trip-bucket
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__GOOGLE_APPLICATION_CREDENTIALS
            value: /opt/airflow/secrets/key.json
          - name: AIRFLOW_VAR_SLACK_WEB_HOOK
            valueFrom:
              secretKeyRef:
                key: SLACK_WEB_HOOK
                name: webhook-secret
          - name: AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_VAR_SLACK_WEB_HOOK
            value: webhook-secret=SLACK_WEB_HOOK
          - name: AIRFLOW__CORE__FERNET_KEY
            valueFrom:
              secretKeyRef:
                key: fernet-key
                name: airflow-fernet-key
          - name: AIRFLOW_HOME
            value: /opt/airflow
          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW_CONN_AIRFLOW_DB
            valueFrom:
              secretKeyRef:
                key: connection
                name: airflow-metadata
          - name: AIRFLOW__WEBSERVER__SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: webserver-secret-key
                name: airflow-webserver-secret-key
          image: docker.io/minhtuyenvp02/airflow-spark:pr-49
          imagePullPolicy: Always
          name: wait-for-airflow-migrations
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/airflow/airflow.cfg
            name: config
            readOnly: true
            subPath: airflow.cfg
          - mountPath: /opt/airflow/config/airflow_local_settings.py
            name: config
            readOnly: true
            subPath: airflow_local_settings.py
        - env:
          - name: GIT_SSH_KEY_FILE
            value: /etc/git-secret/ssh
          - name: GITSYNC_SSH_KEY_FILE
            value: /etc/git-secret/ssh
          - name: GIT_SYNC_SSH
            value: "true"
          - name: GITSYNC_SSH
            value: "true"
          - name: GIT_KNOWN_HOSTS
            value: "false"
          - name: GITSYNC_SSH_KNOWN_HOSTS
            value: "false"
          - name: GIT_SYNC_REV
            value: HEAD
          - name: GITSYNC_REF
            value: v2-2-stable
          - name: GIT_SYNC_BRANCH
            value: main
          - name: GIT_SYNC_REPO
            value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
          - name: GITSYNC_REPO
            value: https://github.com/minhtuyenvp02/Graduation_Thesis_SOICT_2024.git
          - name: GIT_SYNC_DEPTH
            value: "1"
          - name: GITSYNC_DEPTH
            value: "1"
          - name: GIT_SYNC_ROOT
            value: /git
          - name: GITSYNC_ROOT
            value: /git
          - name: GIT_SYNC_DEST
            value: repo
          - name: GITSYNC_LINK
            value: repo
          - name: GIT_SYNC_ADD_USER
            value: "true"
          - name: GITSYNC_ADD_USER
            value: "true"
          - name: GITSYNC_PERIOD
            value: 5s
          - name: GIT_SYNC_MAX_SYNC_FAILURES
            value: "0"
          - name: GITSYNC_MAX_FAILURES
            value: "0"
          - name: GIT_SYNC_ONE_TIME
            value: "true"
          - name: GITSYNC_ONE_TIME
            value: "true"
          image: registry.k8s.io/git-sync/git-sync:v4.1.0
          imagePullPolicy: IfNotPresent
          name: git-sync-init
          resources: {}
          securityContext:
            runAsUser: 65533
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /git
            name: dags
          - mountPath: /etc/git-secret/ssh
            name: git-sync-ssh-key
            readOnly: true
            subPath: gitSshKey
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
          runAsUser: 50000
        serviceAccount: airflow-triggerer
        serviceAccountName: airflow-triggerer
        terminationGracePeriodSeconds: 60
        volumes:
        - configMap:
            defaultMode: 420
            name: airflow-config
          name: config
        - emptyDir: {}
          name: dags
        - name: git-sync-ssh-key
          secret:
            defaultMode: 288
            secretName: airflow-ssh-secret
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: logs
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 15Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: airflow-triggerer-659668cfc8
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: airflow-triggerer-659668cfc8
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: kafka
      meta.helm.sh/release-namespace: kafka
    creationTimestamp: "2024-06-13T13:51:30Z"
    generation: 2
    labels:
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
      app.kubernetes.io/version: 3.7.0
      helm.sh/chart: kafka-29.3.2
    name: kafka-controller
    namespace: kafka
    resourceVersion: "7816802"
    uid: a173ae5e-dc2b-4807-80c6-27fec8d2b4cf
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 3
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller-eligible
        app.kubernetes.io/instance: kafka
        app.kubernetes.io/name: kafka
        app.kubernetes.io/part-of: kafka
    serviceName: kafka-controller-headless
    template:
      metadata:
        annotations:
          checksum/configuration: bc0132febe701d72fa3f24a7edd2bdcb339930dab72a2c941e8523a833f64cc0
          checksum/jmx-configuration: 7d1557a23f87db78f1d531f35aaaff742125fbf3dd218346346b5c707cd8d6e1
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller-eligible
          app.kubernetes.io/instance: kafka
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kafka
          app.kubernetes.io/part-of: kafka
          app.kubernetes.io/version: 3.7.0
          helm.sh/chart: kafka-29.3.2
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: controller-eligible
                    app.kubernetes.io/instance: kafka
                    app.kubernetes.io/name: kafka
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: true
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: KAFKA_HEAP_OPTS
            value: -Xmx1024m -Xms1024m
          - name: KAFKA_KRAFT_CLUSTER_ID
            valueFrom:
              secretKeyRef:
                key: kraft-cluster-id
                name: kafka-kraft-cluster-id
          - name: JMX_PORT
            value: "5555"
          image: docker.io/bitnami/kafka:3.6.1-debian-11-r6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - pgrep
              - -f
              - kafka
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kafka
          ports:
          - containerPort: 9093
            name: controller
            protocol: TCP
          - containerPort: 9092
            name: client
            protocol: TCP
          - containerPort: 9094
            name: interbroker
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: controller
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 1500Mi
            requests:
              cpu: "1"
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /bitnami/kafka
            name: data
          - mountPath: /opt/bitnami/kafka/logs
            name: logs
          - mountPath: /opt/bitnami/kafka/config/server.properties
            name: kafka-config
            subPath: server.properties
          - mountPath: /tmp
            name: tmp
        - args:
          - -XX:MaxRAMPercentage=100
          - -XshowSettings:vm
          - -jar
          - jmx_prometheus_httpserver.jar
          - "5556"
          - /etc/jmx-kafka/jmx-kafka-prometheus.yml
          command:
          - java
          image: docker.io/bitnami/jmx-exporter:0.20.0-debian-11-r6
          imagePullPolicy: IfNotPresent
          name: jmx-exporter
          ports:
          - containerPort: 5556
            name: metrics
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/jmx-kafka
            name: jmx-config
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - args:
          - -ec
          - |
            mkdir -p "/bitnami/kafka" "/opt/bitnami/kafka/logs"
            chown -R 1001:1001 "/bitnami/kafka" "/opt/bitnami/kafka/logs"
            find "/bitnami/kafka" -mindepth 1 -maxdepth 1 -not -name ".snapshot" -not -name "lost+found" | xargs -r chown -R 1001:1001
            find "/opt/bitnami/kafka/logs" -mindepth 1 -maxdepth 1 -not -name ".snapshot" -not -name "lost+found" | xargs -r chown -R 1001:1001
          command:
          - /bin/bash
          image: docker.io/bitnami/os-shell:11-debian-11-r96
          imagePullPolicy: IfNotPresent
          name: volume-permissions
          resources: {}
          securityContext:
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /bitnami/kafka
            name: data
          - mountPath: /opt/bitnami/kafka/logs
            name: logs
        - args:
          - -ec
          - |
            /scripts/kafka-init.sh
          command:
          - /bin/bash
          env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: KAFKA_VOLUME_DIR
            value: /bitnami/kafka
          - name: KAFKA_MIN_ID
            value: "0"
          image: docker.io/bitnami/kafka:3.6.1-debian-11-r6
          imagePullPolicy: IfNotPresent
          name: kafka-init
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /bitnami/kafka
            name: data
          - mountPath: /config
            name: kafka-config
          - mountPath: /configmaps
            name: kafka-configmaps
          - mountPath: /secret-config
            name: kafka-secret-config
          - mountPath: /scripts
            name: scripts
          - mountPath: /tmp
            name: tmp
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kafka
        serviceAccountName: kafka
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kafka-controller-configuration
          name: kafka-configmaps
        - emptyDir: {}
          name: kafka-secret-config
        - emptyDir: {}
          name: kafka-config
        - emptyDir: {}
          name: tmp
        - configMap:
            defaultMode: 493
            name: kafka-scripts
          name: scripts
        - configMap:
            defaultMode: 420
            name: kafka-jmx-configuration
          name: jmx-config
        - emptyDir: {}
          name: logs
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 2
    collisionCount: 0
    currentReplicas: 3
    currentRevision: kafka-controller-7c45646788
    observedGeneration: 2
    readyReplicas: 2
    replicas: 3
    updateRevision: kafka-controller-7c45646788
    updatedReplicas: 3
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: minio
      meta.helm.sh/release-namespace: minio
    creationTimestamp: "2024-06-14T12:43:48Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: minio
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: minio
      app.kubernetes.io/version: 2024.6.13
      helm.sh/chart: minio-14.6.11
    name: minio
    namespace: minio
    resourceVersion: "7816670"
    uid: a6e6347b-3fbf-49bc-927d-e79ce0e17bd6
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 3
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: minio
        app.kubernetes.io/name: minio
    serviceName: minio-headless
    template:
      metadata:
        annotations:
          checksum/credentials-secret: f83fbdfae78c625e0f0d9e490afbd2044d84c4ececfeed24133f02d487e506b9
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: minio
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: minio
          app.kubernetes.io/version: 2024.6.13
          helm.sh/chart: minio-14.6.11
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: minio
                    app.kubernetes.io/name: minio
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: MINIO_DISTRIBUTED_MODE_ENABLED
            value: "yes"
          - name: MINIO_DISTRIBUTED_NODES
            value: minio-{0...2}.minio-headless.minio.svc.cluster.local:9000/bitnami/minio/data-{0...1}
          - name: MINIO_SCHEME
            value: http
          - name: MINIO_FORCE_NEW_KEYS
            value: "no"
          - name: MINIO_ROOT_USER
            valueFrom:
              secretKeyRef:
                key: root-user
                name: minio
          - name: MINIO_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                key: root-password
                name: minio
          - name: MINIO_SKIP_CLIENT
            value: "yes"
          - name: MINIO_BROWSER
            value: "on"
          - name: MINIO_PROMETHEUS_AUTH_TYPE
            value: public
          - name: MINIO_DATA_DIR
            value: /bitnami/minio/data-0
          image: docker.io/bitnami/minio:2024.4.18-debian-12-r0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /minio/health/live
              port: minio-api
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
          name: minio
          ports:
          - containerPort: 9000
            name: minio-api
            protocol: TCP
          - containerPort: 9001
            name: minio-console
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: minio-api
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 800Mi
            requests:
              cpu: "1"
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
          - mountPath: /opt/bitnami/minio/tmp
            name: empty-dir
            subPath: app-tmp-dir
          - mountPath: /.mc
            name: empty-dir
            subPath: app-mc-dir
          - mountPath: /bitnami/minio/data-0
            name: data-0
          - mountPath: /bitnami/minio/data-1
            name: data-1
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -ec
          - "chown -R 1001:1001 /bitnami/minio/data-0 /bitnami/minio/data-1 \n"
          image: docker.io/bitnami/os-shell:12-debian-12-r18
          imagePullPolicy: IfNotPresent
          name: volume-permissions
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 1Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            runAsUser: 0
            seLinuxOptions: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
          - mountPath: /bitnami/minio/data-0
            name: data-0
          - mountPath: /bitnami/minio/data-1
            name: data-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: OnRootMismatch
        serviceAccount: minio
        serviceAccountName: minio
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: empty-dir
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: minio
          app.kubernetes.io/name: minio
        name: data-0
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 6Gi
        volumeMode: Filesystem
      status:
        phase: Pending
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: minio
          app.kubernetes.io/name: minio
        name: data-1
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 6Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 2
    collisionCount: 0
    currentReplicas: 3
    currentRevision: minio-56bfc7b957
    observedGeneration: 3
    readyReplicas: 2
    replicas: 3
    updateRevision: minio-56bfc7b957
    updatedReplicas: 3
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
      prometheus-operator-input-hash: "2789661313505585578"
    creationTimestamp: "2024-06-16T08:51:51Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 60.1.0
      chart: kube-prometheus-stack-60.1.0
      heritage: Helm
      managed-by: prometheus-operator
      release: prometheus
    name: alertmanager-prometheus-kube-prometheus-alertmanager
    namespace: prometheus
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Alertmanager
      name: prometheus-kube-prometheus-alertmanager
      uid: 2739cf3b-2557-45be-bcbd-4bae6a842853
    resourceVersion: "2393697"
    uid: 64cc3ccf-2a14-4efc-8dfd-e3bfedd9a409
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        alertmanager: prometheus-kube-prometheus-alertmanager
        app.kubernetes.io/instance: prometheus-kube-prometheus-alertmanager
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: alertmanager
    serviceName: alertmanager-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: alertmanager
        creationTimestamp: null
        labels:
          alertmanager: prometheus-kube-prometheus-alertmanager
          app.kubernetes.io/instance: prometheus-kube-prometheus-alertmanager
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: alertmanager
          app.kubernetes.io/version: 0.27.0
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --storage.path=/alertmanager
          - --data.retention=120h
          - --cluster.listen-address=
          - --web.listen-address=:9093
          - --web.external-url=http://prometheus-kube-prometheus-alertmanager.prometheus:9093
          - --web.route-prefix=/
          - --cluster.label=prometheus/prometheus-kube-prometheus-alertmanager
          - --cluster.peer=alertmanager-prometheus-kube-prometheus-alertmanager-0.alertmanager-operated:9094
          - --cluster.reconnect-timeout=5m
          - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/prometheus/alertmanager:v0.27.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: alertmanager
          ports:
          - containerPort: 9093
            name: http-web
            protocol: TCP
          - containerPort: 9094
            name: mesh-tcp
            protocol: TCP
          - containerPort: 9094
            name: mesh-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 10
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
          - mountPath: /etc/alertmanager/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/alertmanager/certs
            name: tls-assets
            readOnly: true
          - mountPath: /alertmanager
            name: alertmanager-prometheus-kube-prometheus-alertmanager-db
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        - args:
          - --listen-address=:8080
          - --reload-url=http://127.0.0.1:9093/-/reload
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8080
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-prometheus-alertmanager
        serviceAccountName: prometheus-kube-prometheus-alertmanager
        terminationGracePeriodSeconds: 120
        volumes:
        - name: config-volume
          secret:
            defaultMode: 420
            secretName: alertmanager-prometheus-kube-prometheus-alertmanager-generated
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: alertmanager-prometheus-kube-prometheus-alertmanager-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - name: web-config
          secret:
            defaultMode: 420
            secretName: alertmanager-prometheus-kube-prometheus-alertmanager-web-config
        - emptyDir: {}
          name: alertmanager-prometheus-kube-prometheus-alertmanager-db
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: alertmanager-prometheus-kube-prometheus-alertmanager-5dc495957f
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: alertmanager-prometheus-kube-prometheus-alertmanager-5dc495957f
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: prometheus
      prometheus-operator-input-hash: "9283029898946296012"
    creationTimestamp: "2024-06-16T08:51:51Z"
    generation: 2
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 60.1.0
      chart: kube-prometheus-stack-60.1.0
      heritage: Helm
      managed-by: prometheus-operator
      operator.prometheus.io/mode: server
      operator.prometheus.io/name: prometheus-kube-prometheus-prometheus
      operator.prometheus.io/shard: "0"
      release: prometheus
    name: prometheus-prometheus-kube-prometheus-prometheus
    namespace: prometheus
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Prometheus
      name: prometheus-kube-prometheus-prometheus
      uid: f4cb9b67-49dd-41ec-91fa-6be5c504f4ec
    resourceVersion: "5341167"
    uid: 1a165e0a-2ba5-4669-bac9-c110eded300b
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus-kube-prometheus-prometheus
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: prometheus
        operator.prometheus.io/name: prometheus-kube-prometheus-prometheus
        operator.prometheus.io/shard: "0"
        prometheus: prometheus-kube-prometheus-prometheus
    serviceName: prometheus-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: prometheus
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus-kube-prometheus-prometheus
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/version: 2.52.0
          operator.prometheus.io/name: prometheus-kube-prometheus-prometheus
          operator.prometheus.io/shard: "0"
          prometheus: prometheus-kube-prometheus-prometheus
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --web.console.templates=/etc/prometheus/consoles
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
          - --web.enable-lifecycle
          - --web.external-url=http://prometheus.minhtuyenvp02.id.vn/
          - --web.route-prefix=/
          - --storage.tsdb.retention.time=10d
          - --storage.tsdb.path=/prometheus
          - --storage.tsdb.wal-compression
          - --web.config.file=/etc/prometheus/web_config/web-config.yaml
          image: quay.io/prometheus/prometheus:v2.52.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          name: prometheus
          ports:
          - containerPort: 9090
            name: http-web
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/prometheus/certs
            name: tls-assets
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-prometheus-kube-prometheus-prometheus-db
          - mountPath: /etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
            name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
          - mountPath: /etc/prometheus/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        - args:
          - --listen-address=:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
            name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8080
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
            name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-prometheus-prometheus
        serviceAccountName: prometheus-kube-prometheus-prometheus
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 600
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: prometheus-prometheus-kube-prometheus-prometheus
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: prometheus-prometheus-kube-prometheus-prometheus-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - configMap:
            defaultMode: 420
            name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
          name: prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
        - name: web-config
          secret:
            defaultMode: 420
            secretName: prometheus-prometheus-kube-prometheus-prometheus-web-config
        - emptyDir: {}
          name: prometheus-prometheus-kube-prometheus-prometheus-db
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: prometheus-prometheus-kube-prometheus-prometheus-87649c98c
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updateRevision: prometheus-prometheus-kube-prometheus-prometheus-87649c98c
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-09T21:32:39Z"
    generation: 1
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: superset
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      helm.sh/chart: postgresql-12.1.6
    name: superset-postgresql
    namespace: superset
    resourceVersion: "7817224"
    uid: 34b79911-c3c2-4fa7-887f-1aa8d2c51bab
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: superset
        app.kubernetes.io/name: postgresql
    serviceName: superset-postgresql-hl
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: primary
          app.kubernetes.io/instance: superset
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: postgresql
          helm.sh/chart: postgresql-12.1.6
        name: superset-postgresql
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: superset
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: POSTGRESQL_PORT_NUMBER
            value: "5432"
          - name: POSTGRESQL_VOLUME_DIR
            value: /bitnami/postgresql
          - name: PGDATA
            value: /bitnami/postgresql/data
          - name: POSTGRES_USER
            value: superset
          - name: POSTGRES_POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: postgres-password
                name: superset-postgresql
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: superset-postgresql
          - name: POSTGRES_DB
            value: superset
          - name: POSTGRESQL_ENABLE_LDAP
            value: "no"
          - name: POSTGRESQL_ENABLE_TLS
            value: "no"
          - name: POSTGRESQL_LOG_HOSTNAME
            value: "false"
          - name: POSTGRESQL_LOG_CONNECTIONS
            value: "false"
          - name: POSTGRESQL_LOG_DISCONNECTIONS
            value: "false"
          - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
            value: "off"
          - name: POSTGRESQL_CLIENT_MIN_MESSAGES
            value: error
          - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
            value: pgaudit
          image: docker.io/bitnami/postgresql:14.6.0-debian-11-r13
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - exec pg_isready -U "superset" -d "dbname=superset" -h 127.0.0.1 -p
                5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
          - containerPort: 5432
            name: tcp-postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - -e
              - |
                exec pg_isready -U "superset" -d "dbname=superset" -h 127.0.0.1 -p 5432
                [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            runAsUser: 1001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /bitnami/postgresql
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: dshm
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 0
    collisionCount: 0
    currentReplicas: 1
    currentRevision: superset-postgresql-6c6cb67ff4
    observedGeneration: 1
    replicas: 1
    updateRevision: superset-postgresql-6c6cb67ff4
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: superset
      meta.helm.sh/release-namespace: superset
    creationTimestamp: "2024-06-09T21:32:39Z"
    generation: 1
    labels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: superset
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      helm.sh/chart: redis-17.9.4
    name: superset-redis-master
    namespace: superset
    resourceVersion: "7817346"
    uid: 4441c812-7d4e-4ab4-8ee4-5d08a881a2fe
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: master
        app.kubernetes.io/instance: superset
        app.kubernetes.io/name: redis
    serviceName: superset-redis-headless
    template:
      metadata:
        annotations:
          checksum/configmap: fe1cca2dea4020cb4b1623fc950515f7338ac9fd6e0328821274263d9e0e4d64
          checksum/health: 73f335f1a98f6f244d0ef2e3f93af8046efda1328e0421e6099d13241de96558
          checksum/scripts: 9790c77d02ac85c0f4b6067139404f8bc34ddcdf1da13c05021f0904c0451a8d
          checksum/secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: master
          app.kubernetes.io/instance: superset
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: redis
          helm.sh/chart: redis-17.9.4
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: master
                    app.kubernetes.io/instance: superset
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - -c
          - /opt/bitnami/scripts/start-scripts/start-master.sh
          command:
          - /bin/bash
          env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: REDIS_REPLICATION_MODE
            value: master
          - name: ALLOW_EMPTY_PASSWORD
            value: "yes"
          - name: REDIS_TLS_ENABLED
            value: "no"
          - name: REDIS_PORT
            value: "6379"
          image: docker.io/bitnami/redis:7.0.10-debian-11-r4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - /health/ping_liveness_local.sh 5
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: redis
          ports:
          - containerPort: 6379
            name: redis
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - sh
              - -c
              - /health/ping_readiness_local.sh 1
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            runAsUser: 1001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/bitnami/scripts/start-scripts
            name: start-scripts
          - mountPath: /health
            name: health
          - mountPath: /data
            name: redis-data
          - mountPath: /opt/bitnami/redis/mounted-etc
            name: config
          - mountPath: /opt/bitnami/redis/etc/
            name: redis-tmp-conf
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
        serviceAccount: superset-redis
        serviceAccountName: superset-redis
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 493
            name: superset-redis-scripts
          name: start-scripts
        - configMap:
            defaultMode: 493
            name: superset-redis-health
          name: health
        - configMap:
            defaultMode: 420
            name: superset-redis-configuration
          name: config
        - emptyDir: {}
          name: redis-tmp-conf
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: redis-data
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: superset-redis-master-6bcdc58944
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: superset-redis-master-6bcdc58944
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: hive-metastore-postgresql
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-09T16:26:01Z"
    generation: 1
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hive-metastore-postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.3.0
      helm.sh/chart: postgresql-15.5.1
    name: hive-metastore-postgresql
    namespace: trino
    resourceVersion: "7817156"
    uid: 6c164673-2de4-4ad3-a649-998fc8ace919
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: hive-metastore-postgresql
        app.kubernetes.io/name: postgresql
    serviceName: hive-metastore-postgresql-hl
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: primary
          app.kubernetes.io/instance: hive-metastore-postgresql
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: postgresql
          app.kubernetes.io/version: 16.3.0
          helm.sh/chart: postgresql-15.5.1
        name: hive-metastore-postgresql
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: hive-metastore-postgresql
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: POSTGRESQL_PORT_NUMBER
            value: "5432"
          - name: POSTGRESQL_VOLUME_DIR
            value: /bitnami/postgresql
          - name: PGDATA
            value: /bitnami/postgresql/data
          - name: POSTGRES_USER
            value: admin
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hive-metastore-postgresql
          - name: POSTGRES_POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: postgres-password
                name: hive-metastore-postgresql
          - name: POSTGRES_DATABASE
            value: metastore_db
          - name: POSTGRESQL_ENABLE_LDAP
            value: "no"
          - name: POSTGRESQL_ENABLE_TLS
            value: "no"
          - name: POSTGRESQL_LOG_HOSTNAME
            value: "false"
          - name: POSTGRESQL_LOG_CONNECTIONS
            value: "false"
          - name: POSTGRESQL_LOG_DISCONNECTIONS
            value: "false"
          - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
            value: "off"
          - name: POSTGRESQL_CLIENT_MIN_MESSAGES
            value: error
          - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
            value: pgaudit
          image: docker.io/bitnami/postgresql:16.3.0-debian-12-r12
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - exec pg_isready -U "admin" -d "dbname=metastore_db" -h 127.0.0.1 -p
                5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
          - containerPort: 5432
            name: tcp-postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - -e
              - |
                exec pg_isready -U "admin" -d "dbname=metastore_db" -h 127.0.0.1 -p 5432
                [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 1Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
          - mountPath: /opt/bitnami/postgresql/conf
            name: empty-dir
            subPath: app-conf-dir
          - mountPath: /opt/bitnami/postgresql/tmp
            name: empty-dir
            subPath: app-tmp-dir
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /bitnami/postgresql
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: hive-metastore-postgresql
        serviceAccountName: hive-metastore-postgresql
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: empty-dir
        - emptyDir:
            medium: Memory
          name: dshm
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 0
    collisionCount: 0
    currentReplicas: 1
    currentRevision: hive-metastore-postgresql-b59c8b646
    observedGeneration: 1
    replicas: 1
    updateRevision: hive-metastore-postgresql-b59c8b646
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: my-hive-metastore
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-09T16:26:03Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metastore
      app.kubernetes.io/instance: my-hive-metastore
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: hive-metastore
      app.kubernetes.io/part-of: hive-metastore
      app.kubernetes.io/version: 3.1.2
      helm.sh/chart: hive-metastore-0.0.1
    name: my-hive-metastore
    namespace: trino
    resourceVersion: "7817154"
    uid: afaaefdd-02bb-4945-b75e-5d26f65c0d2a
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: metastore
        app.kubernetes.io/instance: my-hive-metastore
        app.kubernetes.io/name: hive-metastore
    serviceName: my-hive-metastore
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metastore
          app.kubernetes.io/instance: my-hive-metastore
          app.kubernetes.io/name: hive-metastore
      spec:
        containers:
        - command:
          - bash
          - entrypoint/entrypoint.sh
          image: rtdl/hive-metastore:3.1.2
          imagePullPolicy: IfNotPresent
          name: metastore
          ports:
          - containerPort: 9083
            name: thrift
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/apache-hive-metastore-3.1.2-bin/conf
            name: hive-config
          - mountPath: /opt/entrypoint
            name: entrypoint
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - |
            until nc -z -v -w90 hive-metastore-postgresql 5432; do
              echo "Waiting for Hive Metastore DB to be ready..."
              sleep 5
            done
          command:
          - /bin/sh
          - -c
          image: busybox:latest
          imagePullPolicy: IfNotPresent
          name: init-wait-db
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: my-hive-metastore
          name: hive-config
        - configMap:
            defaultMode: 420
            name: my-hive-metastore-entrypoint
          name: entrypoint
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
  status:
    availableReplicas: 0
    collisionCount: 0
    currentReplicas: 1
    currentRevision: my-hive-metastore-6cd9478865
    observedGeneration: 1
    replicas: 1
    updateRevision: my-hive-metastore-6cd9478865
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: my-redis
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-09T16:26:09Z"
    generation: 1
    labels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: my-redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.5
      helm.sh/chart: redis-19.5.0
    name: my-redis-master
    namespace: trino
    resourceVersion: "7816753"
    uid: bed2c503-0e52-4108-bf7d-dc1cc90fd067
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: master
        app.kubernetes.io/instance: my-redis
        app.kubernetes.io/name: redis
    serviceName: my-redis-headless
    template:
      metadata:
        annotations:
          checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
          checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
          checksum/scripts: 0e449b4bbf9dd08eea4509bc20671b359bfe0fae25ee5e966be27cb3a1659993
          checksum/secret: 9b054bc7354510843a124643c05d7230876d508e6d4c33eed0f7073787f8b50f
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: master
          app.kubernetes.io/instance: my-redis
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: redis
          app.kubernetes.io/version: 7.2.5
          helm.sh/chart: redis-19.5.0
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: master
                    app.kubernetes.io/instance: my-redis
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - args:
          - -c
          - /opt/bitnami/scripts/start-scripts/start-master.sh
          command:
          - /bin/bash
          env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: REDIS_REPLICATION_MODE
            value: master
          - name: ALLOW_EMPTY_PASSWORD
            value: "no"
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: my-redis
          - name: REDIS_TLS_ENABLED
            value: "no"
          - name: REDIS_PORT
            value: "6379"
          image: docker.io/bitnami/redis:7.2.5-debian-12-r0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - /health/ping_liveness_local.sh 5
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: redis
          ports:
          - containerPort: 6379
            name: redis
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - sh
              - -c
              - /health/ping_readiness_local.sh 1
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 1Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/bitnami/scripts/start-scripts
            name: start-scripts
          - mountPath: /health
            name: health
          - mountPath: /data
            name: redis-data
          - mountPath: /opt/bitnami/redis/mounted-etc
            name: config
          - mountPath: /opt/bitnami/redis/etc/
            name: empty-dir
            subPath: app-conf-dir
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: my-redis-master
        serviceAccountName: my-redis-master
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 493
            name: my-redis-scripts
          name: start-scripts
        - configMap:
            defaultMode: 493
            name: my-redis-health
          name: health
        - configMap:
            defaultMode: 420
            name: my-redis-configuration
          name: config
        - emptyDir: {}
          name: empty-dir
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: master
          app.kubernetes.io/instance: my-redis
          app.kubernetes.io/name: redis
        name: redis-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 0
    collisionCount: 0
    currentReplicas: 1
    currentRevision: my-redis-master-677fb497bc
    observedGeneration: 1
    replicas: 1
    updateRevision: my-redis-master-677fb497bc
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: my-redis
      meta.helm.sh/release-namespace: trino
    creationTimestamp: "2024-06-09T16:26:09Z"
    generation: 1
    labels:
      app.kubernetes.io/component: replica
      app.kubernetes.io/instance: my-redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.5
      helm.sh/chart: redis-19.5.0
    name: my-redis-replicas
    namespace: trino
    resourceVersion: "7817054"
    uid: 408cf3b1-2206-4f31-9632-334dcf42e34f
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 3
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: replica
        app.kubernetes.io/instance: my-redis
        app.kubernetes.io/name: redis
    serviceName: my-redis-headless
    template:
      metadata:
        annotations:
          checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
          checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
          checksum/scripts: 0e449b4bbf9dd08eea4509bc20671b359bfe0fae25ee5e966be27cb3a1659993
          checksum/secret: 9b054bc7354510843a124643c05d7230876d508e6d4c33eed0f7073787f8b50f
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: replica
          app.kubernetes.io/instance: my-redis
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: redis
          app.kubernetes.io/version: 7.2.5
          helm.sh/chart: redis-19.5.0
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: replica
                    app.kubernetes.io/instance: my-redis
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - args:
          - -c
          - /opt/bitnami/scripts/start-scripts/start-replica.sh
          command:
          - /bin/bash
          env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: REDIS_REPLICATION_MODE
            value: replica
          - name: REDIS_MASTER_HOST
            value: my-redis-master-0.my-redis-headless.trino.svc.cluster.local
          - name: REDIS_MASTER_PORT_NUMBER
            value: "6379"
          - name: ALLOW_EMPTY_PASSWORD
            value: "no"
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: my-redis
          - name: REDIS_MASTER_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: my-redis
          - name: REDIS_TLS_ENABLED
            value: "no"
          - name: REDIS_PORT
            value: "6379"
          image: docker.io/bitnami/redis:7.2.5-debian-12-r0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - /health/ping_liveness_local_and_master.sh 5
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: redis
          ports:
          - containerPort: 6379
            name: redis
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - sh
              - -c
              - /health/ping_readiness_local_and_master.sh 1
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 1Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          startupProbe:
            failureThreshold: 22
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: redis
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/bitnami/scripts/start-scripts
            name: start-scripts
          - mountPath: /health
            name: health
          - mountPath: /data
            name: redis-data
          - mountPath: /opt/bitnami/redis/mounted-etc
            name: config
          - mountPath: /opt/bitnami/redis/etc
            name: empty-dir
            subPath: app-conf-dir
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: my-redis-replica
        serviceAccountName: my-redis-replica
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 493
            name: my-redis-scripts
          name: start-scripts
        - configMap:
            defaultMode: 493
            name: my-redis-health
          name: health
        - configMap:
            defaultMode: 420
            name: my-redis-configuration
          name: config
        - emptyDir: {}
          name: empty-dir
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: replica
          app.kubernetes.io/instance: my-redis
          app.kubernetes.io/name: redis
        name: redis-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 0
    collisionCount: 0
    currentReplicas: 3
    currentRevision: my-redis-replicas-dd87c5698
    observedGeneration: 1
    replicas: 3
    updateRevision: my-redis-replicas-dd87c5698
    updatedReplicas: 3
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      helm.sh/hook: post-install,post-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
    creationTimestamp: "2024-06-14T04:36:57Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 244a525a-e816-4b1e-b9ee-581b11d9e2e3
      batch.kubernetes.io/job-name: superset-init-db
      controller-uid: 244a525a-e816-4b1e-b9ee-581b11d9e2e3
      job-name: superset-init-db
    name: superset-init-db
    namespace: superset
    resourceVersion: "1596863"
    uid: 244a525a-e816-4b1e-b9ee-581b11d9e2e3
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 244a525a-e816-4b1e-b9ee-581b11d9e2e3
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 244a525a-e816-4b1e-b9ee-581b11d9e2e3
          batch.kubernetes.io/job-name: superset-init-db
          controller-uid: 244a525a-e816-4b1e-b9ee-581b11d9e2e3
          job-name: superset-init-db
        name: superset-init-db
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - . /app/pythonpath/superset_bootstrap.sh; . /app/pythonpath/superset_init.sh
          envFrom:
          - secretRef:
              name: superset-env
          image: apachesuperset.docker.scarf.sh/apache/superset:4.0.1
          imagePullPolicy: IfNotPresent
          name: superset-init-db
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/pythonpath
            name: superset-config
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/sh
          - -c
          - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -timeout 120s
          envFrom:
          - secretRef:
              name: superset-env
          image: apache/superset:dockerize
          imagePullPolicy: IfNotPresent
          name: wait-for-postgres
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        terminationGracePeriodSeconds: 30
        volumes:
        - name: superset-config
          secret:
            defaultMode: 420
            secretName: superset-config
  status:
    completionTime: "2024-06-14T04:37:56Z"
    conditions:
    - lastProbeTime: "2024-06-14T04:37:56Z"
      lastTransitionTime: "2024-06-14T04:37:56Z"
      status: "True"
      type: Complete
    ready: 0
    startTime: "2024-06-14T04:36:57Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
kind: List
metadata:
  resourceVersion: ""
